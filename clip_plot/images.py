# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_images.ipynb.

# %% auto 0
__all__ = ['load_image', 'resize_to_max_side', 'resize_to_height', 'autocontrast', 'get_image_paths', 'load_metadata',
           'get_metadata_list', 'ValidImage', 'ImageFactory', 'create_atlases_and_thumbs']

# %% ../nbs/03_images.ipynb 3
import os
import json
import copy
from dataclasses import dataclass, field
from glob import glob
from collections.abc import Generator
import random
from abc import ABC, abstractmethod
from pathlib import Path

import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from PIL import Image

from .utils import timestamp, FILE_NAME

# %% ../nbs/03_images.ipynb 5
def load_image(image_path:str, format:str="RGB")->Image.Image:
    '''load an image and convert to desired format'''
    return Image.open(image_path).convert(format)

# %% ../nbs/03_images.ipynb 8
def resize_to_max_side(img: Image.Image, n:int=128):
    '''
    resize to a maximum side length
    '''
    w, h = img.size
    if w > h:
        # preserve ratio but ensure height is >=1 pixel
        size =  (n, max(1, int(n * h / w)))
    else:
        # preserve ratio but ensure width is >=1 pixel
        size =  (max(1, int(n * w / h)), n)
    return img.resize(size, reducing_gap=2.0)

# %% ../nbs/03_images.ipynb 9
def resize_to_height(img: Image.Image, height:int=128):
    '''
    resize to an exact height
    '''
    w, h = img.size
    if (w / h * height) < 1:
            resizedwidth = 1
    else:
        resizedwidth = int(w / h * height)
    size= (resizedwidth, height)
    return img.resize(size, reducing_gap=2.0)

# %% ../nbs/03_images.ipynb 11
def autocontrast(img: Image.Image) -> Image.Image:
    '''autocontrast lifted from keras library --
    added lightness normalization'''
    x = np.asarray(img, dtype=float)
    mean_before = x.mean()
    ## autocontrast from Keras
    x = x - np.min(x)
    x_max = np.max(x)
    if x_max != 0:
        x /= x_max
    x *= 255
    ## return to average lightness of input image
    mean_shift = x.mean() - mean_before
    x = np.clip(x - mean_shift, 0, 255)
    return Image.fromarray(x.astype("uint8"))

# %% ../nbs/03_images.ipynb 15
def get_image_paths(images: str | list) -> list[Path]:
    """
    Called once to provide a list of image paths
    """
    if isinstance(images, list):
        image_paths = [Path(im) for im in images]
    elif isinstance(images, str):
        image_paths = [Path(im) for im in glob(images, recursive=True)]

    if len(image_paths) == 0:
        raise FileNotFoundError("Error: No input images were found. Please check your --images glob")
    else:
        return image_paths

# %% ../nbs/03_images.ipynb 17
def load_metadata(metadata_paths: list[Path]) -> pd.DataFrame:
    """load metadata from disk into a single dataframe"""
    if metadata_paths[0].suffix.lower() == ".csv":
        return pd.concat(pd.read_csv(loc) for loc in metadata_paths)
    elif metadata_paths[0].suffix.lower() == ".json":
        return pd.concat(pd.read_json(loc) for loc in metadata_paths)

# %% ../nbs/03_images.ipynb 19
def get_metadata_list(metadata_paths: list[Path]) -> tuple[list[dict] , list[str]]:
    """Return a list of objects with image metadata.

    Will create 'tags' key if 'category' is in metadata
    but not 'tags'.

    Returns:
        l (List[dict]): List of metadata

    Notes:
        No check for 'filename' is performed

    Todo:
        Think about separating .csv and json functionality.
        Can we use pandas numpy to process csv?
    """
    # handle csv metadata
    metaList = []

    if metadata_paths[0].suffix == ".csv":
        if len(metadata_paths) != 1:
            raise NotImplementedError("Currently hardcoded for a single csv file or multiple json")
        with open(metadata_paths[0]) as f:
            reader = csv.reader(f)
            headers = [i.lower() for i in next(reader)]
            for i in reader:
                metaList.append(
                    {
                        headers[j]: i[j] if len(i) > j and i[j] else ""
                        for j, _ in enumerate(headers)
                    }
                )
    
    # handle json metadata
    else:
        for p in metadata_paths:
            with open(p) as f:
                metaList.append(json.load(f))

    # if the user provided a category but not a tag, use the category as the tag
    for metaDict in metaList:
        if "category" in metaDict and ("tags" in metaDict) is False:
            metaDict.update({"tags": metaDict["category"]})
    return metaList, headers

# %% ../nbs/03_images.ipynb 20
@dataclass
class ValidImage:
    path: Path
    _original: Image.Image | None = None
    _unique_name: str | None = None
    metadata: dict = field(default_factory=dict)

    @property
    def original(self):
        if self._original is None:
            self._original = load_image(self.path.as_posix())
        return self._original

    @property
    def unique_name(self):
        """Save as name when copying image."""
        if self._unique_name is None:
            self._unique_name = self.path.name
        return self._unique_name


    def valid(self, lod_cell_height: int, oblong_ratio: int | float) -> tuple[bool, str]:
        """Validate that image can be opened and loaded correctly.

        Args:
            lod_cell_height (int):
            oblong_ratio (int|float): atlas_size/cell_size ratio

        Returns:
            Tuple[pass,msg]:
                pass (bool): True if passed validation
                msg (str): Reason why validation failed 
        """
        w, h = self.original.size
        # remove images with 0 height or width when resized to lod height
        if (h == 0) or (w == 0):
            return False, f"Skipping {self.path} because it contains 0 height or width"
        # remove images that have 0 height or width when resized
        try:
            _ = resize_to_height(self.original, height=lod_cell_height)
        except ValueError:
            return False, f"Skipping {self.path} because it contains 0 height or width when resized"
        except OSError:
            return False, f"Skipping {self.path} because it could not be resized"
        # remove images that are too wide for the atlas
        if (w / h) > (oblong_ratio):
            return False, f"Skipping {self.path} because its dimensions are oblong"

        return True, ""

# %% ../nbs/03_images.ipynb 22
@dataclass
class ImageFactory:
    """
    Class encapsulates functionality required to access images,
    including compiling metadata.

    Factory is responsible for:
        - Compiling image files and their metadata
        - Filtering and validating images
        - Naming image output names
        - Providing property values

    Image factory needs to be able to provide an Image instance
        - The image instance needs to be have its metadata (if applicable)
    """
    image_paths: list[Path]
    data_dir: Path
    metadata_paths: list[Path] | None = None
    count: int = 0  # Total number of images
    meta_headers: list[str] = field(default_factory=list) # Headers in metadata
    metadata: list[dict] = field(default_factory=list) # list of metadata
    filenames: list[str] = field(default_factory=list)

    shuffle: bool = False
    atlas_size: int = 2048
    cell_size: int = 32
    lod_cell_height: int = 128
    seed: int | None = None
    max_images: int | None = None

    def __post_init__(self):
        self.filter_images()

    def __iter__(self):
        yield from self.stream_images(self.image_paths, self.metadata)

    def __getitem__(self, index):
        if index < len(self.image_paths):
            if self.metadata:
                meta = self.metadata[index]
            else:
                meta = None

            return ValidImage(self.image_paths[index], meta)

    def filter_images(self):
        """
        Filtering images, using user metadata (if provided)

        -Validate image: loads, size, resizing, oblongness

        Returns:
            images (list[Path])
            metadata (list[dict])

        Notes:
            Assumes 'filename' is provided in metadata
            Convoluted compiling of metadata
            Need to split function
        """
        # validate that input image names are unique
        image_names = [p.name for p in self.image_paths]
        duplicates = {x for x in image_names if image_names.count(x) > 1}
        print(timestamp(),
              "First three paths:", "\n".join(p.as_posix() for p in self.image_paths[:3])
              )
        if duplicates:
            dups_to_print = "\n".join(duplicates)
            raise Exception(
                f"Image filenames should be unique. These are duplicated:\n{dups_to_print}")

        # optionally shuffle the image_paths
        if self.shuffle:
            print(timestamp(), "Shuffling input images")
            random.Random(self.seed).shuffle(self.image_paths)
        else:
            self.image_paths = sorted(self.image_paths)

        # Optionally limit the number of images in image_paths
        if self.max_images:
            self.image_paths = self.image_paths[: self.max_images]

        # process and filter the images
        filtered_image_paths = []
        oblong_ratio = self.atlas_size/ self.cell_size

        print(timestamp(), "Validating input images")
        for img in tqdm(self.stream_images(self.image_paths), total=len(self.image_paths)):
            if img is None:
                pass
            else:
                valid, msg = img.valid(lod_cell_height=self.lod_cell_height, oblong_ratio=oblong_ratio)
                if valid is True:
                    filtered_image_paths += [img.path]
                else:
                    print(timestamp(), msg)

        # if there are no remaining images, throw an error
        if len(filtered_image_paths) == 0:
            raise Exception("No images were found! Please check your input image glob.")

        # handle the case user provided no metadata
        if self.metadata_paths is None:
            print(timestamp(), "No metadata locations were provided.")
            self.image_paths = filtered_image_paths
            self.count = len(filtered_image_paths)
            self.filenames = [p.name for p in filtered_image_paths]
            return

        # handle user metadata: retain only records with image and metadata
        metalist, self.meta_headers = get_metadata_list(metadata_paths=self.metadata_paths)
        metaDict = {Path(i.get(FILE_NAME, "")).name: i for i in metalist}
        meta_bn = metaDict.keys()
        img_bn = {p.name for p in self.image_paths}

        # identify images with metadata and those without metadata
        meta_present = img_bn.intersection(meta_bn)
        meta_missing = list(img_bn - meta_bn)

        # notify the user of images that are missing metadata
        if meta_missing:
            print(
                timestamp(),
                " ! Some images are missing metadata:\n  -",
                "\n  - ".join(meta_missing[:10]),
            )
            if len(meta_missing) > 10:
                print(timestamp(), " ...", len(meta_missing) - 10, "more")

            self.data_dir.mkdir(exist_ok=True, parents=True)

            missing_dir = os.path.join(self.data_dir,"missing-metadata.txt")
            with open(missing_dir, "w") as out:
                out.write("\n".join(meta_missing))

        if not meta_present:
            raise Exception( f"""No image has matching metadata. Check if '{FILE_NAME}' key was provided in metadata files""")

        # get the sorted lists of images and metadata
        images = []
        metadata = []
        for path in filtered_image_paths:
            if path.name in meta_present:
                images.append(path)
                metadata.append(copy.deepcopy(metaDict[path.name]))
                self.filenames.append(path.name)

        self.image_paths = images
        self.metadata = metadata
        self.count = len(self.image_paths)


    @staticmethod
    def stream_images(image_paths: list[Path], metadata: list[dict] | None = None
                      ) -> Generator[ValidImage | None, None, None]:
        """Read in all images from args[0], a list of image paths

        Args:
            image_paths (list[str]): list of image locations
            metadata (Optional[list[dict]]): metadata for each image

        Returns:
            yields ValidImage instance

        Notes:
            image is matched to metadata by index location
                Matching by key would be better
        """
        for i, p in enumerate(image_paths):
            try:
                m = None
                if metadata and metadata[i]:
                    m = metadata[i]
                yield ValidImage(p, metadata=m)
            except Exception as exc:
                print(timestamp(), f"Image {p} could not be processed -- {exc}")
                yield None

# %% ../nbs/03_images.ipynb 23
def create_atlases_and_thumbs(imageEngine: ImageFactory, plot_id: str, use_cache: bool = False):
    '''create folder with atlases in data dir'''

    print(timestamp(), "Copying images to output directory")

    # create directories
    atlas_dir = imageEngine.data_dir / "atlases" / str(plot_id)
    atlas_dir.mkdir(exist_ok=True, parents=True)

    thumbs_dir = imageEngine.data_dir / "thumbs"
    thumbs_dir.mkdir(exist_ok=True)

    orig_dir = imageEngine.data_dir / "originals"
    orig_dir.mkdir(exist_ok=True)

    # initialize some atlas values
    n_atlases, x, y = 0,0,0
    positions = []
    atlas_size = (imageEngine.atlas_size, imageEngine.atlas_size)
    atlas = Image.new(mode="RGB", size=atlas_size)

    for img in tqdm(imageEngine, total=imageEngine.count):

        # copy thumbnail
        thumb = resize_to_max_side(img.original, n=imageEngine.lod_cell_height)
        thumb = autocontrast(thumb)
        thumb_w, thumb_h = thumb.width, thumb.height
        thumb.save(thumbs_dir / img.unique_name)

        # copy resized original
        fullsize_path = orig_dir / img.unique_name
        if use_cache and fullsize_path.exists():
            pass
        else:
            fullsize = resize_to_height(img.original, height=600)
            fullsize.save(orig_dir / img.unique_name)

        # create atlas
        cell = resize_to_height(img.original, height=imageEngine.cell_size)
        cell = autocontrast(cell)

        if (x+cell.width) > atlas_size[0]: # end of a row
            y+=cell.height
            x=0
        if (y+cell.height) > atlas_size[0]: # end of first column
            atlas.save(atlas_dir/f"atlas-{n_atlases}.jpg")
            n_atlases += 1
            x,y=0,0 # start a new atlas
        if x == 0 and y == 0:
            atlas = Image.new(mode="RGB", size=atlas_size)
        atlas.paste(cell, (x,y))

        # store in dict
        positions.append({
            "idx": n_atlases,
            "x":x, "y":y, "w": thumb_w, "h": thumb_h
        })
        x+=cell.width

    if not (x == 0 and y == 0): # if last atlas wasn't already written
        atlas.save(atlas_dir/f"atlas-{n_atlases}.jpg")

    return atlas_dir.as_posix(), positions
