{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clip_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Find out why globals[\"_dh\"] behaves differently in nbdev_test vs. in Jupyter vs. in `.py`\n",
    "- Find robust way to get location of .ipynb -- it should work in both Jupyter and `nbdev_test`\n",
    "- Why does nbdev_readme run `00_clip_plot.ipynb`?\n",
    "- Why does nbdev_readme not run the `|# hide` cells?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup\n",
    "\n",
    "### Unconditional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import division\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from clip_plot import utils\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from os.path import join, exists, dirname, realpath\n",
    "from shutil import rmtree\n",
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "import pkg_resources\n",
    "import datetime\n",
    "import argparse\n",
    "from typing import Optional, List, Union, Tuple\n",
    "import glob2\n",
    "import uuid\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"Return a string for printing the current time\"\"\"\n",
    "    return str(datetime.datetime.now()) + \":\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing imports\n",
    "\n",
    "Note that I have removed the \"copy-web-only\" conditional import path for now\n",
    "\n",
    "`nbdev` does not like cells to have cells to have code and imports in the same cell:\n",
    "\n",
    "https://nbdev.fast.ai/getting_started.html#q-what-is-the-warning-found-a-cell-containing-mix-of-imports-and-computations.-please-use-separate-cells\n",
    "\n",
    "I think this may mean we don't get to do conditional imports. If we find a code path that really should have conditional imports, we can see if there is a workaround. For now, I don't feel \"copy web only\" is a very important functionality to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from collections import defaultdict, namedtuple\n",
    "from dateutil.parser import parse as parse_date\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from pointgrid import align_points_to_grid\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from iiif_downloader import Manifest\n",
    "from rasterfairy import coonswarp\n",
    "from scipy.stats import kde\n",
    "from PIL import ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm.autonotebook import tqdm\n",
    "import rasterfairy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP, AlignedUMAP\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Keras imports\n",
    "# from tensorflow.keras.preprocessing.image import save_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3, imagenet_utils # imagenet_utils not being used\n",
    "# from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import compat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional install imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"images\": None,\n",
    "    \"meta_dir\": None,\n",
    "    \"out_dir\": \"output\",\n",
    "    \"max_images\": None,\n",
    "    \"use_cache\": True,\n",
    "    \"encoding\": \"utf8\",\n",
    "    \"min_cluster_size\": 20,\n",
    "    \"max_clusters\": 10,\n",
    "    \"atlas_size\": 2048,\n",
    "    \"cell_size\": 32,\n",
    "    \"lod_cell_height\": 128, # Why is not in parser?\n",
    "    \"n_neighbors\": [15],\n",
    "    \"min_dist\": [0.01],\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"correlation\",\n",
    "    \"pointgrid_fill\": 0.05,\n",
    "    \"gzip\": False,\n",
    "    \"min_size\": 100,\n",
    "    \"min_score\": 0.3,\n",
    "    \"min_vertices\": 18,\n",
    "    \"plot_id\": str(uuid.uuid1()),\n",
    "    \"seed\": 24,\n",
    "    \"n_clusters\": 12,\n",
    "    \"geojson\": None,\n",
    "}\n",
    "\n",
    "FILE_NAME = \"filename\"  # Filename name key\n",
    "\n",
    "print(timestamp(), \"Ignoring cuml-umap for now to avoid conditional import\")\n",
    "print(\"we may add cuml-umap option back later\")\n",
    "cuml_ready = False\n",
    "\n",
    "cluster_method = \"hdbscan\"\n",
    "\n",
    "# handle truncated images in PIL (managed by Pillow)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\"\"\"\n",
    "NB: Keras Image class objects return image.size as w,h\n",
    "    Numpy array representations of images return image.shape as h,w,c\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entry\n",
    "\n",
    "`process_images` will kick off all the main functions for the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_clip_plot_root() -> Path:\n",
    "    # ipython doesn't have __file__ attribute\n",
    "    if in_ipython():\n",
    "        return Path(utils.__file__).parents[1]\n",
    "    else:\n",
    "        print(__file__)\n",
    "        return Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "copy_root_dir = get_clip_plot_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process_images(**kwargs):\n",
    "    \"\"\"Main method for processing user images and metadata\"\"\"\n",
    "    kwargs = preprocess_kwargs(**kwargs)\n",
    "\n",
    "    copy_web_assets(out_dir=kwargs['out_dir'])\n",
    "    if kwargs[\"copy_web_only\"]:\n",
    "        print(timestamp(), \"Done!\")\n",
    "        sys.exit()\n",
    "    \n",
    "    np.random.seed(kwargs[\"seed\"])\n",
    "    compat.v1.set_random_seed(kwargs[\"seed\"])\n",
    "    kwargs[\"out_dir\"] = join(kwargs[\"out_dir\"], \"data\")\n",
    "    kwargs[\"image_paths\"], kwargs[\"metadata\"] = filter_images(**kwargs)\n",
    "    write_metadata(**kwargs)\n",
    "    \n",
    "    kwargs[\"atlas_dir\"] = get_atlas_data(**kwargs)\n",
    "    kwargs[\"vecs\"] = get_inception_vectors(**kwargs)\n",
    "    get_manifest(**kwargs)\n",
    "    write_images(**kwargs)\n",
    "    print(timestamp(), \"Done!\")\n",
    "\n",
    "\n",
    "def preprocess_kwargs(**kwargs):\n",
    "    \"\"\"Preprocess incoming key word arguments\n",
    "    Converts n_neighbors and min_dist arguments into a list\n",
    "\n",
    "    Args:\n",
    "        n_neighbors (int, list[int], default = [15])\n",
    "        min_dist (int, list[int], default = [0.01])\n",
    "\n",
    "    Notes:\n",
    "        Convenient hook for preprocessing arguments\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in [\"n_neighbors\", \"min_dist\"]:\n",
    "        if not isinstance(kwargs[i], list):\n",
    "            kwargs[i] = [kwargs[i]]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def copy_web_assets(out_dir: str) -> None:\n",
    "    \"\"\"Copy the /web directory from the clipplot source to the users cwd.\n",
    "    Copies version number into assets.\n",
    "    \n",
    "    Args: \n",
    "        out_dir (str): directory to copy web assets\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    copy_root_dir = get_clip_plot_root()\n",
    "    src = copy_root_dir / \"clip_plot/web\"\n",
    "\n",
    "    # resolve will handle cases with ../ in the path\n",
    "    dest = Path.cwd() / Path(out_dir).resolve()\n",
    "    utils.copytree_agnostic(src.as_posix(), dest.as_posix())\n",
    "\n",
    "    # write version numbers into output\n",
    "    for i in [\"index.html\", os.path.join(\"assets\", \"js\", \"tsne.js\")]:\n",
    "        path = join(dest, i)\n",
    "        with open(path, \"r\") as f:\n",
    "            f = f.read().replace(\"VERSION_NUMBER\", get_version())\n",
    "            with open(path, \"w\") as out:\n",
    "                out.write(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_images(**kwargs):\n",
    "    \"\"\"Main method for filtering images given user metadata (if provided)\n",
    "\n",
    "    -Validate image:\n",
    "        Loading (done by stream_images and Images)\n",
    "        Size\n",
    "        resizing\n",
    "        oblong\n",
    "\n",
    "    -Compare against metadata\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "        shuffle (Optional[bool], default = False) \n",
    "\n",
    "    Returns:\n",
    "        images (list[str])\n",
    "        metadata (list[dict])\n",
    "\n",
    "    Notes:\n",
    "        Assumes 'filename' is provided in metadata\n",
    "        Convoluted compiling of metadata\n",
    "        Should All Validation should belong to Image class?\n",
    "        Need to split function\n",
    "    \"\"\"\n",
    "    # validate that input image names are unique\n",
    "    image_paths = get_image_paths(images=kwargs[\"images\"], out_dir=kwargs[\"out_dir\"])\n",
    "    image_names = list(map(clean_filename,image_paths))\n",
    "    duplicates = set([x for x in image_names if image_names.count(x) > 1])\n",
    "\n",
    "    if duplicates:\n",
    "        raise Exception(\n",
    "            \"\"\"Image filenames should be unique, but the following \n",
    "            filenames are duplicated\\n{}\"\"\".format(\"\\n\".join(duplicates)))\n",
    "    \n",
    "    # optionally shuffle the image_paths\n",
    "    if kwargs.get(\"shuffle\", False):\n",
    "        print(timestamp(), \"Shuffling input images\")\n",
    "        random.Random(kwargs[\"seed\"]).shuffle(image_paths)\n",
    "    else:\n",
    "        image_paths = sorted(image_paths)\n",
    "\n",
    "    # optionally limit the number of images in image_paths\n",
    "    if kwargs.get(\"max_images\", False):\n",
    "        image_paths = image_paths[: kwargs[\"max_images\"]]        \n",
    "\n",
    "    # process and filter the images\n",
    "    filtered_image_paths = {}\n",
    "    oblong_ratio = kwargs[\"atlas_size\"] / kwargs[\"cell_size\"]\n",
    "    for img in Image.stream_images(image_paths=image_paths):\n",
    "        valid, msg = img.valid(lod_cell_height=kwargs[\"lod_cell_height\"], oblong_ratio=oblong_ratio) \n",
    "        if valid is True:\n",
    "            filtered_image_paths[img.path] = img.filename\n",
    "        else:\n",
    "            print(timestamp(), msg)\n",
    "\n",
    "    # if there are no remaining images, throw an error\n",
    "    if len(filtered_image_paths) == 0:\n",
    "        raise Exception(\"No images were found! Please check your input image glob.\")\n",
    "\n",
    "    # handle the case user provided no metadata\n",
    "    if not kwargs.get(\"meta_dir\", False):\n",
    "        return [filtered_image_paths.keys(), []]\n",
    "\n",
    "    # handle user metadata: retain only records with image and metadata\n",
    "    metaList = get_metadata_list(meta_dir=kwargs['meta_dir'])\n",
    "    metaDict = {clean_filename(i.get(FILE_NAME, \"\")): i for i in metaList}\n",
    "    meta_bn = set(metaDict.keys())\n",
    "    img_bn = set(filtered_image_paths.values())\n",
    "\n",
    "    # identify images with metadata and those without metadata\n",
    "    meta_present = img_bn.intersection(meta_bn)\n",
    "    meta_missing = list(img_bn - meta_bn)\n",
    "\n",
    "    # notify the user of images that are missing metadata\n",
    "    if meta_missing:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \" ! Some images are missing metadata:\\n  -\",\n",
    "            \"\\n  - \".join(meta_missing[:10]),\n",
    "        )\n",
    "        if len(meta_missing) > 10:\n",
    "            print(timestamp(), \" ...\", len(meta_missing) - 10, \"more\")\n",
    "\n",
    "        if os.path.exists(kwargs['out_dir']) is False:\n",
    "            os.makedirs(kwargs['out_dir'])\n",
    "            \n",
    "        missing_dir = os.path.join(kwargs['out_dir'],\"missing-metadata.txt\")\n",
    "        with open(missing_dir, \"w\") as out:\n",
    "            out.write(\"\\n\".join(meta_missing))\n",
    "\n",
    "    if not meta_present:\n",
    "        raise Exception( f\"\"\"No image has matching metadata. Check if '{FILE_NAME}' key was provided in metadata files\"\"\")\n",
    "\n",
    "    # get the sorted lists of images and metadata\n",
    "    images = []\n",
    "    metadata = []\n",
    "    for path, fileName in filtered_image_paths.items():\n",
    "        if fileName in meta_present:\n",
    "            images.append(path)\n",
    "            metadata.append(copy.deepcopy(metaDict[fileName]))\n",
    "\n",
    "    return [images, metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_image_paths(images:str, out_dir: str) -> List[str]:\n",
    "    \"\"\"Called once to provide a list of image paths--handles IIIF manifest input.\n",
    "    \n",
    "    args:\n",
    "        images (str): directory location of images.\n",
    "        out_dir (str): output directory for downloaded IIIF files.\n",
    "\n",
    "    returns:\n",
    "        image_paths list(str): list of image paths.\n",
    "\n",
    "    Note:\n",
    "        Old/previous images are not deleted from IIIF directory.\n",
    "\n",
    "    Todo:\n",
    "        Consider separate function that handles IIIF images\n",
    "        from glob images.\n",
    "    \"\"\"\n",
    "\n",
    "    image_paths = None\n",
    "\n",
    "    # Is images a iiif file or image directory?\n",
    "    if os.path.isfile(images):\n",
    "        # Handle list of IIIF image inputs\n",
    "        iiif_dir = os.path.join(out_dir,\"iiif-downloads\")\n",
    "\n",
    "        # Check if directory already contains anything\n",
    "        if os.path.exists(iiif_dir) and os.listdir(iiif_dir):\n",
    "            print(\"Warning: IIIF directory already contains content!\")\n",
    "\n",
    "        with open(images) as f:\n",
    "            urls = [url.strip() for url in f.read().split(\"\\n\") if url.startswith(\"http\")]\n",
    "            count = 0\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    Manifest(url=url, out_dir=iiif_dir).save_images(limit=1)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    print(timestamp(), \"Could not download url \" + url)\n",
    "\n",
    "            if count == 0:\n",
    "                raise Exception('No IIIF images were successfully downloaded!')\n",
    "\n",
    "            image_paths = glob2.glob(os.path.join(out_dir,\"iiif-downloads\", \"images\", \"*\"))\n",
    "   \n",
    "    # handle case where images flag points to a glob of images\n",
    "    if not image_paths:\n",
    "        image_paths = glob2.glob(images)\n",
    "\n",
    "    # handle case user provided no images\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(\"Error: No input images were found. Please check your --images glob\")\n",
    "\n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_filename(s, **kwargs):\n",
    "    \"\"\"Given a string that points to a filename, return a clean filename\n",
    "    \n",
    "    Args:\n",
    "        s (str): filename path\n",
    "\n",
    "    Returns:\n",
    "        s (str): clean file name\n",
    "\n",
    "    Notes:\n",
    "        kwargs is not used at all\n",
    "    \n",
    "    \"\"\"\n",
    "    s = unquote(os.path.basename(s))\n",
    "    invalid_chars = '<>:;,\"/\\\\|?*[]'\n",
    "    for i in invalid_chars:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Metadata\n",
    "##\n",
    "\n",
    "\n",
    "def get_metadata_list(meta_dir: str) -> List[dict]:\n",
    "    \"\"\"Return a list of objects with image metadata.\n",
    "\n",
    "    Will create 'tags' key if 'category' is in metadata\n",
    "    but not 'tags'.\n",
    "    \n",
    "    Args:\n",
    "        metadata (str, default = None): Metadata location\n",
    "\n",
    "    Returns:\n",
    "        l (List[dict]): List of metadata \n",
    "\n",
    "    Notes:\n",
    "        No check for 'filename' is performed\n",
    "\n",
    "    Todo:\n",
    "        Think about separating .csv and json functionality.\n",
    "        Can we use pandas numpy to process csv?\n",
    "    \"\"\"\n",
    "\n",
    "    # handle csv metadata\n",
    "    metaList = []\n",
    "    if meta_dir.endswith(\".csv\"):\n",
    "        with open(meta_dir) as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = [i.lower() for i in next(reader)]\n",
    "            for i in reader:\n",
    "                metaList.append(\n",
    "                    {\n",
    "                        headers[j]: i[j] if len(i) > j and i[j] else \"\"\n",
    "                        for j, _ in enumerate(headers)\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    # handle json metadata\n",
    "    else:\n",
    "        for i in glob2.glob(meta_dir):\n",
    "            with open(i) as f:\n",
    "                metaList.append(json.load(f))\n",
    "\n",
    "    # if the user provided a category but not a tag, use the category as the tag\n",
    "    for metaDict in metaList:\n",
    "        if \"category\" in metaDict and (\"tags\" in metaDict) is False:\n",
    "            metaDict.update({\"tags\": metaDict[\"category\"]})\n",
    "    return metaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_metadata(metadata, **kwargs):\n",
    "    \"\"\"Write list `metadata` of objects to disk\n",
    "    \n",
    "    Args:\n",
    "        metadata (list[dict])\n",
    "        out_dir (str)\n",
    "\n",
    "        subfunctions:\n",
    "            write_json():\n",
    "                gzip (Optional[bool]):\n",
    "                encoding (Optional[str]): Required if gzip is provided\n",
    "                    default = 'utf8'\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        Improve variable naming\n",
    "    \n",
    "    \"\"\"\n",
    "    if not metadata:\n",
    "        return\n",
    "\n",
    "    out_dir = join(kwargs[\"out_dir\"], \"metadata\")\n",
    "    for i in [\"filters\", \"options\", \"file\"]:\n",
    "        out_path = join(out_dir, i)\n",
    "        if not exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "    \n",
    "    # create the lists of images with each tag\n",
    "    d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        filename = clean_filename(i[FILE_NAME])\n",
    "        i[\"tags\"] = [j.strip() for j in i.get(\"tags\", \"\").split(\"|\")]\n",
    "        for j in i[\"tags\"]:\n",
    "            d[\"__\".join(j.split())].append(filename)\n",
    "        write_json(os.path.join(out_dir, \"file\", filename + \".json\"), i, **kwargs)\n",
    "\n",
    "    write_json(\n",
    "        os.path.join(out_dir, \"filters\", \"filters.json\"),\n",
    "        [\n",
    "            {\n",
    "                \"filter_name\": \"select\",\n",
    "                \"filter_values\": list(d.keys()),\n",
    "            }\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # create the options for the category dropdown\n",
    "    for i in d:\n",
    "        write_json(os.path.join(out_dir, \"options\", i + \".json\"), d[i], **kwargs)\n",
    "    # create the map from date to images with that date (if dates present)\n",
    "    date_d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        date = i.get(\"year\", \"\")\n",
    "        if date:\n",
    "            date_d[date].append(clean_filename(i[FILE_NAME]))\n",
    "\n",
    "    # find the min and max dates to show on the date slider\n",
    "    dates = np.array([int(i.strip()) for i in date_d if is_number(i)])\n",
    "    domain = {\"min\": float(\"inf\"), \"max\": -float(\"inf\")}\n",
    "    mean = np.mean(dates)\n",
    "    std = np.std(dates)\n",
    "    for i in dates:\n",
    "        # update the date domain with all non-outlier dates\n",
    "        if abs(mean - i) < (std * 4):\n",
    "            domain[\"min\"] = int(min(i, domain[\"min\"]))\n",
    "            domain[\"max\"] = int(max(i, domain[\"max\"]))\n",
    "\n",
    "    # write the dates json\n",
    "    if len(date_d) > 1:\n",
    "        write_json(\n",
    "            os.path.join(out_dir, \"dates.json\"),\n",
    "            {\n",
    "                \"domain\": domain,\n",
    "                \"dates\": date_d,\n",
    "            },\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Return a boolean indicating if a string is a number\n",
    "    \n",
    "    Args:\n",
    "        s (Any); Value to be checked\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Main\n",
    "##\n",
    "\n",
    "\n",
    "def get_manifest(**kwargs):\n",
    "    \"\"\"Create and return the base object for the manifest output file\n",
    "    \n",
    "    Args:\n",
    "        atlas_dir (str)\n",
    "        image_paths (str)\n",
    "        plot_id (str, default = str(uuid.uuid1()))\n",
    "        out_dir (str)\n",
    "        metadata (list[dict]): Only checking if provided\n",
    "        gzip (bool, default = False)\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "\n",
    "        Need to check subfunctions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        Original description is inadequate\n",
    "        Function is to big (god function)\n",
    "    \n",
    "    \"\"\"\n",
    "    # load the atlas data\n",
    "    atlas_data = json.load(open(join(kwargs[\"atlas_dir\"], \"atlas_positions.json\")))\n",
    "    # store each cell's size and atlas position\n",
    "    atlas_ids = set([i[\"idx\"] for i in atlas_data])\n",
    "    sizes = [[] for _ in atlas_ids]\n",
    "    pos = [[] for _ in atlas_ids]\n",
    "    for idx, i in enumerate(atlas_data):\n",
    "        sizes[i[\"idx\"]].append([i[\"w\"], i[\"h\"]])\n",
    "        pos[i[\"idx\"]].append([i[\"x\"], i[\"y\"]])\n",
    "    # obtain the paths to each layout's JSON positions\n",
    "    layouts = get_layouts(**kwargs)\n",
    "    # create a heightmap for the umap layout\n",
    "    if \"umap\" in layouts and layouts[\"umap\"]:\n",
    "        get_heightmap(layouts[\"umap\"][\"variants\"][0][\"layout\"], \"umap\", **kwargs)\n",
    "    # specify point size scalars\n",
    "    point_sizes = {}\n",
    "    point_sizes[\"min\"] = 0\n",
    "    point_sizes[\"grid\"] = 1 / math.ceil(len(kwargs[\"image_paths\"]) ** (1 / 2))\n",
    "    point_sizes[\"max\"] = point_sizes[\"grid\"] * 1.2\n",
    "    point_sizes[\"scatter\"] = point_sizes[\"grid\"] * 0.2\n",
    "    point_sizes[\"initial\"] = point_sizes[\"scatter\"]\n",
    "    point_sizes[\"categorical\"] = point_sizes[\"grid\"] * 0.6\n",
    "    point_sizes[\"geographic\"] = point_sizes[\"grid\"] * 0.025\n",
    "    # fetch the date distribution data for point sizing\n",
    "    if \"date\" in layouts and layouts[\"date\"]:\n",
    "        date_layout = read_json(layouts[\"date\"][\"labels\"], **kwargs)\n",
    "        point_sizes[\"date\"] = 1 / (\n",
    "            (date_layout[\"cols\"] + 1) * len(date_layout[\"labels\"])\n",
    "        )\n",
    "    # create manifest json\n",
    "    manifest = {\n",
    "        \"version\": get_version(),\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "        \"output_directory\": os.path.split(kwargs[\"out_dir\"])[0],\n",
    "        \"layouts\": layouts,\n",
    "        \"initial_layout\": \"umap\",\n",
    "        \"point_sizes\": point_sizes,\n",
    "        \"imagelist\": get_path(\"imagelists\", \"imagelist\", **kwargs),\n",
    "        \"atlas_dir\": kwargs[\"atlas_dir\"],\n",
    "        \"metadata\": True if kwargs[\"metadata\"] else False,\n",
    "        \"default_hotspots\": get_hotspots(layouts=layouts, **kwargs),\n",
    "        \"custom_hotspots\": get_path(\n",
    "            \"hotspots\", \"user_hotspots\", add_hash=False, **kwargs\n",
    "        ),\n",
    "        \"gzipped\": kwargs[\"gzip\"],\n",
    "        \"config\": {\n",
    "            \"sizes\": {\n",
    "                \"atlas\": kwargs[\"atlas_size\"],\n",
    "                \"cell\": kwargs[\"cell_size\"],\n",
    "                \"lod\": kwargs[\"lod_cell_height\"],\n",
    "            },\n",
    "        },\n",
    "        \"creation_date\": datetime.datetime.today().strftime(\"%d-%B-%Y-%H:%M:%S\"),\n",
    "    }\n",
    "    # write the manifest without gzipping\n",
    "    no_gzip_kwargs = {\n",
    "        \"out_dir\": kwargs[\"out_dir\"],\n",
    "        \"gzip\": False,\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "    }\n",
    "    path = get_path(\"manifests\", \"manifest\", **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    path = get_path(None, \"manifest\", add_hash=False, **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    # create images json\n",
    "    imagelist = {\n",
    "        \"cell_sizes\": sizes,\n",
    "        \"images\": [clean_filename(i) for i in kwargs[\"image_paths\"]],\n",
    "        \"atlas\": {\n",
    "            \"count\": len(atlas_ids),\n",
    "            \"positions\": pos,\n",
    "        },\n",
    "    }\n",
    "    write_json(manifest[\"imagelist\"], imagelist, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Atlases\n",
    "##\n",
    "\n",
    "\n",
    "def get_atlas_data(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate and save to disk all atlases to be used for this visualization\n",
    "    If square, center each cell in an nxn square, else use uniform height\n",
    "\n",
    "    Args:\n",
    "        out_dir (str)\n",
    "        plot_id (str, default = str(uuid.uuid1()))\n",
    "        use_cache (bool, default = False)\n",
    "        shuffle (Optional[bool], default = False)\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        out_dir (str): Atlas location \n",
    "\n",
    "    Notes:\n",
    "\n",
    "    \"\"\"\n",
    "    # if the atlas files already exist, load from cache\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"atlases\", kwargs[\"plot_id\"])\n",
    "    if (\n",
    "        os.path.exists(out_dir)\n",
    "        and kwargs[\"use_cache\"]\n",
    "        and not kwargs.get(\"shuffle\", False)\n",
    "    ):\n",
    "        print(timestamp(), \"Loading saved atlas data\")\n",
    "        return out_dir\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # else create the atlas images and store the positions of cells in atlases\n",
    "    print(timestamp(), \"Creating atlas files\")\n",
    "    n = 0  # number of atlases\n",
    "    x = 0  # x pos in atlas\n",
    "    y = 0  # y pos in atlas\n",
    "    positions = []  # l[cell_idx] = atlas data\n",
    "    atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        cell_data = i.resize_to_height(kwargs[\"cell_size\"])\n",
    "        _, v, _ = cell_data.shape\n",
    "        appendable = False\n",
    "        if (x + v) <= kwargs[\"atlas_size\"]:\n",
    "            appendable = True\n",
    "        elif (y + (2 * kwargs[\"cell_size\"])) <= kwargs[\"atlas_size\"]:\n",
    "            y += kwargs[\"cell_size\"]\n",
    "            x = 0\n",
    "            appendable = True\n",
    "        if not appendable:\n",
    "            save_atlas(atlas, out_dir, n)\n",
    "            n += 1\n",
    "            atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "            x = 0\n",
    "            y = 0\n",
    "        atlas[y : y + kwargs[\"cell_size\"], x : x + v] = cell_data\n",
    "        # find the size of the cell in the lod canvas\n",
    "        lod_data = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        h, w, _ = lod_data.shape  # h,w,colors in lod-cell sized image `i`\n",
    "        positions.append(\n",
    "            {\n",
    "                \"idx\": n,  # atlas idx\n",
    "                \"x\": x,  # x offset of cell in atlas\n",
    "                \"y\": y,  # y offset of cell in atlas\n",
    "                \"w\": w,  # w of cell at lod size\n",
    "                \"h\": h,  # h of cell at lod size\n",
    "            }\n",
    "        )\n",
    "        x += v\n",
    "    save_atlas(atlas, out_dir, n)\n",
    "    out_path = os.path.join(out_dir, \"atlas_positions.json\")\n",
    "    with open(out_path, \"w\") as out:\n",
    "        json.dump(positions, out)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def save_atlas(atlas, out_dir, n):\n",
    "    \"\"\"Save an atlas to disk\"\"\"\n",
    "    out_path = join(out_dir, \"atlas-{}.jpg\".format(n))\n",
    "    save_image(out_path, atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Layouts\n",
    "##\n",
    "\n",
    "\n",
    "def get_layouts(**kwargs):\n",
    "    \"\"\"Get the image positions in each projection\"\"\"\n",
    "    umap = get_umap_layout(**kwargs)\n",
    "    layouts = {\n",
    "        \"umap\": umap,\n",
    "        \"alphabetic\": {\n",
    "            \"layout\": get_alphabetic_layout(**kwargs),\n",
    "        },\n",
    "        \"grid\": {\n",
    "            \"layout\": get_rasterfairy_layout(umap=umap, **kwargs),\n",
    "        },\n",
    "        \"categorical\": get_categorical_layout(**kwargs),\n",
    "        \"date\": get_date_layout(**kwargs),\n",
    "        \"geographic\": get_geographic_layout(**kwargs),\n",
    "        \"custom\": get_custom_layout(**kwargs),\n",
    "    }\n",
    "    return layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_inception_vectors(**kwargs):\n",
    "    \"\"\"Create and return Inception vector representation of Image() instances\"\"\"\n",
    "    print(\n",
    "        timestamp(),\n",
    "        \"Creating Inception vectors for {} images\".format(len(kwargs[\"image_paths\"])),\n",
    "    )\n",
    "    vector_dir = os.path.join(kwargs[\"out_dir\"], \"image-vectors\", \"inception\")\n",
    "    if not os.path.exists(vector_dir):\n",
    "        os.makedirs(vector_dir)\n",
    "    base = InceptionV3(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    model = Model(inputs=base.input, outputs=base.get_layer(\"avg_pool\").output)\n",
    "    print(timestamp(), \"Creating image array\")\n",
    "    vecs = []\n",
    "    with tqdm(total=len(kwargs[\"image_paths\"])) as progress_bar:\n",
    "        for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "            vector_path = os.path.join(vector_dir, clean_filename(i.path) + \".npy\")\n",
    "            if os.path.exists(vector_path) and kwargs[\"use_cache\"]:\n",
    "                vec = np.load(vector_path)\n",
    "            else:\n",
    "                im = preprocess_input(image_to_array(i.original.resize((299, 299))))\n",
    "                vec = model.predict(np.expand_dims(im, 0)).squeeze()\n",
    "                np.save(vector_path, vec)\n",
    "            vecs.append(vec)\n",
    "            progress_bar.update(1)\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_umap_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images passed through a umap projection\"\"\"\n",
    "    vecs = kwargs[\"vecs\"]\n",
    "    w = PCA(n_components=min(100, len(vecs))).fit_transform(vecs)\n",
    "    # single model umap\n",
    "    if len(kwargs[\"n_neighbors\"]) == 1 and len(kwargs[\"min_dist\"]) == 1:\n",
    "        return process_single_layout_umap(w, **kwargs)\n",
    "    else:\n",
    "        return process_multi_layout_umap(w, **kwargs)\n",
    "\n",
    "\n",
    "def process_single_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a single layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating single umap layout\")\n",
    "    model = get_umap_model(**kwargs)\n",
    "    out_path = get_path(\"layouts\", \"umap\", **kwargs)\n",
    "    if cuml_ready:\n",
    "        z = model.fit(v).embedding_\n",
    "    else:\n",
    "        if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "            return out_path\n",
    "        y = []\n",
    "        if kwargs.get(\"metadata\", False):\n",
    "            labels = [i.get(\"label\", None) for i in kwargs[\"metadata\"]]\n",
    "            # if the user provided labels, integerize them\n",
    "            if any([i for i in labels]):\n",
    "                d = defaultdict(lambda: len(d))\n",
    "                for i in labels:\n",
    "                    if i == None:\n",
    "                        y.append(-1)\n",
    "                    else:\n",
    "                        y.append(d[i])\n",
    "                y = np.array(y)\n",
    "        # project the PCA space down to 2d for visualization\n",
    "        z = model.fit(v, y=y if np.any(y) else None).embedding_\n",
    "    return {\n",
    "        \"variants\": [\n",
    "            {\n",
    "                \"n_neighbors\": kwargs[\"n_neighbors\"][0],\n",
    "                \"min_dist\": kwargs[\"min_dist\"][0],\n",
    "                \"layout\": write_layout(out_path, z, **kwargs),\n",
    "                \"jittered\": get_pointgrid_layout(out_path, \"umap\", **kwargs),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def process_multi_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a multi-layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating multi-umap layout\")\n",
    "    params = []\n",
    "    for n_neighbors, min_dist in itertools.product(\n",
    "        kwargs[\"n_neighbors\"], kwargs[\"min_dist\"]\n",
    "    ):\n",
    "        filename = \"umap-n_neighbors_{}-min_dist_{}\".format(n_neighbors, min_dist)\n",
    "        out_path = get_path(\"layouts\", filename, **kwargs)\n",
    "        params.append(\n",
    "            {\n",
    "                \"n_neighbors\": n_neighbors,\n",
    "                \"min_dist\": min_dist,\n",
    "                FILE_NAME: filename,\n",
    "                \"out_path\": out_path,\n",
    "            }\n",
    "        )\n",
    "    # map each image's index to itself and create one copy of that map for each layout\n",
    "    relations_dict = {idx: idx for idx, _ in enumerate(v)}\n",
    "    # determine the subset of params that have already been computed\n",
    "    uncomputed_params = [i for i in params if not os.path.exists(i[\"out_path\"])]\n",
    "    # determine the filepath where this model will be saved\n",
    "    model_filename = \"umap-\" + str(abs(hash(kwargs[\"images\"])))\n",
    "    model_path = get_path(\"models\", model_filename, **kwargs).replace(\".json\", \".gz\")\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"models\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # load or create the model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        for i in uncomputed_params:\n",
    "            model.update(v, relations_dict.copy())\n",
    "        # after updating, we can read the results from the end of the updated model\n",
    "        for idx, i in enumerate(uncomputed_params):\n",
    "            embedding = z.embeddings_[len(uncomputed_params) - idx]\n",
    "            write_layout(i[\"out_path\"], embedding, **kwargs)\n",
    "    else:\n",
    "        model = AlignedUMAP(\n",
    "            n_neighbors=[i[\"n_neighbors\"] for i in uncomputed_params],\n",
    "            min_dist=[i[\"min_dist\"] for i in uncomputed_params],\n",
    "        )\n",
    "        # fit the model on the data\n",
    "        z = model.fit(\n",
    "            [v for _ in params], relations=[relations_dict for _ in params[1:]]\n",
    "        )\n",
    "        for idx, i in enumerate(params):\n",
    "            write_layout(i[\"out_path\"], z.embeddings_[idx], **kwargs)\n",
    "        # save the model\n",
    "        save_model(model, model_path)\n",
    "    # load the list of layout variants\n",
    "    l = []\n",
    "    for i in params:\n",
    "        l.append(\n",
    "            {\n",
    "                \"n_neighbors\": i[\"n_neighbors\"],\n",
    "                \"min_dist\": i[\"min_dist\"],\n",
    "                \"layout\": i[\"out_path\"],\n",
    "                \"jittered\": get_pointgrid_layout(\n",
    "                    i[\"out_path\"], i[FILE_NAME], **kwargs\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"variants\": l,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        params = model.get_params()\n",
    "        attributes_names = [\n",
    "            attr for attr in model.__dir__() if attr not in params and attr[0] != \"_\"\n",
    "        ]\n",
    "        attributes = {key: model.__getattribute__(key) for key in attributes_names}\n",
    "        attributes[\"embeddings_\"] = list(model.embeddings_)\n",
    "        for x in [\"fit\", \"fit_transform\", \"update\", \"get_params\", \"set_params\"]:\n",
    "            del attributes[x]\n",
    "        all_params = {\n",
    "            \"umap_params\": params,\n",
    "            \"umap_attributes\": {key: value for key, value in attributes.items()},\n",
    "        }\n",
    "        pickle.dump(all_params, open(path, \"wb\"))\n",
    "    except:\n",
    "        print(timestamp(), \"Could not save model\")\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    params = pickle.load(open(path, \"rb\"))\n",
    "    model = AlignedUMAP()\n",
    "    model.set_params(**params.get(\"umap_params\"))\n",
    "    for attr, value in params.get(\"umap_attributes\").items():\n",
    "        model.__setattr__(attr, value)\n",
    "    model.__setattr__(\n",
    "        \"embeddings_\", List(params.get(\"umap_attributes\").get(\"embeddings_\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_umap_model(**kwargs):\n",
    "    if cuml_ready:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            verbose=5,\n",
    "        )\n",
    "    else:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            metric=kwargs[\"metric\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            transform_seed=kwargs[\"seed\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rasterfairy_layout(**kwargs):\n",
    "    \"\"\"Get the x, y position of images passed through a rasterfairy projection\"\"\"\n",
    "    print(timestamp(), \"Creating rasterfairy layout\")\n",
    "    out_path = get_path(\"layouts\", \"rasterfairy\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    if umap.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create rasterfairy layout because data is not 2D\")\n",
    "        return None\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    try:\n",
    "        umap = coonswarp.rectifyCloud(\n",
    "            umap,  # stretch the distribution\n",
    "            perimeterSubdivisionSteps=4,\n",
    "            autoPerimeterOffset=False,\n",
    "            paddingScale=1.05,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Coonswarp rectification could not be performed\", exc)\n",
    "    pos = rasterfairy.transformPointCloud2D(umap)[0]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_alphabetic_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images in a grid projection\"\"\"\n",
    "    print(timestamp(), \"Creating grid layout\")\n",
    "    out_path = get_path(\"layouts\", \"grid\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    paths = kwargs[\"image_paths\"]\n",
    "    n = math.ceil(len(paths) ** (1 / 2))\n",
    "    l = []  # positions\n",
    "    for i, _ in enumerate(paths):\n",
    "        x = i % n\n",
    "        y = math.floor(i / n)\n",
    "        l.append([x, y])\n",
    "    z = np.array(l)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_pointgrid_layout(path, label, **kwargs):\n",
    "    \"\"\"Gridify the positions in `path` and return the path to this new layout\"\"\"\n",
    "    print(timestamp(), \"Creating {} pointgrid\".format(label))\n",
    "    out_path = get_path(\"layouts\", label + \"-jittered\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    arr = np.array(read_json(path, **kwargs))\n",
    "    if arr.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create pointgrid layout because data is not 2D\")\n",
    "        return None\n",
    "    z = align_points_to_grid(arr, fill=0.01)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_custom_layout(**kwargs):\n",
    "    out_path = get_path(\"layouts\", \"custom\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    if not kwargs.get(\"metadata\"):\n",
    "        return\n",
    "    found_coords = False\n",
    "    coords = []\n",
    "    for i in Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"]):\n",
    "        x = i.metadata.get(\"x\")\n",
    "        y = i.metadata.get(\"y\")\n",
    "        if x and y:\n",
    "            found_coords = True\n",
    "            coords.append([x, y])\n",
    "        else:\n",
    "            if found_coords:\n",
    "                print(\n",
    "                    timestamp(),\n",
    "                    \"Some images are missing coordinates; skipping custom layout\",\n",
    "                )\n",
    "    if not found_coords:\n",
    "        return\n",
    "    coords = np.array(coords).astype(np.float)\n",
    "    coords = (minmax_scale(coords) - 0.5) * 2\n",
    "    print(timestamp(), \"Creating custom layout\")\n",
    "    return {\n",
    "        \"layout\": write_layout(\n",
    "            out_path, coords.tolist(), scale=False, round=False, **kwargs\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_date_layout(cols=3, bin_units=\"years\", **kwargs):\n",
    "    \"\"\"\n",
    "    Get the x,y positions of input images based on their dates\n",
    "    @param int cols: the number of columns to plot for each bar\n",
    "    @param str bin_units: the temporal units to use when creating bins\n",
    "    \"\"\"\n",
    "    date_vals = [\n",
    "        kwargs[\"metadata\"][i].get(\"year\", False) for i in range(len(kwargs[\"metadata\"]))\n",
    "    ]\n",
    "    if not kwargs[\"metadata\"] or not any(date_vals):\n",
    "        return False\n",
    "    # if the data layouts have been cached, return them\n",
    "    positions_out_path = get_path(\"layouts\", \"timeline\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"timeline-labels\", **kwargs)\n",
    "    if (\n",
    "        os.path.exists(positions_out_path)\n",
    "        and os.path.exists(labels_out_path)\n",
    "        and kwargs[\"use_cache\"]\n",
    "    ):\n",
    "        return {\n",
    "            \"layout\": positions_out_path,\n",
    "            \"labels\": labels_out_path,\n",
    "        }\n",
    "    # date layout is not cached, so fetch dates and process\n",
    "    print(timestamp(), \"Creating date layout with {} columns\".format(cols))\n",
    "    datestrings = [i.metadata.get(\"year\", \"no_date\") for i in Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])]\n",
    "    dates = [datestring_to_date(i) for i in datestrings]\n",
    "    rounded_dates = [round_date(i, bin_units) for i in dates]\n",
    "    # create d[formatted_date] = [indices into datestrings of dates that round to formatted_date]\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(rounded_dates):\n",
    "        d[i].append(idx)\n",
    "    # determine the number of distinct grid positions in the x and y axes\n",
    "    n_coords_x = (cols + 1) * len(d)\n",
    "    n_coords_y = 1 + max([len(d[i]) for i in d]) // cols\n",
    "    if n_coords_y > n_coords_x:\n",
    "        return get_date_layout(cols=int(cols * 2), **kwargs)\n",
    "    # create a mesh of grid positions in clip space -1:1 given the time distribution\n",
    "    grid_x = (np.arange(0, n_coords_x) / (n_coords_x - 1)) * 2\n",
    "    grid_y = (np.arange(0, n_coords_y) / (n_coords_x - 1)) * 2\n",
    "    # divide each grid axis by half its max length to center at the origin 0,0\n",
    "    grid_x = grid_x - np.max(grid_x) / 2.0\n",
    "    grid_y = grid_y - np.max(grid_y) / 2.0\n",
    "    # make dates increase from left to right by sorting keys of d\n",
    "    d_keys = np.array(list(d.keys()))\n",
    "    seconds = np.array([date_to_seconds(dates[d[i][0]]) for i in d_keys])\n",
    "    d_keys = d_keys[np.argsort(seconds)]\n",
    "    # determine which images will fill which units of the grid established above\n",
    "    coords = np.zeros(\n",
    "        (len(datestrings), 2)\n",
    "    )  # 2D array with x, y clip-space coords of each date\n",
    "    for jdx, j in enumerate(d_keys):\n",
    "        for kdx, k in enumerate(d[j]):\n",
    "            x = jdx * (cols + 1) + (kdx % cols)\n",
    "            y = kdx // cols\n",
    "            coords[k] = [grid_x[x], grid_y[y]]\n",
    "    # find the positions of labels\n",
    "    label_positions = np.array(\n",
    "        [[grid_x[i * (cols + 1)], grid_y[0]] for i in range(len(d))]\n",
    "    )\n",
    "    # move the labels down in the y dimension by a grid unit\n",
    "    dx = grid_x[1] - grid_x[0]  # size of a single cell\n",
    "    label_positions[:, 1] = label_positions[:, 1] - dx\n",
    "    # quantize the label positions and label positions\n",
    "    image_positions = round_floats(coords)\n",
    "    label_positions = round_floats(label_positions.tolist())\n",
    "    # write and return the paths to the date based layout\n",
    "    return {\n",
    "        \"layout\": write_json(positions_out_path, image_positions, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": label_positions,\n",
    "                \"labels\": d_keys.tolist(),\n",
    "                \"cols\": cols,\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def datestring_to_date(datestring):\n",
    "    \"\"\"\n",
    "    Given a string representing a date return a datetime object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parse_date(\n",
    "            str(datestring), fuzzy=True, default=datetime.datetime(9999, 1, 1)\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Could not parse datestring {}\".format(datestring))\n",
    "        return datestring\n",
    "\n",
    "\n",
    "def date_to_seconds(date):\n",
    "    \"\"\"\n",
    "    Given a datetime object return an integer representation for that datetime\n",
    "    \"\"\"\n",
    "    if isinstance(date, datetime.datetime):\n",
    "        return (date - datetime.datetime.today()).total_seconds()\n",
    "    else:\n",
    "        return -float(\"inf\")\n",
    "\n",
    "\n",
    "def round_date(date, unit):\n",
    "    \"\"\"\n",
    "    Return `date` truncated to the temporal unit specified in `units`\n",
    "    \"\"\"\n",
    "    if not isinstance(date, datetime.datetime):\n",
    "        return \"no_date\"\n",
    "    formatted = date.strftime(\"%d %B %Y -- %X\")\n",
    "    if unit in set([\"seconds\", \"minutes\", \"hours\"]):\n",
    "        date = formatted.split(\"--\")[1].strip()\n",
    "        if unit == \"seconds\":\n",
    "            date = date\n",
    "        elif unit == \"minutes\":\n",
    "            date = \":\".join(d.split(\":\")[:-1]) + \":00\"\n",
    "        elif unit == \"hours\":\n",
    "            date = date.split(\":\")[0] + \":00:00\"\n",
    "    elif unit in set([\"days\", \"months\", \"years\", \"decades\", \"centuries\"]):\n",
    "        date = formatted.split(\"--\")[0].strip()\n",
    "        if unit == \"days\":\n",
    "            date = date\n",
    "        elif unit == \"months\":\n",
    "            date = \" \".join(date.split()[1:])\n",
    "        elif unit == \"years\":\n",
    "            date = date.split()[-1]\n",
    "        elif unit == \"decades\":\n",
    "            date = str(int(date.split()[-1]) // 10) + \"0\"\n",
    "        elif unit == \"centuries\":\n",
    "            date = str(int(date.split()[-1]) // 100) + \"00\"\n",
    "    return date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_categorical_layout(null_category=\"Other\", margin=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a numpy array with shape (n_points, 2) with the point\n",
    "    positions of observations in box regions determined by\n",
    "    each point's category metadata attribute (if applicable)\n",
    "    \"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return False\n",
    "    # determine the out path and return from cache if possible\n",
    "    out_path = get_path(\"layouts\", \"categorical\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"categorical-labels\", **kwargs)\n",
    "    # accumulate d[category] = [indices of points with category]\n",
    "    categories = [i.get(\"category\", None) for i in kwargs[\"metadata\"]]\n",
    "    if not any(categories) or len(set(categories) - set([None])) == 1:\n",
    "        return False\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(categories):\n",
    "        d[i].append(idx)\n",
    "    # store the number of observations in each group\n",
    "    keys_and_counts = [{\"key\": i, \"count\": len(d[i])} for i in d]\n",
    "    keys_and_counts.sort(key=operator.itemgetter(\"count\"), reverse=True)\n",
    "    # get the box layout then subdivide into discrete points\n",
    "    boxes = get_categorical_boxes([i[\"count\"] for i in keys_and_counts], margin=margin)\n",
    "    points = get_categorical_points(boxes)\n",
    "    # sort the points into the order of the observations in the metadata\n",
    "    counts = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    offsets = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    for idx, i in enumerate(keys_and_counts):\n",
    "        offsets[i[\"key\"]] += sum([j[\"count\"] for j in keys_and_counts[:idx]])\n",
    "    sorted_points = []\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        category = i.metadata.get(\"category\", null_category)\n",
    "        sorted_points.append(points[offsets[category] + counts[category]])\n",
    "        counts[category] += 1\n",
    "    sorted_points = np.array(sorted_points)\n",
    "    # add to the sorted points the anchors for the text labels for each group\n",
    "    text_anchors = np.array([[i.x, i.y - margin / 2] for i in boxes])\n",
    "    # add the anchors to the points - these will be removed after the points are projected\n",
    "    sorted_points = np.vstack([sorted_points, text_anchors])\n",
    "    # scale -1:1 using the largest axis as the scaling metric\n",
    "    _max = np.max(sorted_points)\n",
    "    for i in range(2):\n",
    "        _min = np.min(sorted_points[:, i])\n",
    "        sorted_points[:, i] -= _min\n",
    "        sorted_points[:, i] /= _max - _min\n",
    "        sorted_points[:, i] -= np.max(sorted_points[:, i]) / 2\n",
    "        sorted_points[:, i] *= 2\n",
    "    # separate out the sorted points and text positions\n",
    "    text_anchors = sorted_points[-len(text_anchors) :]\n",
    "    sorted_points = sorted_points[: -len(text_anchors)]\n",
    "    z = round_floats(sorted_points.tolist())\n",
    "    return {\n",
    "        \"layout\": write_json(out_path, z, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": round_floats(text_anchors.tolist()),\n",
    "                \"labels\": [i[\"key\"] for i in keys_and_counts],\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_categorical_boxes(group_counts, margin=2):\n",
    "    \"\"\"\n",
    "    @arg [int] group_counts: counts of the number of images in each\n",
    "      distinct level within the metadata's caetgories\n",
    "    @kwarg int margin: space between boxes in the 2D layout\n",
    "    @returns [Box] an array of Box() objects; one per level in `group_counts`\n",
    "    \"\"\"\n",
    "    group_counts = sorted(group_counts, reverse=True)\n",
    "    boxes = []\n",
    "    for i in group_counts:\n",
    "        w = h = math.ceil(i ** (1 / 2))\n",
    "        boxes.append(Box(i, w, h, None, None))\n",
    "    # find the position along x axis where we want to create a break\n",
    "    wrap = math.floor(sum([i.cells for i in boxes]) ** (1 / 2)) - (2 * margin)\n",
    "    # find the valid positions on the y axis\n",
    "    y = margin\n",
    "    y_spots = []\n",
    "    for i in boxes:\n",
    "        if (y + i.h + margin) <= wrap:\n",
    "            y_spots.append(y)\n",
    "            y += i.h + margin\n",
    "        else:\n",
    "            y_spots.append(y)\n",
    "            break\n",
    "    # get a list of lists where sublists contain elements at the same y position\n",
    "    y_spot_index = 0\n",
    "    for i in boxes:\n",
    "        # find the y position\n",
    "        y = y_spots[y_spot_index]\n",
    "        # find members with this y position\n",
    "        row_members = [j.x + j.w for j in boxes if j.y == y]\n",
    "        # assign the y position\n",
    "        i.y = y\n",
    "        y_spot_index = (y_spot_index + 1) % len(y_spots)\n",
    "        # assign the x position\n",
    "        i.x = max(row_members) + margin if row_members else margin\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_categorical_points(arr, unit_size=None):\n",
    "    \"\"\"Given an array of Box() objects, return a 2D distribution with shape (n_cells, 2)\"\"\"\n",
    "    points_arr = []\n",
    "    for i in arr:\n",
    "        area = i.w * i.h\n",
    "        per_unit = (area / i.cells) ** (1 / 2)\n",
    "        x_units = math.ceil(i.w / per_unit)\n",
    "        y_units = math.ceil(i.h / per_unit)\n",
    "        if not unit_size:\n",
    "            unit_size = min(i.w / x_units, i.h / y_units)\n",
    "        for j in range(i.cells):\n",
    "            x = j % x_units\n",
    "            y = j // x_units\n",
    "            points_arr.append(\n",
    "                [\n",
    "                    i.x + x * unit_size,\n",
    "                    i.y + y * unit_size,\n",
    "                ]\n",
    "            )\n",
    "    return np.array(points_arr)\n",
    "\n",
    "\n",
    "class Box:\n",
    "    \"\"\"Store the width, height, and x, y coords of a box\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.cells = args[0]\n",
    "        self.w = args[1]\n",
    "        self.h = args[2]\n",
    "        self.x = None if len(args) < 4 else args[3]\n",
    "        self.y = None if len(args) < 5 else args[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_geographic_layout(**kwargs):\n",
    "    \"\"\"Return a 2D array of image positions corresponding to lat, lng coordinates\"\"\"\n",
    "    out_path = get_path(\"layouts\", \"geographic\", **kwargs)\n",
    "    l = []\n",
    "    coords = False\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        lat = float(i.metadata.get(\"lat\", 0)) / 180\n",
    "        lng = (\n",
    "            float(i.metadata.get(\"lng\", 0)) / 180\n",
    "        )  # the plot draws longitude twice as tall as latitude\n",
    "        if lat or lng:\n",
    "            coords = True\n",
    "        l.append([lng, lat])\n",
    "    if coords:\n",
    "        print(timestamp(), \"Creating geographic layout\")\n",
    "        if kwargs[\"geojson\"]:\n",
    "            process_geojson(kwargs[\"geojson\"])\n",
    "        return {\"layout\": write_layout(out_path, l, scale=False, **kwargs)}\n",
    "    elif kwargs[\"geojson\"]:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \"GeoJSON is only processed if you also provide lat/lng coordinates for your images in a metadata file!\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path):\n",
    "    \"\"\"Given a GeoJSON filepath, write a minimal JSON output in lat lng coordinates\"\"\"\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson = json.load(f)\n",
    "    l = []\n",
    "    for i in geojson:\n",
    "        if isinstance(i, dict):\n",
    "            for j in i.get(\"coordinates\", []):\n",
    "                for k in j:\n",
    "                    l.append(k)\n",
    "    with open(\n",
    "        os.path.join(\"output\", \"assets\", \"json\", \"geographic-features.json\"), \"w\"\n",
    "    ) as out:\n",
    "        json.dump(l, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_path(*args, **kwargs):\n",
    "    \"\"\"Return the path to a JSON file with conditional gz extension\"\"\"\n",
    "    sub_dir, filename = args\n",
    "    out_dir = join(kwargs[\"out_dir\"], sub_dir) if sub_dir else kwargs[\"out_dir\"]\n",
    "    if kwargs.get(\"add_hash\", True):\n",
    "        filename += \"-\" + kwargs[\"plot_id\"]\n",
    "    path = join(out_dir, filename + \".json\")\n",
    "    return path + \".gz\" if kwargs.get(\"gzip\", False) else path\n",
    "\n",
    "\n",
    "def write_layout(path, obj, **kwargs):\n",
    "    \"\"\"Write layout json `obj` to disk and return the path to the saved file\"\"\"\n",
    "    if kwargs.get(\"scale\", True) != False:\n",
    "        obj = (minmax_scale(obj) - 0.5) * 2  # scale -1:1\n",
    "    if kwargs.get(\"round\", True) != False:\n",
    "        obj = round_floats(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        obj = obj.tolist()\n",
    "    return write_json(path, obj, **kwargs)\n",
    "\n",
    "\n",
    "def round_floats(obj, digits=5):\n",
    "    \"\"\"Return 2D array obj with rounded float precision\"\"\"\n",
    "    return [[round(float(j), digits) for j in i] for i in obj]\n",
    "\n",
    "\n",
    "def write_json(path, obj, **kwargs):\n",
    "    \"\"\"Write json object `obj` to disk and return the path to that file\"\"\"\n",
    "    out_dir, filename = os.path.split(path)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"w\") as out:\n",
    "            out.write(json.dumps(obj, indent=4).encode(kwargs[\"encoding\"]))\n",
    "        return path\n",
    "    else:\n",
    "        with open(path, \"w\") as out:\n",
    "            json.dump(obj, out, indent=4)\n",
    "        return path\n",
    "\n",
    "\n",
    "def read_json(path, **kwargs):\n",
    "    \"\"\"Read and return the json object written by the current process at `path`\"\"\"\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"r\") as f:\n",
    "            return json.loads(f.read().decode(kwargs[\"encoding\"]))\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_hotspots(layouts={}, use_high_dimensional_vectors=True, **kwargs):\n",
    "    \"\"\"Return the stable clusters from the condensed tree of connected components from the density graph\"\"\"\n",
    "    print(timestamp(), \"Clustering data with {}\".format(cluster_method))\n",
    "    if use_high_dimensional_vectors:\n",
    "        vecs = kwargs[\"vecs\"]\n",
    "    else:\n",
    "        vecs = read_json(layouts[\"umap\"][\"variants\"][0][\"layout\"], **kwargs)\n",
    "    model = get_cluster_model(**kwargs)\n",
    "    z = model.fit(vecs)\n",
    "    # create a map from cluster label to image indices in cluster\n",
    "    d = defaultdict(lambda: defaultdict(list))\n",
    "    for idx, i in enumerate(z.labels_):\n",
    "        if i != -1:\n",
    "            d[i][\"images\"].append(idx)\n",
    "            d[i][\"img\"] = clean_filename(kwargs[\"image_paths\"][idx])\n",
    "            d[i][\"layout\"] = \"inception_vectors\"\n",
    "    # remove massive clusters\n",
    "    deletable = []\n",
    "    for i in d:\n",
    "        # find percent of images in cluster\n",
    "        image_percent = len(d[i][\"images\"]) / len(vecs)\n",
    "        # determine if image or area percent is too large\n",
    "        if image_percent > 0.5:\n",
    "            deletable.append(i)\n",
    "    for i in deletable:\n",
    "        del d[i]\n",
    "    # sort the clusers by size and then label the clusters\n",
    "    clusters = d.values()\n",
    "    clusters = sorted(clusters, key=lambda i: len(i[\"images\"]), reverse=True)\n",
    "    for idx, i in enumerate(clusters):\n",
    "        i[\"label\"] = \"Cluster {}\".format(idx + 1)\n",
    "    # slice off the first `max_clusters`\n",
    "    clusters = clusters[: kwargs[\"max_clusters\"]]\n",
    "    # save the hotspots to disk and return the path to the saved json\n",
    "    print(timestamp(), \"Found\", len(clusters), \"hotspots\")\n",
    "    return write_json(get_path(\"hotspots\", \"hotspot\", **kwargs), clusters, **kwargs)\n",
    "\n",
    "\n",
    "def get_cluster_model(**kwargs):\n",
    "    \"\"\"Return a model with .fit() method that can be used to cluster input vectors\"\"\"\n",
    "    config = {\n",
    "        \"core_dist_n_jobs\": multiprocessing.cpu_count(),\n",
    "        \"min_cluster_size\": kwargs[\"min_cluster_size\"],\n",
    "        \"cluster_selection_epsilon\": 0.01,\n",
    "        \"min_samples\": 1,\n",
    "        \"approx_min_span_tree\": False,\n",
    "    }\n",
    "    return HDBSCAN(**config)\n",
    "\n",
    "\n",
    "def get_heightmap(path, label, **kwargs):\n",
    "    \"\"\"Create a heightmap using the distribution of points stored at `path`\"\"\"\n",
    "\n",
    "    X = read_json(path, **kwargs)\n",
    "    if \"positions\" in X:\n",
    "        X = X[\"positions\"]\n",
    "    X = np.array(X)\n",
    "    if X.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create heightmap because data is not 2D\")\n",
    "        return\n",
    "    # create kernel density estimate of distribution X\n",
    "    nbins = 200\n",
    "    x, y = X.T\n",
    "    xi, yi = np.mgrid[x.min() : x.max() : nbins * 1j, y.min() : y.max() : nbins * 1j]\n",
    "    zi = kde.gaussian_kde(X.T)(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading=\"gouraud\", cmap=plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # save the plot\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"heightmaps\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_path = os.path.join(out_dir, label + \"-heightmap.png\")\n",
    "    plt.savefig(out_path, pad_inches=0)\n",
    "\n",
    "\n",
    "def write_images(**kwargs):\n",
    "    \"\"\"Write all originals and thumbs to the output dir\"\"\"\n",
    "    for i in Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"]):\n",
    "        filename = clean_filename(i.path)\n",
    "        # copy original for lightbox\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"originals\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        if not os.path.exists(out_path):\n",
    "            resized = i.resize_to_height(600)\n",
    "            resized = array_to_image(resized)\n",
    "            save_image(out_path, resized)\n",
    "        # copy thumb for lod texture\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"thumbs\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        img = array_to_image(i.resize_to_max(kwargs[\"lod_cell_height\"]))\n",
    "        save_image(out_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_version():\n",
    "    \"\"\"\n",
    "    Return the version of clipplot installed\n",
    "    Hardcoded for now\n",
    "    \"\"\"\n",
    "    # return pkg_resources.get_distribution(\"clipplot\").version\n",
    "    return \"0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Image:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.path = args[0]\n",
    "        self.filename = clean_filename(self.path)\n",
    "        self.original = load_image(self.path) \n",
    "        self.metadata = kwargs[\"metadata\"] if kwargs[\"metadata\"] else {}\n",
    "\n",
    "    def resize_to_max(self, n: int) -> np.array:\n",
    "        \"\"\"Resize self.original so its longest side has n pixels (maintain proportion).\n",
    "\n",
    "        Args:\n",
    "            n (int): maximum pixel length\n",
    "\n",
    "        Returns:\n",
    "            np.array: re-sized to n length\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        if w > h:\n",
    "            size = (n, int(n * h / w))\n",
    "        else:\n",
    "            size = (int(n * w / h), n)\n",
    "    \n",
    "        return image_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_height(self, height: int) -> np.array:\n",
    "        \"\"\"Resize self.original into an image with height h and proportional width.\n",
    "\n",
    "        Args:\n",
    "            height (int): New height to resize to\n",
    "\n",
    "        Returns:\n",
    "            np.array: re-sized to height\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        if (w / h * height) < 1:\n",
    "            resizedwidth = 1\n",
    "        else:\n",
    "            resizedwidth = int(w / h * height)\n",
    "        size = (resizedwidth, height)\n",
    "        return image_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_square(self, n: int, center: Optional[bool] = False) -> np.array:\n",
    "        \"\"\"Resize self.original to an image with nxn pixels (maintain proportion)\n",
    "        if center, center the colored pixels in the square, else left align.\n",
    "\n",
    "        Args:\n",
    "            n (int)\n",
    "\n",
    "        Notes:\n",
    "            Function not being used\n",
    "        \"\"\"\n",
    "        a = self.resize_to_max(n)\n",
    "        h, w, c = a.shape\n",
    "        pad_lr = int((n - w) / 2)  # left right pad\n",
    "        pad_tb = int((n - h) / 2)  # top bottom pad\n",
    "        b = np.zeros((n, n, 3))\n",
    "        if center:\n",
    "            b[pad_tb : pad_tb + h, pad_lr : pad_lr + w, :] = a\n",
    "        else:\n",
    "            b[:h, :w, :] = a\n",
    "        return b\n",
    "\n",
    "    def valid(self, lod_cell_height: int, oblong_ratio: Union[int,float]) -> tuple[bool, str]:\n",
    "        \"\"\"Validate that image can be opened and loaded correctly.\n",
    "\n",
    "        Args:\n",
    "            lod_cell_height (int):\n",
    "            oblong_ratio (int|float): atlas_size/cell_size ratio\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pass,msg]:\n",
    "                pass (bool): True if passed validation\n",
    "                msg (str): Reason why validation failed \n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        # remove images with 0 height or width when resized to lod height\n",
    "        if (h == 0) or (w == 0):\n",
    "            return False, f\"Skipping {self.path} because it contains 0 height or width\"\n",
    "        # remove images that have 0 height or width when resized\n",
    "        try:\n",
    "            resized = self.resize_to_max(lod_cell_height)\n",
    "        except ValueError:\n",
    "            return False, f\"Skipping {self.path} because it contains 0 height or width when resized\"\n",
    "        except OSError:\n",
    "            return False, f\"Skipping {self.path} because it could not be resized\"\n",
    "        # remove images that are too wide for the atlas\n",
    "        if (w / h) > (oblong_ratio):\n",
    "            return False, f\"Skipping {self.path} because its dimensions are oblong\"\n",
    "\n",
    "        return True, \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def stream_images(image_paths: List[str], metadata: Optional[List[dict]] = None) -> 'Image':\n",
    "        \"\"\"Read in all images from args[0], a list of image paths\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list[str]): list of image locations\n",
    "            metadata (Optional[list[dist]]): metadata for each image\n",
    "        \n",
    "        Returns:\n",
    "            yields Image instance\n",
    "\n",
    "        Notes:\n",
    "            image is matched to metadata by index location\n",
    "                Matching by key would be better\n",
    "        \"\"\"\n",
    "        for idx, imgPath in enumerate(image_paths):\n",
    "            try:\n",
    "                meta = None\n",
    "                if metadata and metadata[idx]:\n",
    "                    meta = metadata[idx]\n",
    "                yield Image(imgPath, metadata=meta)\n",
    "            except Exception as exc:\n",
    "                print(timestamp(), \"Image\", imgPath, \"could not be processed --\", exc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the command-line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse():\n",
    "    \"\"\"Read command line args and begin data processing\"\"\"\n",
    "    description = \"Create the data required to create a clipplot viewer\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--images\",\n",
    "        \"-i\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"images\"],\n",
    "        help=\"path to a glob of images to process\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metadata\",\n",
    "        \"-m\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"meta_dir\"],\n",
    "        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_images\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"max_images\"],\n",
    "        help=\"maximum number of images to process from the input glob\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cache\",\n",
    "        type=bool,\n",
    "        default=DEFAULTS[\"use_cache\"],\n",
    "        help=\"given inputs identical to prior inputs, load outputs from cache\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoding\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"encoding\"],\n",
    "        help=\"the encoding of input metadata\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_cluster_size\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"min_cluster_size\"],\n",
    "        help=\"the minimum number of images in a cluster\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_clusters\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"max_clusters\"],\n",
    "        help=\"the maximum number of clusters to return\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_dir\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"out_dir\"],\n",
    "        help=\"the directory to which outputs will be saved\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cell_size\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"cell_size\"],\n",
    "        help=\"the size of atlas cells in px\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_neighbors\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_neighbors\"],\n",
    "        help=\"the n_neighbors arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_dist\",\n",
    "        nargs=\"+\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"min_dist\"],\n",
    "        help=\"the min_dist arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_components\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_components\"],\n",
    "        help=\"the n_components argument for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"metric\"],\n",
    "        help=\"the metric argument for umap\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pointgrid_fill\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"pointgrid_fill\"],\n",
    "        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--copy_web_only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"update ./output/assets without reprocessing data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_size\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"min_size\"],\n",
    "        help=\"min size of cropped images\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gzip\", action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\",\n",
    "        action=\"store_true\",\n",
    "        help=\"shuffle the input images before data processing begins\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plot_id\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"plot_id\"],\n",
    "        help=\"unique id for a plot; useful for resuming processing on a started plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=DEFAULTS[\"seed\"], help=\"seed for random processes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_clusters\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_clusters\"],\n",
    "        help=\"number of clusters to use when clustering with kmeans\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--geojson\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"geojson\"],\n",
    "        help=\"path to a GeoJSON file with shapes to be rendered on a map\",\n",
    "    )\n",
    "    config = DEFAULTS.copy()\n",
    "    if in_ipython():\n",
    "        config.update(vars(parser.parse_args({})))\n",
    "    else:\n",
    "        config.update(vars(parser.parse_args()))\n",
    "\n",
    "    return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing some Keras functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import io\n",
    "from PIL import Image as pil_image\n",
    "\n",
    "# The type of float to use throughout a session.\n",
    "FLOATX = \"float32\"\n",
    "\n",
    "def load_image(path: str) -> pil_image.Image:\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = pil_image.open(io.BytesIO(f.read()))\n",
    "\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_to_array(img: pil_image.Image) -> np.array:\n",
    "    \"\"\"Converts a PIL Image instance to a Numpy array.\n",
    "\n",
    "    Args:\n",
    "        img: Input PIL Image instance.\n",
    "\n",
    "    Returns:\n",
    "        A 3D Numpy array.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if invalid `img` or `data_format` is passed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Numpy array x has format (height, width, channel)\n",
    "    # or (channel, height, width)\n",
    "    # but original PIL image has format (width, height, channel)\n",
    "    x = np.asarray(img, dtype=FLOATX)\n",
    "    if len(x.shape) not in [2, 3]:\n",
    "        raise ValueError(f\"Unsupported image shape: {x.shape}\")\n",
    "\n",
    "    if len(x.shape) == 2:\n",
    "        x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def array_to_image(x: np.array)-> pil_image.Image:\n",
    "    \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n",
    "\n",
    "    Args:\n",
    "        x: Input data, in any form that can be converted to a Numpy array.\n",
    "\n",
    "    Returns:\n",
    "        A PIL Image instance.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if invalid `x` or `data_format` is passed.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=FLOATX)\n",
    "    if x.ndim != 3:\n",
    "        raise ValueError(\n",
    "            \"Expected image array to have rank 3 (single image). \"\n",
    "            f\"Got array with shape: {x.shape}\"\n",
    "        )\n",
    "\n",
    "    # Original Numpy array x has format (height, width, channel)\n",
    "    # or (channel, height, width)\n",
    "    # but target PIL image has format (width, height, channel)\n",
    "\n",
    "    x = x - np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    if x_max != 0:\n",
    "        x /= x_max\n",
    "    x *= 255\n",
    "\n",
    "    if x.shape[2] == 4:  # RGBA\n",
    "        return pil_image.fromarray(x.astype(\"uint8\"), \"RGBA\")\n",
    "    elif x.shape[2] == 3:  # RGB\n",
    "        return pil_image.fromarray(x.astype(\"uint8\"), \"RGB\")\n",
    "    elif x.shape[2] == 1:  # grayscale\n",
    "        if np.max(x) > 255:\n",
    "            # 32-bit signed integer grayscale image. PIL mode \"I\"\n",
    "            return pil_image.fromarray(x[:, :, 0].astype(\"int32\"), \"I\")\n",
    "        return pil_image.fromarray(x[:, :, 0].astype(\"uint8\"), \"L\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported channel number: {x.shape[2]}\")\n",
    "\n",
    "\n",
    "def save_image(path: str, x: np.array) -> None:\n",
    "    \"\"\"Saves an image stored as a Numpy array to a path or file object.\n",
    "\n",
    "    Args:\n",
    "        path: Path or file object.\n",
    "        x: Numpy array.\n",
    "    \"\"\"\n",
    "    img = array_to_image(x)\n",
    "    img.save(path,format=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carlo's Test Functions\n",
    "# Need to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_iiif(config):\n",
    "    test_images = copy_root_dir/\"tests/IIIF_examples/iif_example.txt\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    # meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    # config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly_duplicate(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs_duplicates/**/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly_missing_meta(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/meta_missing_filename.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_no_meta_dir(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = parse()\n",
    "    copy_root_dir = get_clip_plot_root()\n",
    "\n",
    "    if in_ipython() and config[\"images\"] == None:\n",
    "        # at least for now, this means we're in testing mode.\n",
    "        # TODO: pass explicit \"test_mode\" flag to argparse\n",
    "        config[\"test_mode\"] = True\n",
    "        config = test_butterfly(config)\n",
    "        \n",
    "\n",
    "    process_images(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@call_parse\n",
    "def project_imgs(images:Param(type=str,\n",
    "                        help=\"path to a glob of images to process\"\n",
    "                        )=DEFAULTS[\"images\"],\n",
    "                metadata:Param(type=str,\n",
    "                        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\"\n",
    "                        )=DEFAULTS[\"meta_dir\"],\n",
    "                max_images:Param(type=int,\n",
    "                        help=\"maximum number of images to process from the input glob\"\n",
    "                        )=DEFAULTS[\"max_images\"],\n",
    "                use_cache:Param(type=bool,\n",
    "                        help=\"given inputs identical to prior inputs, load outputs from cache\"\n",
    "                        )=DEFAULTS[\"use_cache\"],\n",
    "                encoding:Param(type=str,\n",
    "                        help=\"the encoding of input metadata\"\n",
    "                        )=DEFAULTS[\"encoding\"],\n",
    "                min_cluster_size:Param(type=int,\n",
    "                        help=\"the minimum number of images in a cluster\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"min_cluster_size\"],\n",
    "                max_clusters:Param(type=int,\n",
    "                        help=\"the maximum number of clusters to return\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"max_clusters\"],\n",
    "                out_dir:Param(type=str,\n",
    "                        help=\"the directory to which outputs will be saved\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"out_dir\"],\n",
    "                cell_size:Param(type=int,\n",
    "                        help=\"the size of atlas cells in px\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"cell_size\"],\n",
    "                n_neighbors:Param(type=int,\n",
    "                        nargs=\"+\",\n",
    "                        help=\"the n_neighbors arguments for UMAP\"\n",
    "                        )=DEFAULTS[\"n_neighbors\"],\n",
    "                min_dist:Param(type=float,\n",
    "                        nargs=\"+\",\n",
    "                        help=\"the min_dist arguments for UMAP\"\n",
    "                        )=DEFAULTS[\"min_dist\"],\n",
    "                n_components:Param(type=int,\n",
    "                        help=\"the n_components argument for UMAP\"\n",
    "                        )=DEFAULTS[\"n_components\"],\n",
    "                metric:Param(type=str,\n",
    "                        help=\"the metric argument for umap\"\n",
    "                        )=DEFAULTS[\"metric\"],\n",
    "                pointgrid_fill:Param(type=float,\n",
    "                        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\"\n",
    "                        )=DEFAULTS[\"pointgrid_fill\"],\n",
    "                copy_web_only:Param(type=bool,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"update ./output/assets without reprocessing data\"\n",
    "                        )=False,\n",
    "                min_size:Param(type=float,\n",
    "                        help=\"min size of cropped images\"\n",
    "                        )=DEFAULTS[\"min_size\"],\n",
    "                gzip:Param(type=bool,\n",
    "                        action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "                        )=False,\n",
    "                shuffle:Param(type=bool,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"shuffle the input images before data processing begins\"\n",
    "                        )=False,\n",
    "                plot_id:Param(type=str,\n",
    "                        help=\"unique id for a plot; useful for resuming processing on a started plot\"\n",
    "                        )=DEFAULTS[\"plot_id\"],\n",
    "                seed:Param(type=int, help=\"seed for random processes\"\n",
    "                           )=DEFAULTS[\"seed\"],\n",
    "                n_clusters:Param(type=int,\n",
    "                        help=\"number of clusters if using kmeans\"\n",
    "                        )=DEFAULTS[\"n_clusters\"],\n",
    "                geojson:Param(type=str,\n",
    "                        help=\"path to a GeoJSON file with shapes to be rendered on a map\"\n",
    "                        )=DEFAULTS[\"geojson\"]\n",
    "                ):\n",
    "                \"Convert a folder of images into a clip-plot visualization\"\n",
    "\n",
    "                config = parse()\n",
    "                copy_root_dir = get_clip_plot_root()\n",
    "\n",
    "                if in_ipython() and config[\"images\"] == None:\n",
    "                        print(\"we're in ipython\")\n",
    "                        # at least for now, this means we're in testing mode.\n",
    "                        # TODO: pass explicit \"test_mode\" flag\n",
    "                        config[\"test_mode\"] = True\n",
    "                        test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "                        test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "                        meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "                        if Path(test_out_dir).exists(): rmtree(test_out_dir)\n",
    "\n",
    "                        config[\"images\"] = test_images.as_posix()\n",
    "                        config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "                        config[\"metadata\"] = meta_dir.as_posix()\n",
    "\n",
    "                process_images(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# try running the replacement for __main__ method\n",
    "\n",
    "project_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
