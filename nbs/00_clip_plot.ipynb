{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clip_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup\n",
    "\n",
    "### Unconditional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import division\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from fastcore.all import *\n",
    "\n",
    "from clip_plot import utils\n",
    "from clip_plot.layouts import get_layouts\n",
    "from clip_plot.utils import clean_filename, timestamp\n",
    "from clip_plot.utils import  get_version, FILE_NAME\n",
    "from clip_plot.embeddings import get_inception_vectors\n",
    "from clip_plot.metadata import get_manifest, write_metadata, get_metadata_list\n",
    "from clip_plot.images import PillLoadTruncated, save_image, write_images, Image, get_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from shutil import rmtree\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from typing import Optional, List, Union, Tuple\n",
    "import uuid\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing imports\n",
    "\n",
    "Note that I have removed the \"copy-web-only\" conditional import path for now\n",
    "\n",
    "`nbdev` does not like cells to have cells to have code and imports in the same cell:\n",
    "\n",
    "https://nbdev.fast.ai/getting_started.html#q-what-is-the-warning-found-a-cell-containing-mix-of-imports-and-computations.-please-use-separate-cells\n",
    "\n",
    "I think this may mean we don't get to do conditional imports. If we find a code path that really should have conditional imports, we can see if there is a workaround. For now, I don't feel \"copy web only\" is a very important functionality to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "\n",
    "\n",
    "# Keras imports\n",
    "# from tensorflow.keras.preprocessing.image import save_img, img_to_array, array_to_img\n",
    "# from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "# from tensorflow.keras.applications import InceptionV3, imagenet_utils # imagenet_utils not being used\n",
    "# from tensorflow.keras.preprocessing.image import load_img\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow import compat\n",
    "\n",
    "# Suppress annoying info and warning logs from tensorflow\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional install imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"images\": None,\n",
    "    \"meta_dir\": None,\n",
    "    \"out_dir\": \"output\",\n",
    "    \"max_images\": None,\n",
    "    \"use_cache\": True,\n",
    "    \"encoding\": \"utf8\",\n",
    "    \"min_cluster_size\": 20,\n",
    "    \"max_clusters\": 10,\n",
    "    \"atlas_size\": 2048,\n",
    "    \"cell_size\": 32,\n",
    "    \"lod_cell_height\": 128, # Why is not in parser?\n",
    "    \"n_neighbors\": [15],\n",
    "    \"min_dist\": [0.01],\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"correlation\",\n",
    "    \"pointgrid_fill\": 0.05,\n",
    "    \"gzip\": False,\n",
    "    \"min_size\": 100,\n",
    "    \"min_score\": 0.3,\n",
    "    \"min_vertices\": 18,\n",
    "    \"plot_id\": str(uuid.uuid1()),\n",
    "    \"seed\": 24,\n",
    "    \"n_clusters\": 12,\n",
    "    \"geojson\": None,\n",
    "}\n",
    "\n",
    "# handle truncated images in PIL (managed by Pillow)\n",
    "PillLoadTruncated  = True\n",
    "\n",
    "\"\"\"\n",
    "NB: Keras Image class objects return image.size as w,h\n",
    "    Numpy array representations of images return image.shape as h,w,c\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entry\n",
    "\n",
    "`process_images` will kick off all the main functions for the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_clip_plot_root() -> Path:\n",
    "    # ipython doesn't have __file__ attribute\n",
    "    if in_ipython():\n",
    "        return Path(utils.__file__).parents[1]\n",
    "    else:\n",
    "        print(__file__)\n",
    "        return Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "copy_root_dir = get_clip_plot_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process_images(**kwargs):\n",
    "    \"\"\"Main method for processing user images and metadata\"\"\"\n",
    "    kwargs = preprocess_kwargs(**kwargs)\n",
    "\n",
    "    copy_web_assets(out_dir=kwargs['out_dir'])\n",
    "    if kwargs[\"copy_web_only\"]:\n",
    "        print(timestamp(), \"Done!\")\n",
    "        sys.exit()\n",
    "    \n",
    "    np.random.seed(kwargs[\"seed\"])\n",
    "    compat.v1.set_random_seed(kwargs[\"seed\"])\n",
    "    kwargs[\"out_dir\"] = os.path.join(kwargs[\"out_dir\"], \"data\")\n",
    "    kwargs[\"image_paths\"], kwargs[\"metadata\"] = filter_images(**kwargs)\n",
    "    write_metadata(**kwargs)\n",
    "    \n",
    "    kwargs[\"atlas_dir\"] = get_atlas_data(**kwargs)\n",
    "    kwargs[\"vecs\"] = get_inception_vectors(**kwargs)\n",
    "    get_manifest(**kwargs)\n",
    "    write_images(**kwargs)\n",
    "    print(timestamp(), \"Done!\")\n",
    "\n",
    "\n",
    "def preprocess_kwargs(**kwargs):\n",
    "    \"\"\"Preprocess incoming key word arguments\n",
    "    Converts n_neighbors and min_dist arguments into a list\n",
    "\n",
    "    Args:\n",
    "        n_neighbors (int, list[int], default = [15])\n",
    "        min_dist (int, list[int], default = [0.01])\n",
    "\n",
    "    Notes:\n",
    "        Convenient hook for preprocessing arguments\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in [\"n_neighbors\", \"min_dist\"]:\n",
    "        if not isinstance(kwargs[i], list):\n",
    "            kwargs[i] = [kwargs[i]]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def copy_web_assets(out_dir: str) -> None:\n",
    "    \"\"\"Copy the /web directory from the clipplot source to the users cwd.\n",
    "    Copies version number into assets.\n",
    "    \n",
    "    Args: \n",
    "        out_dir (str): directory to copy web assets\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    copy_root_dir = get_clip_plot_root()\n",
    "    src = copy_root_dir / \"clip_plot/web\"\n",
    "\n",
    "    # resolve will handle cases with ../ in the path\n",
    "    dest = Path.cwd() / Path(out_dir).resolve()\n",
    "    utils.copytree_agnostic(src.as_posix(), dest.as_posix())\n",
    "\n",
    "    # write version numbers into output\n",
    "    for i in [\"index.html\", os.path.join(\"assets\", \"js\", \"tsne.js\")]:\n",
    "        path = os.path.join(dest, i)\n",
    "        with open(path, \"r\") as f:\n",
    "            f = f.read().replace(\"VERSION_NUMBER\", get_version())\n",
    "            with open(path, \"w\") as out:\n",
    "                out.write(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_images(**kwargs):\n",
    "    \"\"\"Main method for filtering images given user metadata (if provided)\n",
    "\n",
    "    -Validate image:\n",
    "        Loading (done by stream_images and Images)\n",
    "        Size\n",
    "        resizing\n",
    "        oblong\n",
    "\n",
    "    -Compare against metadata\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "        shuffle (Optional[bool], default = False) \n",
    "\n",
    "    Returns:\n",
    "        images (list[str])\n",
    "        metadata (list[dict])\n",
    "\n",
    "    Notes:\n",
    "        Assumes 'filename' is provided in metadata\n",
    "        Convoluted compiling of metadata\n",
    "        Should All Validation should belong to Image class?\n",
    "        Need to split function\n",
    "    \"\"\"\n",
    "    # validate that input image names are unique\n",
    "    image_paths = get_image_paths(images=kwargs[\"images\"], out_dir=kwargs[\"out_dir\"])\n",
    "    image_names = list(map(clean_filename,image_paths))\n",
    "    duplicates = set([x for x in image_names if image_names.count(x) > 1])\n",
    "\n",
    "    if duplicates:\n",
    "        raise Exception(\n",
    "            \"\"\"Image filenames should be unique, but the following \n",
    "            filenames are duplicated\\n{}\"\"\".format(\"\\n\".join(duplicates)))\n",
    "    \n",
    "    # optionally shuffle the image_paths\n",
    "    if kwargs.get(\"shuffle\", False):\n",
    "        print(timestamp(), \"Shuffling input images\")\n",
    "        random.Random(kwargs[\"seed\"]).shuffle(image_paths)\n",
    "    else:\n",
    "        image_paths = sorted(image_paths)\n",
    "\n",
    "    # optionally limit the number of images in image_paths\n",
    "    if kwargs.get(\"max_images\", False):\n",
    "        image_paths = image_paths[: kwargs[\"max_images\"]]        \n",
    "\n",
    "    # process and filter the images\n",
    "    filtered_image_paths = {}\n",
    "    oblong_ratio = kwargs[\"atlas_size\"] / kwargs[\"cell_size\"]\n",
    "    for img in Image.stream_images(image_paths=image_paths):\n",
    "        valid, msg = img.valid(lod_cell_height=kwargs[\"lod_cell_height\"], oblong_ratio=oblong_ratio) \n",
    "        if valid is True:\n",
    "            filtered_image_paths[img.path] = img.filename\n",
    "        else:\n",
    "            print(timestamp(), msg)\n",
    "\n",
    "    # if there are no remaining images, throw an error\n",
    "    if len(filtered_image_paths) == 0:\n",
    "        raise Exception(\"No images were found! Please check your input image glob.\")\n",
    "\n",
    "    # handle the case user provided no metadata\n",
    "    if not kwargs.get(\"meta_dir\", False):\n",
    "        return [filtered_image_paths.keys(), []]\n",
    "\n",
    "    # handle user metadata: retain only records with image and metadata\n",
    "    metaList = get_metadata_list(meta_dir=kwargs['meta_dir'])\n",
    "    metaDict = {clean_filename(i.get(FILE_NAME, \"\")): i for i in metaList}\n",
    "    meta_bn = set(metaDict.keys())\n",
    "    img_bn = set(filtered_image_paths.values())\n",
    "\n",
    "    # identify images with metadata and those without metadata\n",
    "    meta_present = img_bn.intersection(meta_bn)\n",
    "    meta_missing = list(img_bn - meta_bn)\n",
    "\n",
    "    # notify the user of images that are missing metadata\n",
    "    if meta_missing:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \" ! Some images are missing metadata:\\n  -\",\n",
    "            \"\\n  - \".join(meta_missing[:10]),\n",
    "        )\n",
    "        if len(meta_missing) > 10:\n",
    "            print(timestamp(), \" ...\", len(meta_missing) - 10, \"more\")\n",
    "\n",
    "        if os.path.exists(kwargs['out_dir']) is False:\n",
    "            os.makedirs(kwargs['out_dir'])\n",
    "            \n",
    "        missing_dir = os.path.join(kwargs['out_dir'],\"missing-metadata.txt\")\n",
    "        with open(missing_dir, \"w\") as out:\n",
    "            out.write(\"\\n\".join(meta_missing))\n",
    "\n",
    "    if not meta_present:\n",
    "        raise Exception( f\"\"\"No image has matching metadata. Check if '{FILE_NAME}' key was provided in metadata files\"\"\")\n",
    "\n",
    "    # get the sorted lists of images and metadata\n",
    "    images = []\n",
    "    metadata = []\n",
    "    for path, fileName in filtered_image_paths.items():\n",
    "        if fileName in meta_present:\n",
    "            images.append(path)\n",
    "            metadata.append(copy.deepcopy(metaDict[fileName]))\n",
    "\n",
    "    return [images, metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Atlases\n",
    "##\n",
    "\n",
    "\n",
    "def get_atlas_data(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate and save to disk all atlases to be used for this visualization\n",
    "    If square, center each cell in an nxn square, else use uniform height\n",
    "\n",
    "    Args:\n",
    "        out_dir (str)\n",
    "        plot_id (str, default = str(uuid.uuid1()))\n",
    "        use_cache (bool, default = False)\n",
    "        shuffle (Optional[bool], default = False)\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        out_dir (str): Atlas location \n",
    "\n",
    "    Notes:\n",
    "\n",
    "    \"\"\"\n",
    "    # if the atlas files already exist, load from cache\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"atlases\", kwargs[\"plot_id\"])\n",
    "    if (\n",
    "        os.path.exists(out_dir)\n",
    "        and kwargs[\"use_cache\"]\n",
    "        and not kwargs.get(\"shuffle\", False)\n",
    "    ):\n",
    "        print(timestamp(), \"Loading saved atlas data\")\n",
    "        return out_dir\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # else create the atlas images and store the positions of cells in atlases\n",
    "    print(timestamp(), \"Creating atlas files\")\n",
    "    n = 0  # number of atlases\n",
    "    x = 0  # x pos in atlas\n",
    "    y = 0  # y pos in atlas\n",
    "    positions = []  # l[cell_idx] = atlas data\n",
    "    atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        cell_data = i.resize_to_height(kwargs[\"cell_size\"])\n",
    "        _, v, _ = cell_data.shape\n",
    "        appendable = False\n",
    "        if (x + v) <= kwargs[\"atlas_size\"]:\n",
    "            appendable = True\n",
    "        elif (y + (2 * kwargs[\"cell_size\"])) <= kwargs[\"atlas_size\"]:\n",
    "            y += kwargs[\"cell_size\"]\n",
    "            x = 0\n",
    "            appendable = True\n",
    "        if not appendable:\n",
    "            save_atlas(atlas, out_dir, n)\n",
    "            n += 1\n",
    "            atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "            x = 0\n",
    "            y = 0\n",
    "        atlas[y : y + kwargs[\"cell_size\"], x : x + v] = cell_data\n",
    "        # find the size of the cell in the lod canvas\n",
    "        lod_data = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        h, w, _ = lod_data.shape  # h,w,colors in lod-cell sized image `i`\n",
    "        positions.append(\n",
    "            {\n",
    "                \"idx\": n,  # atlas idx\n",
    "                \"x\": x,  # x offset of cell in atlas\n",
    "                \"y\": y,  # y offset of cell in atlas\n",
    "                \"w\": w,  # w of cell at lod size\n",
    "                \"h\": h,  # h of cell at lod size\n",
    "            }\n",
    "        )\n",
    "        x += v\n",
    "    save_atlas(atlas, out_dir, n)\n",
    "    out_path = os.path.join(out_dir, \"atlas_positions.json\")\n",
    "    with open(out_path, \"w\") as out:\n",
    "        json.dump(positions, out)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def save_atlas(atlas, out_dir, n):\n",
    "    \"\"\"Save an atlas to disk\"\"\"\n",
    "    out_path = os.path.join(out_dir, \"atlas-{}.jpg\".format(n))\n",
    "    save_image(out_path, atlas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the command-line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse():\n",
    "    \"\"\"Read command line args and begin data processing\"\"\"\n",
    "    description = \"Create the data required to create a clipplot viewer\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--images\",\n",
    "        \"-i\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"images\"],\n",
    "        help=\"path to a glob of images to process\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metadata\",\n",
    "        \"-m\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"meta_dir\"],\n",
    "        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_images\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"max_images\"],\n",
    "        help=\"maximum number of images to process from the input glob\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cache\",\n",
    "        type=bool,\n",
    "        default=DEFAULTS[\"use_cache\"],\n",
    "        help=\"given inputs identical to prior inputs, load outputs from cache\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoding\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"encoding\"],\n",
    "        help=\"the encoding of input metadata\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_cluster_size\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"min_cluster_size\"],\n",
    "        help=\"the minimum number of images in a cluster\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_clusters\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"max_clusters\"],\n",
    "        help=\"the maximum number of clusters to return\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_dir\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"out_dir\"],\n",
    "        help=\"the directory to which outputs will be saved\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cell_size\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"cell_size\"],\n",
    "        help=\"the size of atlas cells in px\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_neighbors\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_neighbors\"],\n",
    "        help=\"the n_neighbors arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_dist\",\n",
    "        nargs=\"+\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"min_dist\"],\n",
    "        help=\"the min_dist arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_components\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_components\"],\n",
    "        help=\"the n_components argument for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"metric\"],\n",
    "        help=\"the metric argument for umap\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pointgrid_fill\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"pointgrid_fill\"],\n",
    "        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--copy_web_only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"update ./output/assets without reprocessing data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_size\",\n",
    "        type=float,\n",
    "        default=DEFAULTS[\"min_size\"],\n",
    "        help=\"min size of cropped images\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gzip\", action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\",\n",
    "        action=\"store_true\",\n",
    "        help=\"shuffle the input images before data processing begins\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plot_id\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"plot_id\"],\n",
    "        help=\"unique id for a plot; useful for resuming processing on a started plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=DEFAULTS[\"seed\"], help=\"seed for random processes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_clusters\",\n",
    "        type=int,\n",
    "        default=DEFAULTS[\"n_clusters\"],\n",
    "        help=\"number of clusters to use when clustering with kmeans\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--geojson\",\n",
    "        type=str,\n",
    "        default=DEFAULTS[\"geojson\"],\n",
    "        help=\"path to a GeoJSON file with shapes to be rendered on a map\",\n",
    "    )\n",
    "    config = DEFAULTS.copy()\n",
    "    if in_ipython():\n",
    "        config.update(vars(parser.parse_args({})))\n",
    "    else:\n",
    "        config.update(vars(parser.parse_args()))\n",
    "\n",
    "    return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carlo's Test Functions\n",
    "# Need to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_iiif(config):\n",
    "    test_images = copy_root_dir/\"tests/IIIF_examples/iif_example.txt\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    # meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    # config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly_duplicate(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs_duplicates/**/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/good_meta.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "    config[\"plot_id\"] = \"test_diff\"\n",
    "    \n",
    "    config[\"test_mode\"] = True\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_butterfly_missing_meta(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    meta_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/meta_data/meta_missing_filename.csv\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "    config[\"meta_dir\"] = meta_dir.as_posix()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_no_meta_dir(config):\n",
    "    test_images = copy_root_dir/\"tests/smithsonian_butterflies_10/jpgs/*.jpg\"\n",
    "    test_out_dir = copy_root_dir/\"tests/smithsonian_butterflies_10/output_test_temp\"\n",
    "    if Path(test_out_dir).exists():\n",
    "        rmtree(test_out_dir)\n",
    "\n",
    "    config[\"images\"] = test_images.as_posix()\n",
    "    config[\"out_dir\"] = test_out_dir.as_posix()\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@call_parse\n",
    "def project_imgs(images:Param(type=str,\n",
    "                        help=\"path to a glob of images to process\"\n",
    "                        )=DEFAULTS[\"images\"],\n",
    "                metadata:Param(type=str,\n",
    "                        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\"\n",
    "                        )=DEFAULTS[\"meta_dir\"],\n",
    "                max_images:Param(type=int,\n",
    "                        help=\"maximum number of images to process from the input glob\"\n",
    "                        )=DEFAULTS[\"max_images\"],\n",
    "                use_cache:Param(type=bool,\n",
    "                        help=\"given inputs identical to prior inputs, load outputs from cache\"\n",
    "                        )=DEFAULTS[\"use_cache\"],\n",
    "                encoding:Param(type=str,\n",
    "                        help=\"the encoding of input metadata\"\n",
    "                        )=DEFAULTS[\"encoding\"],\n",
    "                min_cluster_size:Param(type=int,\n",
    "                        help=\"the minimum number of images in a cluster\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"min_cluster_size\"],\n",
    "                max_clusters:Param(type=int,\n",
    "                        help=\"the maximum number of clusters to return\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"max_clusters\"],\n",
    "                out_dir:Param(type=str,\n",
    "                        help=\"the directory to which outputs will be saved\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"out_dir\"],\n",
    "                cell_size:Param(type=int,\n",
    "                        help=\"the size of atlas cells in px\",\n",
    "                        required=False\n",
    "                        )=DEFAULTS[\"cell_size\"],\n",
    "                n_neighbors:Param(type=int,\n",
    "                        nargs=\"+\",\n",
    "                        help=\"the n_neighbors arguments for UMAP\"\n",
    "                        )=DEFAULTS[\"n_neighbors\"],\n",
    "                min_dist:Param(type=float,\n",
    "                        nargs=\"+\",\n",
    "                        help=\"the min_dist arguments for UMAP\"\n",
    "                        )=DEFAULTS[\"min_dist\"],\n",
    "                n_components:Param(type=int,\n",
    "                        help=\"the n_components argument for UMAP\"\n",
    "                        )=DEFAULTS[\"n_components\"],\n",
    "                metric:Param(type=str,\n",
    "                        help=\"the metric argument for umap\"\n",
    "                        )=DEFAULTS[\"metric\"],\n",
    "                pointgrid_fill:Param(type=float,\n",
    "                        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\"\n",
    "                        )=DEFAULTS[\"pointgrid_fill\"],\n",
    "                copy_web_only:Param(type=bool,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"update ./output/assets without reprocessing data\"\n",
    "                        )=False,\n",
    "                min_size:Param(type=float,\n",
    "                        help=\"min size of cropped images\"\n",
    "                        )=DEFAULTS[\"min_size\"],\n",
    "                gzip:Param(type=bool,\n",
    "                        action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "                        )=False,\n",
    "                shuffle:Param(type=bool,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"shuffle the input images before data processing begins\"\n",
    "                        )=False,\n",
    "                plot_id:Param(type=str,\n",
    "                        help=\"unique id for a plot; useful for resuming processing on a started plot\"\n",
    "                        )=DEFAULTS[\"plot_id\"],\n",
    "                seed:Param(type=int, help=\"seed for random processes\"\n",
    "                           )=DEFAULTS[\"seed\"],\n",
    "                n_clusters:Param(type=int,\n",
    "                        help=\"number of clusters if using kmeans\"\n",
    "                        )=DEFAULTS[\"n_clusters\"],\n",
    "                geojson:Param(type=str,\n",
    "                        help=\"path to a GeoJSON file with shapes to be rendered on a map\"\n",
    "                        )=DEFAULTS[\"geojson\"]\n",
    "                ):\n",
    "                \"Convert a folder of images into a clip-plot visualization\"\n",
    "\n",
    "                config = parse()\n",
    "                copy_root_dir = get_clip_plot_root()\n",
    "\n",
    "                if in_ipython() and config[\"images\"] == None:\n",
    "                        print(\"we're in ipython\")\n",
    "                        # at least for now, this means we're in testing mode.\n",
    "                        # TODO: pass explicit \"test_mode\" flag\n",
    "                        config = test_butterfly(config)\n",
    "\n",
    "                process_images(**config)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = parse()\n",
    "    copy_root_dir = get_clip_plot_root()\n",
    "\n",
    "    if in_ipython() and config[\"images\"] == None:\n",
    "        # at least for now, this means we're in testing mode.\n",
    "        # TODO: pass explicit \"test_mode\" flag to argparse\n",
    "        config = test_butterfly(config)\n",
    "        \n",
    "\n",
    "    process_images(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
