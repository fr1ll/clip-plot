{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from clip_plot.images import Image\n",
    "from clip_plot.utils import timestamp, get_path, write_json, read_json, round_floats\n",
    "from clip_plot.utils import datestring_to_date, round_date, date_to_seconds, clean_filename\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List\n",
    "import math\n",
    "import operator\n",
    "import multiprocessing\n",
    "\n",
    "#TODO: Change math references to numpy\n",
    "\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP, AlignedUMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.stats import kde\n",
    "\n",
    "\n",
    "import rasterfairy\n",
    "from rasterfairy import coonswarp\n",
    "from pointgrid import align_points_to_grid\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some constants we want to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "## Ignoring cuml-umap for now to avoid conditional import\n",
    "## We may add cuml-umap option back later\n",
    "## if profiling shows it may give speedup\n",
    "\n",
    "## TODO: Profile clip-plot on mnist as worst-case for umap (cheap embeddings, high number of points)\n",
    "\n",
    "cuml_ready = False\n",
    "cluster_method = \"hdbscan\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create layouts based on dimensionality reduction or other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def write_layout(path, obj, **kwargs):\n",
    "    \"\"\"Write layout json `obj` to disk and return the path to the saved file\"\"\"\n",
    "    if kwargs.get(\"scale\", True) != False:\n",
    "        obj = (minmax_scale(obj) - 0.5) * 2  # scale -1:1\n",
    "    if kwargs.get(\"round\", True) != False:\n",
    "        obj = round_floats(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        obj = obj.tolist()\n",
    "    return write_json(path, obj, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_pointgrid_layout(path, label, **kwargs):\n",
    "    \"\"\"Gridify the positions in `path` and return the path to this new layout\"\"\"\n",
    "    print(timestamp(), \"Creating {} pointgrid\".format(label))\n",
    "    out_path = get_path(\"layouts\", label + \"-jittered\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    arr = np.array(read_json(path, **kwargs))\n",
    "    if arr.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create pointgrid layout because data is not 2D\")\n",
    "        return None\n",
    "    z = align_points_to_grid(arr, fill=0.01)\n",
    "    return write_layout(out_path, z, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_umap_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images passed through a umap projection\"\"\"\n",
    "    vecs = kwargs[\"vecs\"]\n",
    "    w = PCA(n_components=min(100, len(vecs))).fit_transform(vecs)\n",
    "    # single model umap\n",
    "    if len(kwargs[\"n_neighbors\"]) == 1 and len(kwargs[\"min_dist\"]) == 1:\n",
    "        return process_single_layout_umap(w, **kwargs)\n",
    "    else:\n",
    "        return process_multi_layout_umap(w, **kwargs)\n",
    "\n",
    "\n",
    "def process_single_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a single layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating single umap layout\")\n",
    "    model = get_umap_model(**kwargs)\n",
    "    out_path = get_path(\"layouts\", \"umap\", **kwargs)\n",
    "    if cuml_ready:\n",
    "        z = model.fit(v).embedding_\n",
    "    else:\n",
    "        if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "            return out_path\n",
    "        y = []\n",
    "        if kwargs.get(\"metadata\", False):\n",
    "            labels = [i.get(\"label\", None) for i in kwargs[\"metadata\"]]\n",
    "            # if the user provided labels, integerize them\n",
    "            if any([i for i in labels]):\n",
    "                d = defaultdict(lambda: len(d))\n",
    "                for i in labels:\n",
    "                    if i == None:\n",
    "                        y.append(-1)\n",
    "                    else:\n",
    "                        y.append(d[i])\n",
    "                y = np.array(y)\n",
    "        # project the PCA space down to 2d for visualization\n",
    "        z = model.fit(v, y=y if np.any(y) else None).embedding_\n",
    "    return {\n",
    "        \"variants\": [\n",
    "            {\n",
    "                \"n_neighbors\": kwargs[\"n_neighbors\"][0],\n",
    "                \"min_dist\": kwargs[\"min_dist\"][0],\n",
    "                \"layout\": write_layout(out_path, z, **kwargs),\n",
    "                \"jittered\": get_pointgrid_layout(out_path, \"umap\", **kwargs),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def process_multi_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a multi-layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating multi-umap layout\")\n",
    "    params = []\n",
    "    for n_neighbors, min_dist in itertools.product(\n",
    "        kwargs[\"n_neighbors\"], kwargs[\"min_dist\"]\n",
    "    ):\n",
    "        filename = \"umap-n_neighbors_{}-min_dist_{}\".format(n_neighbors, min_dist)\n",
    "        out_path = get_path(\"layouts\", filename, **kwargs)\n",
    "        params.append(\n",
    "            {\n",
    "                \"n_neighbors\": n_neighbors,\n",
    "                \"min_dist\": min_dist,\n",
    "                FILE_NAME: filename,\n",
    "                \"out_path\": out_path,\n",
    "            }\n",
    "        )\n",
    "    # map each image's index to itself and create one copy of that map for each layout\n",
    "    relations_dict = {idx: idx for idx, _ in enumerate(v)}\n",
    "    # determine the subset of params that have already been computed\n",
    "    uncomputed_params = [i for i in params if not os.path.exists(i[\"out_path\"])]\n",
    "    # determine the filepath where this model will be saved\n",
    "    model_filename = \"umap-\" + str(abs(hash(kwargs[\"images\"])))\n",
    "    model_path = get_path(\"models\", model_filename, **kwargs).replace(\".json\", \".gz\")\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"models\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # load or create the model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        for i in uncomputed_params:\n",
    "            model.update(v, relations_dict.copy())\n",
    "        # after updating, we can read the results from the end of the updated model\n",
    "        for idx, i in enumerate(uncomputed_params):\n",
    "            embedding = z.embeddings_[len(uncomputed_params) - idx]\n",
    "            write_layout(i[\"out_path\"], embedding, **kwargs)\n",
    "    else:\n",
    "        model = AlignedUMAP(\n",
    "            n_neighbors=[i[\"n_neighbors\"] for i in uncomputed_params],\n",
    "            min_dist=[i[\"min_dist\"] for i in uncomputed_params],\n",
    "        )\n",
    "        # fit the model on the data\n",
    "        z = model.fit(\n",
    "            [v for _ in params], relations=[relations_dict for _ in params[1:]]\n",
    "        )\n",
    "        for idx, i in enumerate(params):\n",
    "            write_layout(i[\"out_path\"], z.embeddings_[idx], **kwargs)\n",
    "        # save the model\n",
    "        save_model(model, model_path)\n",
    "    # load the list of layout variants\n",
    "    l = []\n",
    "    for i in params:\n",
    "        l.append(\n",
    "            {\n",
    "                \"n_neighbors\": i[\"n_neighbors\"],\n",
    "                \"min_dist\": i[\"min_dist\"],\n",
    "                \"layout\": i[\"out_path\"],\n",
    "                \"jittered\": get_pointgrid_layout(\n",
    "                    i[\"out_path\"], i[FILE_NAME], **kwargs\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"variants\": l,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        params = model.get_params()\n",
    "        attributes_names = [\n",
    "            attr for attr in model.__dir__() if attr not in params and attr[0] != \"_\"\n",
    "        ]\n",
    "        attributes = {key: model.__getattribute__(key) for key in attributes_names}\n",
    "        attributes[\"embeddings_\"] = list(model.embeddings_)\n",
    "        for x in [\"fit\", \"fit_transform\", \"update\", \"get_params\", \"set_params\"]:\n",
    "            del attributes[x]\n",
    "        all_params = {\n",
    "            \"umap_params\": params,\n",
    "            \"umap_attributes\": {key: value for key, value in attributes.items()},\n",
    "        }\n",
    "        pickle.dump(all_params, open(path, \"wb\"))\n",
    "    except:\n",
    "        print(timestamp(), \"Could not save model\")\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    params = pickle.load(open(path, \"rb\"))\n",
    "    model = AlignedUMAP()\n",
    "    model.set_params(**params.get(\"umap_params\"))\n",
    "    for attr, value in params.get(\"umap_attributes\").items():\n",
    "        model.__setattr__(attr, value)\n",
    "    model.__setattr__(\n",
    "        \"embeddings_\", List(params.get(\"umap_attributes\").get(\"embeddings_\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_umap_model(**kwargs):\n",
    "    if cuml_ready:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            verbose=5,\n",
    "        )\n",
    "    else:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            metric=kwargs[\"metric\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            transform_seed=kwargs[\"seed\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rasterfairy_layout(**kwargs):\n",
    "    \"\"\"Get the x, y position of images passed through a rasterfairy projection\"\"\"\n",
    "    print(timestamp(), \"Creating rasterfairy layout\")\n",
    "    out_path = get_path(\"layouts\", \"rasterfairy\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    if umap.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create rasterfairy layout because data is not 2D\")\n",
    "        return None\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    try:\n",
    "        umap = coonswarp.rectifyCloud(\n",
    "            umap,  # stretch the distribution\n",
    "            perimeterSubdivisionSteps=4,\n",
    "            autoPerimeterOffset=False,\n",
    "            paddingScale=1.05,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Coonswarp rectification could not be performed\", exc)\n",
    "    pos = rasterfairy.transformPointCloud2D(umap)[0]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_alphabetic_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images in a grid projection\"\"\"\n",
    "    print(timestamp(), \"Creating grid layout\")\n",
    "    out_path = get_path(\"layouts\", \"grid\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    paths = kwargs[\"image_paths\"]\n",
    "    n = math.ceil(len(paths) ** (1 / 2))\n",
    "    l = []  # positions\n",
    "    for i, _ in enumerate(paths):\n",
    "        x = i % n\n",
    "        y = math.floor(i / n)\n",
    "        l.append([x, y])\n",
    "    z = np.array(l)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_custom_layout(**kwargs):\n",
    "    out_path = get_path(\"layouts\", \"custom\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    if not kwargs.get(\"metadata\"):\n",
    "        return\n",
    "    found_coords = False\n",
    "    coords = []\n",
    "    for i in Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"]):\n",
    "        x = i.metadata.get(\"x\")\n",
    "        y = i.metadata.get(\"y\")\n",
    "        if x and y:\n",
    "            found_coords = True\n",
    "            coords.append([x, y])\n",
    "        else:\n",
    "            if found_coords:\n",
    "                print(\n",
    "                    timestamp(),\n",
    "                    \"Some images are missing coordinates; skipping custom layout\",\n",
    "                )\n",
    "    if not found_coords:\n",
    "        return\n",
    "    coords = np.array(coords).astype(np.float)\n",
    "    coords = (minmax_scale(coords) - 0.5) * 2\n",
    "    print(timestamp(), \"Creating custom layout\")\n",
    "    return {\n",
    "        \"layout\": write_layout(\n",
    "            out_path, coords.tolist(), scale=False, round=False, **kwargs\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_date_layout(cols=3, bin_units=\"years\", **kwargs):\n",
    "    \"\"\"\n",
    "    Get the x,y positions of input images based on their dates\n",
    "    @param int cols: the number of columns to plot for each bar\n",
    "    @param str bin_units: the temporal units to use when creating bins\n",
    "    \"\"\"\n",
    "    date_vals = [\n",
    "        kwargs[\"metadata\"][i].get(\"year\", False) for i in range(len(kwargs[\"metadata\"]))\n",
    "    ]\n",
    "    if not kwargs[\"metadata\"] or not any(date_vals):\n",
    "        return False\n",
    "    # if the data layouts have been cached, return them\n",
    "    positions_out_path = get_path(\"layouts\", \"timeline\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"timeline-labels\", **kwargs)\n",
    "    if (\n",
    "        os.path.exists(positions_out_path)\n",
    "        and os.path.exists(labels_out_path)\n",
    "        and kwargs[\"use_cache\"]\n",
    "    ):\n",
    "        return {\n",
    "            \"layout\": positions_out_path,\n",
    "            \"labels\": labels_out_path,\n",
    "        }\n",
    "    # date layout is not cached, so fetch dates and process\n",
    "    print(timestamp(), \"Creating date layout with {} columns\".format(cols))\n",
    "    datestrings = [i.metadata.get(\"year\", \"no_date\") for i in Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])]\n",
    "    dates = [datestring_to_date(i) for i in datestrings]\n",
    "    rounded_dates = [round_date(i, bin_units) for i in dates]\n",
    "    # create d[formatted_date] = [indices into datestrings of dates that round to formatted_date]\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(rounded_dates):\n",
    "        d[i].append(idx)\n",
    "    # determine the number of distinct grid positions in the x and y axes\n",
    "    n_coords_x = (cols + 1) * len(d)\n",
    "    n_coords_y = 1 + max([len(d[i]) for i in d]) // cols\n",
    "    if n_coords_y > n_coords_x:\n",
    "        return get_date_layout(cols=int(cols * 2), **kwargs)\n",
    "    # create a mesh of grid positions in clip space -1:1 given the time distribution\n",
    "    grid_x = (np.arange(0, n_coords_x) / (n_coords_x - 1)) * 2\n",
    "    grid_y = (np.arange(0, n_coords_y) / (n_coords_x - 1)) * 2\n",
    "    # divide each grid axis by half its max length to center at the origin 0,0\n",
    "    grid_x = grid_x - np.max(grid_x) / 2.0\n",
    "    grid_y = grid_y - np.max(grid_y) / 2.0\n",
    "    # make dates increase from left to right by sorting keys of d\n",
    "    d_keys = np.array(list(d.keys()))\n",
    "    seconds = np.array([date_to_seconds(dates[d[i][0]]) for i in d_keys])\n",
    "    d_keys = d_keys[np.argsort(seconds)]\n",
    "    # determine which images will fill which units of the grid established above\n",
    "    coords = np.zeros(\n",
    "        (len(datestrings), 2)\n",
    "    )  # 2D array with x, y clip-space coords of each date\n",
    "    for jdx, j in enumerate(d_keys):\n",
    "        for kdx, k in enumerate(d[j]):\n",
    "            x = jdx * (cols + 1) + (kdx % cols)\n",
    "            y = kdx // cols\n",
    "            coords[k] = [grid_x[x], grid_y[y]]\n",
    "    # find the positions of labels\n",
    "    label_positions = np.array(\n",
    "        [[grid_x[i * (cols + 1)], grid_y[0]] for i in range(len(d))]\n",
    "    )\n",
    "    # move the labels down in the y dimension by a grid unit\n",
    "    dx = grid_x[1] - grid_x[0]  # size of a single cell\n",
    "    label_positions[:, 1] = label_positions[:, 1] - dx\n",
    "    # quantize the label positions and label positions\n",
    "    image_positions = round_floats(coords)\n",
    "    label_positions = round_floats(label_positions.tolist())\n",
    "    # write and return the paths to the date based layout\n",
    "    return {\n",
    "        \"layout\": write_json(positions_out_path, image_positions, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": label_positions,\n",
    "                \"labels\": d_keys.tolist(),\n",
    "                \"cols\": cols,\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_categorical_layout(null_category=\"Other\", margin=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a numpy array with shape (n_points, 2) with the point\n",
    "    positions of observations in box regions determined by\n",
    "    each point's category metadata attribute (if applicable)\n",
    "    \"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return False\n",
    "    # determine the out path and return from cache if possible\n",
    "    out_path = get_path(\"layouts\", \"categorical\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"categorical-labels\", **kwargs)\n",
    "    # accumulate d[category] = [indices of points with category]\n",
    "    categories = [i.get(\"category\", None) for i in kwargs[\"metadata\"]]\n",
    "    if not any(categories) or len(set(categories) - set([None])) == 1:\n",
    "        return False\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(categories):\n",
    "        d[i].append(idx)\n",
    "    # store the number of observations in each group\n",
    "    keys_and_counts = [{\"key\": i, \"count\": len(d[i])} for i in d]\n",
    "    keys_and_counts.sort(key=operator.itemgetter(\"count\"), reverse=True)\n",
    "    # get the box layout then subdivide into discrete points\n",
    "    boxes = get_categorical_boxes([i[\"count\"] for i in keys_and_counts], margin=margin)\n",
    "    points = get_categorical_points(boxes)\n",
    "    # sort the points into the order of the observations in the metadata\n",
    "    counts = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    offsets = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    for idx, i in enumerate(keys_and_counts):\n",
    "        offsets[i[\"key\"]] += sum([j[\"count\"] for j in keys_and_counts[:idx]])\n",
    "    sorted_points = []\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        category = i.metadata.get(\"category\", null_category)\n",
    "        sorted_points.append(points[offsets[category] + counts[category]])\n",
    "        counts[category] += 1\n",
    "    sorted_points = np.array(sorted_points)\n",
    "    # add to the sorted points the anchors for the text labels for each group\n",
    "    text_anchors = np.array([[i.x, i.y - margin / 2] for i in boxes])\n",
    "    # add the anchors to the points - these will be removed after the points are projected\n",
    "    sorted_points = np.vstack([sorted_points, text_anchors])\n",
    "    # scale -1:1 using the largest axis as the scaling metric\n",
    "    _max = np.max(sorted_points)\n",
    "    for i in range(2):\n",
    "        _min = np.min(sorted_points[:, i])\n",
    "        sorted_points[:, i] -= _min\n",
    "        sorted_points[:, i] /= _max - _min\n",
    "        sorted_points[:, i] -= np.max(sorted_points[:, i]) / 2\n",
    "        sorted_points[:, i] *= 2\n",
    "    # separate out the sorted points and text positions\n",
    "    text_anchors = sorted_points[-len(text_anchors) :]\n",
    "    sorted_points = sorted_points[: -len(text_anchors)]\n",
    "    z = round_floats(sorted_points.tolist())\n",
    "    return {\n",
    "        \"layout\": write_json(out_path, z, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": round_floats(text_anchors.tolist()),\n",
    "                \"labels\": [i[\"key\"] for i in keys_and_counts],\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_categorical_boxes(group_counts, margin=2):\n",
    "    \"\"\"\n",
    "    @arg [int] group_counts: counts of the number of images in each\n",
    "      distinct level within the metadata's caetgories\n",
    "    @kwarg int margin: space between boxes in the 2D layout\n",
    "    @returns [Box] an array of Box() objects; one per level in `group_counts`\n",
    "    \"\"\"\n",
    "    group_counts = sorted(group_counts, reverse=True)\n",
    "    boxes = []\n",
    "    for i in group_counts:\n",
    "        w = h = math.ceil(i ** (1 / 2))\n",
    "        boxes.append(Box(i, w, h, None, None))\n",
    "    # find the position along x axis where we want to create a break\n",
    "    wrap = math.floor(sum([i.cells for i in boxes]) ** (1 / 2)) - (2 * margin)\n",
    "    # find the valid positions on the y axis\n",
    "    y = margin\n",
    "    y_spots = []\n",
    "    for i in boxes:\n",
    "        if (y + i.h + margin) <= wrap:\n",
    "            y_spots.append(y)\n",
    "            y += i.h + margin\n",
    "        else:\n",
    "            y_spots.append(y)\n",
    "            break\n",
    "    # get a list of lists where sublists contain elements at the same y position\n",
    "    y_spot_index = 0\n",
    "    for i in boxes:\n",
    "        # find the y position\n",
    "        y = y_spots[y_spot_index]\n",
    "        # find members with this y position\n",
    "        row_members = [j.x + j.w for j in boxes if j.y == y]\n",
    "        # assign the y position\n",
    "        i.y = y\n",
    "        y_spot_index = (y_spot_index + 1) % len(y_spots)\n",
    "        # assign the x position\n",
    "        i.x = max(row_members) + margin if row_members else margin\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_categorical_points(arr, unit_size=None):\n",
    "    \"\"\"Given an array of Box() objects, return a 2D distribution with shape (n_cells, 2)\"\"\"\n",
    "    points_arr = []\n",
    "    for i in arr:\n",
    "        area = i.w * i.h\n",
    "        per_unit = (area / i.cells) ** (1 / 2)\n",
    "        x_units = math.ceil(i.w / per_unit)\n",
    "        y_units = math.ceil(i.h / per_unit)\n",
    "        if not unit_size:\n",
    "            unit_size = min(i.w / x_units, i.h / y_units)\n",
    "        for j in range(i.cells):\n",
    "            x = j % x_units\n",
    "            y = j // x_units\n",
    "            points_arr.append(\n",
    "                [\n",
    "                    i.x + x * unit_size,\n",
    "                    i.y + y * unit_size,\n",
    "                ]\n",
    "            )\n",
    "    return np.array(points_arr)\n",
    "\n",
    "\n",
    "class Box:\n",
    "    \"\"\"Store the width, height, and x, y coords of a box\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.cells = args[0]\n",
    "        self.w = args[1]\n",
    "        self.h = args[2]\n",
    "        self.x = None if len(args) < 4 else args[3]\n",
    "        self.y = None if len(args) < 5 else args[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_geographic_layout(**kwargs):\n",
    "    \"\"\"Return a 2D array of image positions corresponding to lat, lng coordinates\"\"\"\n",
    "    out_path = get_path(\"layouts\", \"geographic\", **kwargs)\n",
    "    l = []\n",
    "    coords = False\n",
    "    for idx, i in enumerate(Image.stream_images(image_paths=kwargs[\"image_paths\"], metadata=kwargs[\"metadata\"])):\n",
    "        lat = float(i.metadata.get(\"lat\", 0)) / 180\n",
    "        lng = (\n",
    "            float(i.metadata.get(\"lng\", 0)) / 180\n",
    "        )  # the plot draws longitude twice as tall as latitude\n",
    "        if lat or lng:\n",
    "            coords = True\n",
    "        l.append([lng, lat])\n",
    "    if coords:\n",
    "        print(timestamp(), \"Creating geographic layout\")\n",
    "        if kwargs[\"geojson\"]:\n",
    "            process_geojson(kwargs[\"geojson\"])\n",
    "        return {\"layout\": write_layout(out_path, l, scale=False, **kwargs)}\n",
    "    elif kwargs[\"geojson\"]:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \"GeoJSON is only processed if you also provide lat/lng coordinates for your images in a metadata file!\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path):\n",
    "    \"\"\"Given a GeoJSON filepath, write a minimal JSON output in lat lng coordinates\"\"\"\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson = json.load(f)\n",
    "    l = []\n",
    "    for i in geojson:\n",
    "        if isinstance(i, dict):\n",
    "            for j in i.get(\"coordinates\", []):\n",
    "                for k in j:\n",
    "                    l.append(k)\n",
    "    with open(\n",
    "        os.path.join(\"output\", \"assets\", \"json\", \"geographic-features.json\"), \"w\"\n",
    "    ) as out:\n",
    "        json.dump(l, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_hotspots(layouts={}, use_high_dimensional_vectors=True, **kwargs):\n",
    "    \"\"\"Return the stable clusters from the condensed tree of connected components from the density graph\"\"\"\n",
    "    print(timestamp(), \"Clustering data with {}\".format(cluster_method))\n",
    "    if use_high_dimensional_vectors:\n",
    "        vecs = kwargs[\"vecs\"]\n",
    "    else:\n",
    "        vecs = read_json(layouts[\"umap\"][\"variants\"][0][\"layout\"], **kwargs)\n",
    "    model = get_cluster_model(**kwargs)\n",
    "    z = model.fit(vecs)\n",
    "    # create a map from cluster label to image indices in cluster\n",
    "    d = defaultdict(lambda: defaultdict(list))\n",
    "    for idx, i in enumerate(z.labels_):\n",
    "        if i != -1:\n",
    "            d[i][\"images\"].append(idx)\n",
    "            d[i][\"img\"] = clean_filename(kwargs[\"image_paths\"][idx])\n",
    "            d[i][\"layout\"] = \"inception_vectors\"\n",
    "    # remove massive clusters\n",
    "    deletable = []\n",
    "    for i in d:\n",
    "        # find percent of images in cluster\n",
    "        image_percent = len(d[i][\"images\"]) / len(vecs)\n",
    "        # determine if image or area percent is too large\n",
    "        if image_percent > 0.5:\n",
    "            deletable.append(i)\n",
    "    for i in deletable:\n",
    "        del d[i]\n",
    "    # sort the clusers by size and then label the clusters\n",
    "    clusters = d.values()\n",
    "    clusters = sorted(clusters, key=lambda i: len(i[\"images\"]), reverse=True)\n",
    "    for idx, i in enumerate(clusters):\n",
    "        i[\"label\"] = \"Cluster {}\".format(idx + 1)\n",
    "    # slice off the first `max_clusters`\n",
    "    clusters = clusters[: kwargs[\"max_clusters\"]]\n",
    "    # save the hotspots to disk and return the path to the saved json\n",
    "    print(timestamp(), \"Found\", len(clusters), \"hotspots\")\n",
    "    return write_json(get_path(\"hotspots\", \"hotspot\", **kwargs), clusters, **kwargs)\n",
    "\n",
    "\n",
    "def get_cluster_model(**kwargs):\n",
    "    \"\"\"Return a model with .fit() method that can be used to cluster input vectors\"\"\"\n",
    "    config = {\n",
    "        \"core_dist_n_jobs\": multiprocessing.cpu_count(),\n",
    "        \"min_cluster_size\": kwargs[\"min_cluster_size\"],\n",
    "        \"cluster_selection_epsilon\": 0.01,\n",
    "        \"min_samples\": 1,\n",
    "        \"approx_min_span_tree\": False,\n",
    "    }\n",
    "    return HDBSCAN(**config)\n",
    "\n",
    "\n",
    "def get_heightmap(path, label, **kwargs):\n",
    "    \"\"\"Create a heightmap using the distribution of points stored at `path`\"\"\"\n",
    "\n",
    "    X = read_json(path, **kwargs)\n",
    "    if \"positions\" in X:\n",
    "        X = X[\"positions\"]\n",
    "    X = np.array(X)\n",
    "    if X.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create heightmap because data is not 2D\")\n",
    "        return\n",
    "    # create kernel density estimate of distribution X\n",
    "    nbins = 200\n",
    "    x, y = X.T\n",
    "    xi, yi = np.mgrid[x.min() : x.max() : nbins * 1j, y.min() : y.max() : nbins * 1j]\n",
    "    zi = kde.gaussian_kde(X.T)(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading=\"gouraud\", cmap=plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # save the plot\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"heightmaps\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_path = os.path.join(out_dir, label + \"-heightmap.png\")\n",
    "    plt.savefig(out_path, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Layouts\n",
    "##\n",
    "\n",
    "\n",
    "def get_layouts(**kwargs):\n",
    "    \"\"\"Get the image positions in each projection\"\"\"\n",
    "    umap = get_umap_layout(**kwargs)\n",
    "    layouts = {\n",
    "        \"umap\": umap,\n",
    "        \"alphabetic\": {\n",
    "            \"layout\": get_alphabetic_layout(**kwargs),\n",
    "        },\n",
    "        \"grid\": {\n",
    "            \"layout\": get_rasterfairy_layout(umap=umap, **kwargs),\n",
    "        },\n",
    "        \"categorical\": get_categorical_layout(**kwargs),\n",
    "        \"date\": get_date_layout(**kwargs),\n",
    "        \"geographic\": get_geographic_layout(**kwargs),\n",
    "        \"custom\": get_custom_layout(**kwargs),\n",
    "    }\n",
    "    return layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
