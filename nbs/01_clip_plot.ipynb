{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clip_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import division\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##\n",
    "# Unconditional imports\n",
    "##\n",
    "\n",
    "from os.path import join, exists, dirname, realpath\n",
    "from distutils.dir_util import copy_tree\n",
    "import pkg_resources\n",
    "import datetime\n",
    "import argparse\n",
    "import glob2\n",
    "import uuid\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"Return a string for printing the current time\"\"\"\n",
    "    return str(datetime.datetime.now()) + \":\"\n",
    "\n",
    "\n",
    "##\n",
    "# Image processing imports\n",
    "##\n",
    "\n",
    "if \"--copy_web_only\" not in sys.argv:\n",
    "\n",
    "    from sklearn.metrics import pairwise_distances_argmin_min\n",
    "    from collections import defaultdict, namedtuple\n",
    "    from dateutil.parser import parse as parse_date\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    from pointgrid import align_points_to_grid\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from sklearn.decomposition import PCA\n",
    "    from iiif_downloader import Manifest\n",
    "    from rasterfairy import coonswarp\n",
    "    from scipy.stats import kde\n",
    "    from PIL import ImageFile\n",
    "    import multiprocessing\n",
    "    from tqdm import tqdm\n",
    "    import rasterfairy\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    import operator\n",
    "    import pickle\n",
    "    import random\n",
    "    import copy\n",
    "    import math\n",
    "    import gzip\n",
    "    import json\n",
    "    import csv\n",
    "\n",
    "    from urllib.parse import unquote\n",
    "\n",
    "    # Keras imports\n",
    "    from tensorflow.keras.preprocessing.image import save_img, img_to_array, array_to_img\n",
    "    from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "    from tensorflow.keras.applications import InceptionV3, imagenet_utils\n",
    "    from tensorflow.keras.preprocessing.image import load_img\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow import compat\n",
    "\n",
    "\n",
    "    ##\n",
    "    # Optional install imports\n",
    "    ##\n",
    "    from hdbscan import HDBSCAN\n",
    "    cluster_method = \"hdbscan\"\n",
    "\n",
    "    try:\n",
    "        from cuml.manifold.umap import UMAP\n",
    "\n",
    "        print(timestamp(), \"Using cuml UMAP\")\n",
    "        cuml_ready = True\n",
    "        from umap import AlignedUMAP\n",
    "    except:\n",
    "        from umap import UMAP, AlignedUMAP\n",
    "\n",
    "        print(timestamp(), \"CUML not available; using umap-learn UMAP\")\n",
    "        cuml_ready = False\n",
    "\n",
    "    # handle truncated images in PIL (managed by Pillow)\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\"\"\"\n",
    "NB: Keras Image class objects return image.size as w,h\n",
    "    Numpy array representations of images return image.shape as h,w,c\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "config = {\n",
    "    \"images\": None,\n",
    "    \"metadata\": None,\n",
    "    \"out_dir\": \"output\",\n",
    "    \"max_images\": None,\n",
    "    \"use_cache\": True,\n",
    "    \"encoding\": \"utf8\",\n",
    "    \"min_cluster_size\": 20,\n",
    "    \"max_clusters\": 10,\n",
    "    \"atlas_size\": 2048,\n",
    "    \"cell_size\": 32,\n",
    "    \"lod_cell_height\": 128,\n",
    "    \"n_neighbors\": [15],\n",
    "    \"min_dist\": [0.01],\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"correlation\",\n",
    "    \"pointgrid_fill\": 0.05,\n",
    "    \"gzip\": False,\n",
    "    \"min_size\": 100,\n",
    "    \"min_score\": 0.3,\n",
    "    \"min_vertices\": 18,\n",
    "    \"plot_id\": str(uuid.uuid1()),\n",
    "    \"seed\": 24,\n",
    "    \"n_clusters\": 12,\n",
    "    \"geojson\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Entry\n",
    "##\n",
    "\n",
    "\n",
    "def process_images(**kwargs):\n",
    "    \"\"\"Main method for processing user images and metadata\"\"\"\n",
    "    kwargs = preprocess_kwargs(**kwargs)\n",
    "    copy_web_assets(**kwargs)\n",
    "    np.random.seed(kwargs[\"seed\"])\n",
    "    compat.v1.set_random_seed(kwargs[\"seed\"])\n",
    "    kwargs[\"out_dir\"] = join(kwargs[\"out_dir\"], \"data\")\n",
    "    kwargs[\"image_paths\"], kwargs[\"metadata\"] = filter_images(**kwargs)\n",
    "    kwargs[\"atlas_dir\"] = get_atlas_data(**kwargs)\n",
    "    kwargs[\"vecs\"] = get_inception_vectors(**kwargs)\n",
    "    get_manifest(**kwargs)\n",
    "    write_images(**kwargs)\n",
    "    print(timestamp(), \"Done!\")\n",
    "\n",
    "\n",
    "def preprocess_kwargs(**kwargs):\n",
    "    \"\"\"Preprocess incoming key word arguments\"\"\"\n",
    "    for i in [\"n_neighbors\", \"min_dist\"]:\n",
    "        if not isinstance(kwargs[i], list):\n",
    "            kwargs[i] = [kwargs[i]]\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def copy_web_assets(**kwargs):\n",
    "    \"\"\"Copy the /web directory from the clipplot source to the users cwd\"\"\"\n",
    "    src = join(dirname(realpath(__file__)), \"web\")\n",
    "    dest = join(os.getcwd(), kwargs[\"out_dir\"])\n",
    "    copy_tree(src, dest)\n",
    "    # write version numbers into output\n",
    "    for i in [\"index.html\", os.path.join(\"assets\", \"js\", \"tsne.js\")]:\n",
    "        path = join(dest, i)\n",
    "        with open(path, \"r\") as f:\n",
    "            f = f.read().replace(\"VERSION_NUMBER\", get_version())\n",
    "            with open(path, \"w\") as out:\n",
    "                out.write(f)\n",
    "    if kwargs[\"copy_web_only\"]:\n",
    "        print(timestamp(), \"Done!\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Images\n",
    "##\n",
    "\n",
    "\n",
    "def filter_images(**kwargs):\n",
    "    \"\"\"Main method for filtering images given user metadata (if provided)\"\"\"\n",
    "    # validate that input image names are unique\n",
    "    image_paths = set()\n",
    "    duplicates = set()\n",
    "    for i in stream_images(image_paths=get_image_paths(**kwargs)):\n",
    "        if i.path in image_paths:\n",
    "            duplicates.add(i.path)\n",
    "        image_paths.add(i.path)\n",
    "    if duplicates:\n",
    "        raise Exception(\n",
    "            \"\"\"\n",
    "      Image filenames should be unique, but the following filenames are duplicated\\n\n",
    "      {}\n",
    "      \"\"\".format(\n",
    "                \"\\n\".join(duplicates)\n",
    "            )\n",
    "        )\n",
    "    if not kwargs.get(\"shuffle\", False):\n",
    "        image_paths = sorted(image_paths)\n",
    "    # process and filter the images\n",
    "    filtered_image_paths = []\n",
    "    for i in stream_images(image_paths=image_paths):\n",
    "        # get image height and width\n",
    "        w, h = i.original.size\n",
    "        # remove images with 0 height or width when resized to lod height\n",
    "        if (h == 0) or (w == 0):\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it contains 0 height or width\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        # remove images that have 0 height or width when resized\n",
    "        try:\n",
    "            resized = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        except ValueError:\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it contains 0 height or width when resized\".format(\n",
    "                    i.path\n",
    "                ),\n",
    "            )\n",
    "            continue\n",
    "        except OSError:\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it could not be resized\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        # remove images that are too wide for the atlas\n",
    "        if (w / h) > (kwargs[\"atlas_size\"] / kwargs[\"cell_size\"]):\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because its dimensions are oblong\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        filtered_image_paths.append(i.path)\n",
    "    # if there are no remaining images, throw an error\n",
    "    if len(filtered_image_paths) == 0:\n",
    "        raise Exception(\"No images were found! Please check your input image glob.\")\n",
    "    # handle the case user provided no metadata\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return [filtered_image_paths, []]\n",
    "    # handle user metadata: retain only records with image and metadata\n",
    "    l = get_metadata_list(**kwargs)\n",
    "    meta_bn = set([clean_filename(i.get(\"filename\", \"\")) for i in l])\n",
    "    img_bn = set([clean_filename(i, **kwargs) for i in filtered_image_paths])\n",
    "    # identify images with metadata and those without metadata\n",
    "    meta_present = img_bn.intersection(meta_bn)\n",
    "    meta_missing = list(img_bn - meta_bn)\n",
    "    # notify the user of images that are missing metadata\n",
    "    if meta_missing:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \" ! Some images are missing metadata:\\n  -\",\n",
    "            \"\\n  - \".join(meta_missing[:10]),\n",
    "        )\n",
    "        if len(meta_missing) > 10:\n",
    "            print(timestamp(), \" ...\", len(meta_missing) - 10, \"more\")\n",
    "        with open(\"missing-metadata.txt\", \"w\") as out:\n",
    "            out.write(\"\\n\".join(meta_missing))\n",
    "    # get the sorted lists of images and metadata\n",
    "    d = {clean_filename(i[\"filename\"]): i for i in l}\n",
    "    images = []\n",
    "    metadata = []\n",
    "    for i in filtered_image_paths:\n",
    "        if clean_filename(i, **kwargs) in meta_present:\n",
    "            images.append(i)\n",
    "            metadata.append(copy.deepcopy(d[clean_filename(i, **kwargs)]))\n",
    "    kwargs[\"metadata\"] = metadata\n",
    "    write_metadata(**kwargs)\n",
    "    return [images, metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_image_paths(**kwargs):\n",
    "    \"\"\"Called once to provide a list of image paths--handles IIIF manifest input\"\"\"\n",
    "    # handle case where --images points to iiif manifest\n",
    "    image_paths = None\n",
    "    if not kwargs[\"images\"]:\n",
    "        print(\"\\nError: please provide an images argument, e.g.:\")\n",
    "        print('clipplot --images \"cat_pictures/*.jpg\"\\n')\n",
    "        sys.exit()\n",
    "    # handle list of IIIF image inputs\n",
    "    if os.path.exists(kwargs[\"images\"]):\n",
    "        with open(kwargs[\"images\"]) as f:\n",
    "            f = [i.strip() for i in f.read().split(\"\\n\") if i.strip()]\n",
    "            for i in f:\n",
    "                if i.startswith(\"http\"):\n",
    "                    try:\n",
    "                        Manifest(url=i).save_images(limit=1)\n",
    "                    except:\n",
    "                        print(timestamp(), \"Could not download url \" + i)\n",
    "            image_paths = sorted(\n",
    "                glob2.glob(os.path.join(\"iiif-downloads\", \"images\", \"*\"))\n",
    "            )\n",
    "    # handle case where images flag points to a glob of images\n",
    "    if not image_paths:\n",
    "        image_paths = sorted(glob2.glob(kwargs[\"images\"]))\n",
    "    # handle case user provided no images\n",
    "    if not image_paths:\n",
    "        print(\"\\nError: No input images were found. Please check your --images glob\\n\")\n",
    "        sys.exit()\n",
    "    # optionally shuffle the image_paths\n",
    "    if kwargs.get(\"shuffle\", False):\n",
    "        print(timestamp(), \"Shuffling input images\")\n",
    "        random.Random(kwargs[\"seed\"]).shuffle(image_paths)\n",
    "    # optionally limit the number of images in image_paths\n",
    "    if kwargs.get(\"max_images\", False):\n",
    "        image_paths = image_paths[: kwargs[\"max_images\"]]\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def stream_images(**kwargs):\n",
    "    \"\"\"Read in all images from args[0], a list of image paths\"\"\"\n",
    "    for idx, i in enumerate(kwargs[\"image_paths\"]):\n",
    "        try:\n",
    "            metadata = None\n",
    "            if kwargs.get(\"metadata\", False) and kwargs[\"metadata\"][idx]:\n",
    "                metadata = kwargs[\"metadata\"][idx]\n",
    "            yield Image(i, metadata=metadata)\n",
    "        except Exception as exc:\n",
    "            print(timestamp(), \"Image\", i, \"could not be processed --\", exc)\n",
    "\n",
    "\n",
    "def clean_filename(s, **kwargs):\n",
    "    \"\"\"Given a string that points to a filename, return a clean filename\"\"\"\n",
    "    s = unquote(os.path.basename(s))\n",
    "    invalid_chars = '<>:;,\"/\\\\|?*[]'\n",
    "    for i in invalid_chars:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Metadata\n",
    "##\n",
    "\n",
    "\n",
    "def get_metadata_list(**kwargs):\n",
    "    \"\"\"Return a list of objects with image metadata\"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return []\n",
    "    # handle csv metadata\n",
    "    l = []\n",
    "    if kwargs[\"metadata\"].endswith(\".csv\"):\n",
    "        with open(kwargs[\"metadata\"]) as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = [i.lower() for i in next(reader)]\n",
    "            for i in reader:\n",
    "                l.append(\n",
    "                    {\n",
    "                        headers[j]: i[j] if len(i) > j and i[j] else \"\"\n",
    "                        for j, _ in enumerate(headers)\n",
    "                    }\n",
    "                )\n",
    "    # handle json metadata\n",
    "    else:\n",
    "        for i in glob2.glob(kwargs[\"metadata\"]):\n",
    "            with open(i) as f:\n",
    "                l.append(json.load(f))\n",
    "    # if the user provided a category but not a tag, use the category as the tag\n",
    "    for i in l:\n",
    "        if i.get(\"category\", False) and not i.get(\"tags\", False):\n",
    "            i.update({\"tags\": i[\"category\"]})\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_metadata(metadata, **kwargs):\n",
    "    \"\"\"Write list `metadata` of objects to disk\"\"\"\n",
    "    if not metadata:\n",
    "        return\n",
    "    out_dir = join(kwargs[\"out_dir\"], \"metadata\")\n",
    "    for i in [\"filters\", \"options\", \"file\"]:\n",
    "        out_path = join(out_dir, i)\n",
    "        if not exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "    # create the lists of images with each tag\n",
    "    d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        filename = clean_filename(i[\"filename\"])\n",
    "        i[\"tags\"] = [j.strip() for j in i.get(\"tags\", \"\").split(\"|\")]\n",
    "        for j in i[\"tags\"]:\n",
    "            d[\"__\".join(j.split())].append(filename)\n",
    "        write_json(os.path.join(out_dir, \"file\", filename + \".json\"), i, **kwargs)\n",
    "    write_json(\n",
    "        os.path.join(out_dir, \"filters\", \"filters.json\"),\n",
    "        [\n",
    "            {\n",
    "                \"filter_name\": \"select\",\n",
    "                \"filter_values\": list(d.keys()),\n",
    "            }\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    # create the options for the category dropdown\n",
    "    for i in d:\n",
    "        write_json(os.path.join(out_dir, \"options\", i + \".json\"), d[i], **kwargs)\n",
    "    # create the map from date to images with that date (if dates present)\n",
    "    date_d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        date = i.get(\"year\", \"\")\n",
    "        if date:\n",
    "            date_d[date].append(clean_filename(i[\"filename\"]))\n",
    "    # find the min and max dates to show on the date slider\n",
    "    dates = np.array([int(i.strip()) for i in date_d if is_number(i)])\n",
    "    domain = {\"min\": float(\"inf\"), \"max\": -float(\"inf\")}\n",
    "    mean = np.mean(dates)\n",
    "    std = np.std(dates)\n",
    "    for i in dates:\n",
    "        # update the date domain with all non-outlier dates\n",
    "        if abs(mean - i) < (std * 4):\n",
    "            domain[\"min\"] = int(min(i, domain[\"min\"]))\n",
    "            domain[\"max\"] = int(max(i, domain[\"max\"]))\n",
    "    # write the dates json\n",
    "    if len(date_d) > 1:\n",
    "        write_json(\n",
    "            os.path.join(out_dir, \"dates.json\"),\n",
    "            {\n",
    "                \"domain\": domain,\n",
    "                \"dates\": date_d,\n",
    "            },\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Return a boolean indicating if a string is a number\"\"\"\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Main\n",
    "##\n",
    "\n",
    "\n",
    "def get_manifest(**kwargs):\n",
    "    \"\"\"Create and return the base object for the manifest output file\"\"\"\n",
    "    # load the atlas data\n",
    "    atlas_data = json.load(open(join(kwargs[\"atlas_dir\"], \"atlas_positions.json\")))\n",
    "    # store each cell's size and atlas position\n",
    "    atlas_ids = set([i[\"idx\"] for i in atlas_data])\n",
    "    sizes = [[] for _ in atlas_ids]\n",
    "    pos = [[] for _ in atlas_ids]\n",
    "    for idx, i in enumerate(atlas_data):\n",
    "        sizes[i[\"idx\"]].append([i[\"w\"], i[\"h\"]])\n",
    "        pos[i[\"idx\"]].append([i[\"x\"], i[\"y\"]])\n",
    "    # obtain the paths to each layout's JSON positions\n",
    "    layouts = get_layouts(**kwargs)\n",
    "    # create a heightmap for the umap layout\n",
    "    if \"umap\" in layouts and layouts[\"umap\"]:\n",
    "        get_heightmap(layouts[\"umap\"][\"variants\"][0][\"layout\"], \"umap\", **kwargs)\n",
    "    # specify point size scalars\n",
    "    point_sizes = {}\n",
    "    point_sizes[\"min\"] = 0\n",
    "    point_sizes[\"grid\"] = 1 / math.ceil(len(kwargs[\"image_paths\"]) ** (1 / 2))\n",
    "    point_sizes[\"max\"] = point_sizes[\"grid\"] * 1.2\n",
    "    point_sizes[\"scatter\"] = point_sizes[\"grid\"] * 0.2\n",
    "    point_sizes[\"initial\"] = point_sizes[\"scatter\"]\n",
    "    point_sizes[\"categorical\"] = point_sizes[\"grid\"] * 0.6\n",
    "    point_sizes[\"geographic\"] = point_sizes[\"grid\"] * 0.025\n",
    "    # fetch the date distribution data for point sizing\n",
    "    if \"date\" in layouts and layouts[\"date\"]:\n",
    "        date_layout = read_json(layouts[\"date\"][\"labels\"], **kwargs)\n",
    "        point_sizes[\"date\"] = 1 / (\n",
    "            (date_layout[\"cols\"] + 1) * len(date_layout[\"labels\"])\n",
    "        )\n",
    "    # create manifest json\n",
    "    manifest = {\n",
    "        \"version\": get_version(),\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "        \"output_directory\": os.path.split(kwargs[\"out_dir\"])[0],\n",
    "        \"layouts\": layouts,\n",
    "        \"initial_layout\": \"umap\",\n",
    "        \"point_sizes\": point_sizes,\n",
    "        \"imagelist\": get_path(\"imagelists\", \"imagelist\", **kwargs),\n",
    "        \"atlas_dir\": kwargs[\"atlas_dir\"],\n",
    "        \"metadata\": True if kwargs[\"metadata\"] else False,\n",
    "        \"default_hotspots\": get_hotspots(layouts=layouts, **kwargs),\n",
    "        \"custom_hotspots\": get_path(\n",
    "            \"hotspots\", \"user_hotspots\", add_hash=False, **kwargs\n",
    "        ),\n",
    "        \"gzipped\": kwargs[\"gzip\"],\n",
    "        \"config\": {\n",
    "            \"sizes\": {\n",
    "                \"atlas\": kwargs[\"atlas_size\"],\n",
    "                \"cell\": kwargs[\"cell_size\"],\n",
    "                \"lod\": kwargs[\"lod_cell_height\"],\n",
    "            },\n",
    "        },\n",
    "        \"creation_date\": datetime.datetime.today().strftime(\"%d-%B-%Y-%H:%M:%S\"),\n",
    "    }\n",
    "    # write the manifest without gzipping\n",
    "    no_gzip_kwargs = {\n",
    "        \"out_dir\": kwargs[\"out_dir\"],\n",
    "        \"gzip\": False,\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "    }\n",
    "    path = get_path(\"manifests\", \"manifest\", **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    path = get_path(None, \"manifest\", add_hash=False, **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    # create images json\n",
    "    imagelist = {\n",
    "        \"cell_sizes\": sizes,\n",
    "        \"images\": [clean_filename(i) for i in kwargs[\"image_paths\"]],\n",
    "        \"atlas\": {\n",
    "            \"count\": len(atlas_ids),\n",
    "            \"positions\": pos,\n",
    "        },\n",
    "    }\n",
    "    write_json(manifest[\"imagelist\"], imagelist, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Atlases\n",
    "##\n",
    "\n",
    "\n",
    "def get_atlas_data(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate and save to disk all atlases to be used for this visualization\n",
    "    If square, center each cell in an nxn square, else use uniform height\n",
    "    \"\"\"\n",
    "    # if the atlas files already exist, load from cache\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"atlases\", kwargs[\"plot_id\"])\n",
    "    if (\n",
    "        os.path.exists(out_dir)\n",
    "        and kwargs[\"use_cache\"]\n",
    "        and not kwargs.get(\"shuffle\", False)\n",
    "    ):\n",
    "        print(timestamp(), \"Loading saved atlas data\")\n",
    "        return out_dir\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # else create the atlas images and store the positions of cells in atlases\n",
    "    print(timestamp(), \"Creating atlas files\")\n",
    "    n = 0  # number of atlases\n",
    "    x = 0  # x pos in atlas\n",
    "    y = 0  # y pos in atlas\n",
    "    positions = []  # l[cell_idx] = atlas data\n",
    "    atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        cell_data = i.resize_to_height(kwargs[\"cell_size\"])\n",
    "        _, v, _ = cell_data.shape\n",
    "        appendable = False\n",
    "        if (x + v) <= kwargs[\"atlas_size\"]:\n",
    "            appendable = True\n",
    "        elif (y + (2 * kwargs[\"cell_size\"])) <= kwargs[\"atlas_size\"]:\n",
    "            y += kwargs[\"cell_size\"]\n",
    "            x = 0\n",
    "            appendable = True\n",
    "        if not appendable:\n",
    "            save_atlas(atlas, out_dir, n)\n",
    "            n += 1\n",
    "            atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "            x = 0\n",
    "            y = 0\n",
    "        atlas[y : y + kwargs[\"cell_size\"], x : x + v] = cell_data\n",
    "        # find the size of the cell in the lod canvas\n",
    "        lod_data = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        h, w, _ = lod_data.shape  # h,w,colors in lod-cell sized image `i`\n",
    "        positions.append(\n",
    "            {\n",
    "                \"idx\": n,  # atlas idx\n",
    "                \"x\": x,  # x offset of cell in atlas\n",
    "                \"y\": y,  # y offset of cell in atlas\n",
    "                \"w\": w,  # w of cell at lod size\n",
    "                \"h\": h,  # h of cell at lod size\n",
    "            }\n",
    "        )\n",
    "        x += v\n",
    "    save_atlas(atlas, out_dir, n)\n",
    "    out_path = os.path.join(out_dir, \"atlas_positions.json\")\n",
    "    with open(out_path, \"w\") as out:\n",
    "        json.dump(positions, out)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def save_atlas(atlas, out_dir, n):\n",
    "    \"\"\"Save an atlas to disk\"\"\"\n",
    "    out_path = join(out_dir, \"atlas-{}.jpg\".format(n))\n",
    "    save_img(out_path, atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Layouts\n",
    "##\n",
    "\n",
    "\n",
    "def get_layouts(**kwargs):\n",
    "    \"\"\"Get the image positions in each projection\"\"\"\n",
    "    umap = get_umap_layout(**kwargs)\n",
    "    layouts = {\n",
    "        \"umap\": umap,\n",
    "        \"alphabetic\": {\n",
    "            \"layout\": get_alphabetic_layout(**kwargs),\n",
    "        },\n",
    "        \"grid\": {\n",
    "            \"layout\": get_rasterfairy_layout(umap=umap, **kwargs),\n",
    "        },\n",
    "        \"categorical\": get_categorical_layout(**kwargs),\n",
    "        \"date\": get_date_layout(**kwargs),\n",
    "        \"geographic\": get_geographic_layout(**kwargs),\n",
    "        \"custom\": get_custom_layout(**kwargs),\n",
    "    }\n",
    "    return layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_inception_vectors(**kwargs):\n",
    "    \"\"\"Create and return Inception vector representation of Image() instances\"\"\"\n",
    "    print(\n",
    "        timestamp(),\n",
    "        \"Creating Inception vectors for {} images\".format(len(kwargs[\"image_paths\"])),\n",
    "    )\n",
    "    vector_dir = os.path.join(kwargs[\"out_dir\"], \"image-vectors\", \"inception\")\n",
    "    if not os.path.exists(vector_dir):\n",
    "        os.makedirs(vector_dir)\n",
    "    base = InceptionV3(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    model = Model(inputs=base.input, outputs=base.get_layer(\"avg_pool\").output)\n",
    "    print(timestamp(), \"Creating image array\")\n",
    "    vecs = []\n",
    "    with tqdm(total=len(kwargs[\"image_paths\"])) as progress_bar:\n",
    "        for idx, i in enumerate(stream_images(**kwargs)):\n",
    "            vector_path = os.path.join(vector_dir, clean_filename(i.path) + \".npy\")\n",
    "            if os.path.exists(vector_path) and kwargs[\"use_cache\"]:\n",
    "                vec = np.load(vector_path)\n",
    "            else:\n",
    "                im = preprocess_input(img_to_array(i.original.resize((299, 299))))\n",
    "                vec = model.predict(np.expand_dims(im, 0)).squeeze()\n",
    "                np.save(vector_path, vec)\n",
    "            vecs.append(vec)\n",
    "            progress_bar.update(1)\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_umap_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images passed through a umap projection\"\"\"\n",
    "    vecs = kwargs[\"vecs\"]\n",
    "    w = PCA(n_components=min(100, len(vecs))).fit_transform(vecs)\n",
    "    # single model umap\n",
    "    if len(kwargs[\"n_neighbors\"]) == 1 and len(kwargs[\"min_dist\"]) == 1:\n",
    "        return process_single_layout_umap(w, **kwargs)\n",
    "    else:\n",
    "        return process_multi_layout_umap(w, **kwargs)\n",
    "\n",
    "\n",
    "def process_single_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a single layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating single umap layout\")\n",
    "    model = get_umap_model(**kwargs)\n",
    "    out_path = get_path(\"layouts\", \"umap\", **kwargs)\n",
    "    if cuml_ready:\n",
    "        z = model.fit(v).embedding_\n",
    "    else:\n",
    "        if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "            return out_path\n",
    "        y = []\n",
    "        if kwargs.get(\"metadata\", False):\n",
    "            labels = [i.get(\"label\", None) for i in kwargs[\"metadata\"]]\n",
    "            # if the user provided labels, integerize them\n",
    "            if any([i for i in labels]):\n",
    "                d = defaultdict(lambda: len(d))\n",
    "                for i in labels:\n",
    "                    if i == None:\n",
    "                        y.append(-1)\n",
    "                    else:\n",
    "                        y.append(d[i])\n",
    "                y = np.array(y)\n",
    "        # project the PCA space down to 2d for visualization\n",
    "        z = model.fit(v, y=y if np.any(y) else None).embedding_\n",
    "    return {\n",
    "        \"variants\": [\n",
    "            {\n",
    "                \"n_neighbors\": kwargs[\"n_neighbors\"][0],\n",
    "                \"min_dist\": kwargs[\"min_dist\"][0],\n",
    "                \"layout\": write_layout(out_path, z, **kwargs),\n",
    "                \"jittered\": get_pointgrid_layout(out_path, \"umap\", **kwargs),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def process_multi_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a multi-layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating multi-umap layout\")\n",
    "    params = []\n",
    "    for n_neighbors, min_dist in itertools.product(\n",
    "        kwargs[\"n_neighbors\"], kwargs[\"min_dist\"]\n",
    "    ):\n",
    "        filename = \"umap-n_neighbors_{}-min_dist_{}\".format(n_neighbors, min_dist)\n",
    "        out_path = get_path(\"layouts\", filename, **kwargs)\n",
    "        params.append(\n",
    "            {\n",
    "                \"n_neighbors\": n_neighbors,\n",
    "                \"min_dist\": min_dist,\n",
    "                \"filename\": filename,\n",
    "                \"out_path\": out_path,\n",
    "            }\n",
    "        )\n",
    "    # map each image's index to itself and create one copy of that map for each layout\n",
    "    relations_dict = {idx: idx for idx, _ in enumerate(v)}\n",
    "    # determine the subset of params that have already been computed\n",
    "    uncomputed_params = [i for i in params if not os.path.exists(i[\"out_path\"])]\n",
    "    # determine the filepath where this model will be saved\n",
    "    model_filename = \"umap-\" + str(abs(hash(kwargs[\"images\"])))\n",
    "    model_path = get_path(\"models\", model_filename, **kwargs).replace(\".json\", \".gz\")\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"models\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # load or create the model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        for i in uncomputed_params:\n",
    "            model.update(v, relations_dict.copy())\n",
    "        # after updating, we can read the results from the end of the updated model\n",
    "        for idx, i in enumerate(uncomputed_params):\n",
    "            embedding = z.embeddings_[len(uncomputed_params) - idx]\n",
    "            write_layout(i[\"out_path\"], embedding, **kwargs)\n",
    "    else:\n",
    "        model = AlignedUMAP(\n",
    "            n_neighbors=[i[\"n_neighbors\"] for i in uncomputed_params],\n",
    "            min_dist=[i[\"min_dist\"] for i in uncomputed_params],\n",
    "        )\n",
    "        # fit the model on the data\n",
    "        z = model.fit(\n",
    "            [v for _ in params], relations=[relations_dict for _ in params[1:]]\n",
    "        )\n",
    "        for idx, i in enumerate(params):\n",
    "            write_layout(i[\"out_path\"], z.embeddings_[idx], **kwargs)\n",
    "        # save the model\n",
    "        save_model(model, model_path)\n",
    "    # load the list of layout variants\n",
    "    l = []\n",
    "    for i in params:\n",
    "        l.append(\n",
    "            {\n",
    "                \"n_neighbors\": i[\"n_neighbors\"],\n",
    "                \"min_dist\": i[\"min_dist\"],\n",
    "                \"layout\": i[\"out_path\"],\n",
    "                \"jittered\": get_pointgrid_layout(\n",
    "                    i[\"out_path\"], i[\"filename\"], **kwargs\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"variants\": l,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        params = model.get_params()\n",
    "        attributes_names = [\n",
    "            attr for attr in model.__dir__() if attr not in params and attr[0] != \"_\"\n",
    "        ]\n",
    "        attributes = {key: model.__getattribute__(key) for key in attributes_names}\n",
    "        attributes[\"embeddings_\"] = list(model.embeddings_)\n",
    "        for x in [\"fit\", \"fit_transform\", \"update\", \"get_params\", \"set_params\"]:\n",
    "            del attributes[x]\n",
    "        all_params = {\n",
    "            \"umap_params\": params,\n",
    "            \"umap_attributes\": {key: value for key, value in attributes.items()},\n",
    "        }\n",
    "        pickle.dump(all_params, open(path, \"wb\"))\n",
    "    except:\n",
    "        print(timestamp(), \"Could not save model\")\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    params = pickle.load(open(path, \"rb\"))\n",
    "    model = AlignedUMAP()\n",
    "    model.set_params(**params.get(\"umap_params\"))\n",
    "    for attr, value in params.get(\"umap_attributes\").items():\n",
    "        model.__setattr__(attr, value)\n",
    "    model.__setattr__(\n",
    "        \"embeddings_\", List(params.get(\"umap_attributes\").get(\"embeddings_\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_umap_model(**kwargs):\n",
    "    if cuml_ready:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            verbose=5,\n",
    "        )\n",
    "    else:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            metric=kwargs[\"metric\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            transform_seed=kwargs[\"seed\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rasterfairy_layout(**kwargs):\n",
    "    \"\"\"Get the x, y position of images passed through a rasterfairy projection\"\"\"\n",
    "    print(timestamp(), \"Creating rasterfairy layout\")\n",
    "    out_path = get_path(\"layouts\", \"rasterfairy\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    if umap.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create rasterfairy layout because data is not 2D\")\n",
    "        return None\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    try:\n",
    "        umap = coonswarp.rectifyCloud(\n",
    "            umap,  # stretch the distribution\n",
    "            perimeterSubdivisionSteps=4,\n",
    "            autoPerimeterOffset=False,\n",
    "            paddingScale=1.05,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Coonswarp rectification could not be performed\", exc)\n",
    "    pos = rasterfairy.transformPointCloud2D(umap)[0]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_lap_layout(**kwargs):\n",
    "    print(timestamp(), \"Creating linear assignment layout\")\n",
    "    try:\n",
    "        import lap\n",
    "    except:\n",
    "        raise Exception(\"LAP must be installed to use get_lap_layout\")\n",
    "    out_path = get_path(\"layouts\", \"linear-assignment\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    # load the umap layout\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    # determine length of each side in square grid\n",
    "    side = math.ceil(umap.shape[0] ** (1 / 2))\n",
    "    # create square grid 0:1 in each dimension\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(0, 1, side), np.linspace(0, 1, side))\n",
    "    grid = np.dstack((grid_x, grid_y)).reshape(-1, 2)\n",
    "    # compute pairwise distance costs\n",
    "    cost = cdist(grid, umap, \"sqeuclidean\")\n",
    "    # increase cost\n",
    "    cost = cost * (10000000.0 / cost.max())\n",
    "    # run the linear assignment\n",
    "    min_cost, row_assignments, col_assignments = lap.lapjv(\n",
    "        np.copy(cost), extend_cost=True\n",
    "    )\n",
    "    # use the assignment vals to determine gridified positions of `arr`\n",
    "    pos = grid[col_assignments]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_alphabetic_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images in a grid projection\"\"\"\n",
    "    print(timestamp(), \"Creating grid layout\")\n",
    "    out_path = get_path(\"layouts\", \"grid\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    paths = kwargs[\"image_paths\"]\n",
    "    n = math.ceil(len(paths) ** (1 / 2))\n",
    "    l = []  # positions\n",
    "    for i, _ in enumerate(paths):\n",
    "        x = i % n\n",
    "        y = math.floor(i / n)\n",
    "        l.append([x, y])\n",
    "    z = np.array(l)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_pointgrid_layout(path, label, **kwargs):\n",
    "    \"\"\"Gridify the positions in `path` and return the path to this new layout\"\"\"\n",
    "    print(timestamp(), \"Creating {} pointgrid\".format(label))\n",
    "    out_path = get_path(\"layouts\", label + \"-jittered\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    arr = np.array(read_json(path, **kwargs))\n",
    "    if arr.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create pointgrid layout because data is not 2D\")\n",
    "        return None\n",
    "    z = align_points_to_grid(arr, fill=0.01)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_custom_layout(**kwargs):\n",
    "    out_path = get_path(\"layouts\", \"custom\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    if not kwargs.get(\"metadata\"):\n",
    "        return\n",
    "    found_coords = False\n",
    "    coords = []\n",
    "    for i in stream_images(**kwargs):\n",
    "        x = i.metadata.get(\"x\")\n",
    "        y = i.metadata.get(\"y\")\n",
    "        if x and y:\n",
    "            found_coords = True\n",
    "            coords.append([x, y])\n",
    "        else:\n",
    "            if found_coords:\n",
    "                print(\n",
    "                    timestamp(),\n",
    "                    \"Some images are missing coordinates; skipping custom layout\",\n",
    "                )\n",
    "    if not found_coords:\n",
    "        return\n",
    "    coords = np.array(coords).astype(np.float)\n",
    "    coords = (minmax_scale(coords) - 0.5) * 2\n",
    "    print(timestamp(), \"Creating custom layout\")\n",
    "    return {\n",
    "        \"layout\": write_layout(\n",
    "            out_path, coords.tolist(), scale=False, round=False, **kwargs\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Date Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_date_layout(cols=3, bin_units=\"years\", **kwargs):\n",
    "    \"\"\"\n",
    "    Get the x,y positions of input images based on their dates\n",
    "    @param int cols: the number of columns to plot for each bar\n",
    "    @param str bin_units: the temporal units to use when creating bins\n",
    "    \"\"\"\n",
    "    date_vals = [\n",
    "        kwargs[\"metadata\"][i].get(\"year\", False) for i in range(len(kwargs[\"metadata\"]))\n",
    "    ]\n",
    "    if not kwargs[\"metadata\"] or not any(date_vals):\n",
    "        return False\n",
    "    # if the data layouts have been cached, return them\n",
    "    positions_out_path = get_path(\"layouts\", \"timeline\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"timeline-labels\", **kwargs)\n",
    "    if (\n",
    "        os.path.exists(positions_out_path)\n",
    "        and os.path.exists(labels_out_path)\n",
    "        and kwargs[\"use_cache\"]\n",
    "    ):\n",
    "        return {\n",
    "            \"layout\": positions_out_path,\n",
    "            \"labels\": labels_out_path,\n",
    "        }\n",
    "    # date layout is not cached, so fetch dates and process\n",
    "    print(timestamp(), \"Creating date layout with {} columns\".format(cols))\n",
    "    datestrings = [i.metadata.get(\"year\", \"no_date\") for i in stream_images(**kwargs)]\n",
    "    dates = [datestring_to_date(i) for i in datestrings]\n",
    "    rounded_dates = [round_date(i, bin_units) for i in dates]\n",
    "    # create d[formatted_date] = [indices into datestrings of dates that round to formatted_date]\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(rounded_dates):\n",
    "        d[i].append(idx)\n",
    "    # determine the number of distinct grid positions in the x and y axes\n",
    "    n_coords_x = (cols + 1) * len(d)\n",
    "    n_coords_y = 1 + max([len(d[i]) for i in d]) // cols\n",
    "    if n_coords_y > n_coords_x:\n",
    "        return get_date_layout(cols=int(cols * 2), **kwargs)\n",
    "    # create a mesh of grid positions in clip space -1:1 given the time distribution\n",
    "    grid_x = (np.arange(0, n_coords_x) / (n_coords_x - 1)) * 2\n",
    "    grid_y = (np.arange(0, n_coords_y) / (n_coords_x - 1)) * 2\n",
    "    # divide each grid axis by half its max length to center at the origin 0,0\n",
    "    grid_x = grid_x - np.max(grid_x) / 2.0\n",
    "    grid_y = grid_y - np.max(grid_y) / 2.0\n",
    "    # make dates increase from left to right by sorting keys of d\n",
    "    d_keys = np.array(list(d.keys()))\n",
    "    seconds = np.array([date_to_seconds(dates[d[i][0]]) for i in d_keys])\n",
    "    d_keys = d_keys[np.argsort(seconds)]\n",
    "    # determine which images will fill which units of the grid established above\n",
    "    coords = np.zeros(\n",
    "        (len(datestrings), 2)\n",
    "    )  # 2D array with x, y clip-space coords of each date\n",
    "    for jdx, j in enumerate(d_keys):\n",
    "        for kdx, k in enumerate(d[j]):\n",
    "            x = jdx * (cols + 1) + (kdx % cols)\n",
    "            y = kdx // cols\n",
    "            coords[k] = [grid_x[x], grid_y[y]]\n",
    "    # find the positions of labels\n",
    "    label_positions = np.array(\n",
    "        [[grid_x[i * (cols + 1)], grid_y[0]] for i in range(len(d))]\n",
    "    )\n",
    "    # move the labels down in the y dimension by a grid unit\n",
    "    dx = grid_x[1] - grid_x[0]  # size of a single cell\n",
    "    label_positions[:, 1] = label_positions[:, 1] - dx\n",
    "    # quantize the label positions and label positions\n",
    "    image_positions = round_floats(coords)\n",
    "    label_positions = round_floats(label_positions.tolist())\n",
    "    # write and return the paths to the date based layout\n",
    "    return {\n",
    "        \"layout\": write_json(positions_out_path, image_positions, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": label_positions,\n",
    "                \"labels\": d_keys.tolist(),\n",
    "                \"cols\": cols,\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def datestring_to_date(datestring):\n",
    "    \"\"\"\n",
    "    Given a string representing a date return a datetime object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parse_date(\n",
    "            str(datestring), fuzzy=True, default=datetime.datetime(9999, 1, 1)\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Could not parse datestring {}\".format(datestring))\n",
    "        return datestring\n",
    "\n",
    "\n",
    "def date_to_seconds(date):\n",
    "    \"\"\"\n",
    "    Given a datetime object return an integer representation for that datetime\n",
    "    \"\"\"\n",
    "    if isinstance(date, datetime.datetime):\n",
    "        return (date - datetime.datetime.today()).total_seconds()\n",
    "    else:\n",
    "        return -float(\"inf\")\n",
    "\n",
    "\n",
    "def round_date(date, unit):\n",
    "    \"\"\"\n",
    "    Return `date` truncated to the temporal unit specified in `units`\n",
    "    \"\"\"\n",
    "    if not isinstance(date, datetime.datetime):\n",
    "        return \"no_date\"\n",
    "    formatted = date.strftime(\"%d %B %Y -- %X\")\n",
    "    if unit in set([\"seconds\", \"minutes\", \"hours\"]):\n",
    "        date = formatted.split(\"--\")[1].strip()\n",
    "        if unit == \"seconds\":\n",
    "            date = date\n",
    "        elif unit == \"minutes\":\n",
    "            date = \":\".join(d.split(\":\")[:-1]) + \":00\"\n",
    "        elif unit == \"hours\":\n",
    "            date = date.split(\":\")[0] + \":00:00\"\n",
    "    elif unit in set([\"days\", \"months\", \"years\", \"decades\", \"centuries\"]):\n",
    "        date = formatted.split(\"--\")[0].strip()\n",
    "        if unit == \"days\":\n",
    "            date = date\n",
    "        elif unit == \"months\":\n",
    "            date = \" \".join(date.split()[1:])\n",
    "        elif unit == \"years\":\n",
    "            date = date.split()[-1]\n",
    "        elif unit == \"decades\":\n",
    "            date = str(int(date.split()[-1]) // 10) + \"0\"\n",
    "        elif unit == \"centuries\":\n",
    "            date = str(int(date.split()[-1]) // 100) + \"00\"\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Metadata Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_categorical_layout(null_category=\"Other\", margin=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a numpy array with shape (n_points, 2) with the point\n",
    "    positions of observations in box regions determined by\n",
    "    each point's category metadata attribute (if applicable)\n",
    "    \"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return False\n",
    "    # determine the out path and return from cache if possible\n",
    "    out_path = get_path(\"layouts\", \"categorical\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"categorical-labels\", **kwargs)\n",
    "    # accumulate d[category] = [indices of points with category]\n",
    "    categories = [i.get(\"category\", None) for i in kwargs[\"metadata\"]]\n",
    "    if not any(categories) or len(set(categories) - set([None])) == 1:\n",
    "        return False\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(categories):\n",
    "        d[i].append(idx)\n",
    "    # store the number of observations in each group\n",
    "    keys_and_counts = [{\"key\": i, \"count\": len(d[i])} for i in d]\n",
    "    keys_and_counts.sort(key=operator.itemgetter(\"count\"), reverse=True)\n",
    "    # get the box layout then subdivide into discrete points\n",
    "    boxes = get_categorical_boxes([i[\"count\"] for i in keys_and_counts], margin=margin)\n",
    "    points = get_categorical_points(boxes)\n",
    "    # sort the points into the order of the observations in the metadata\n",
    "    counts = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    offsets = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    for idx, i in enumerate(keys_and_counts):\n",
    "        offsets[i[\"key\"]] += sum([j[\"count\"] for j in keys_and_counts[:idx]])\n",
    "    sorted_points = []\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        category = i.metadata.get(\"category\", null_category)\n",
    "        sorted_points.append(points[offsets[category] + counts[category]])\n",
    "        counts[category] += 1\n",
    "    sorted_points = np.array(sorted_points)\n",
    "    # add to the sorted points the anchors for the text labels for each group\n",
    "    text_anchors = np.array([[i.x, i.y - margin / 2] for i in boxes])\n",
    "    # add the anchors to the points - these will be removed after the points are projected\n",
    "    sorted_points = np.vstack([sorted_points, text_anchors])\n",
    "    # scale -1:1 using the largest axis as the scaling metric\n",
    "    _max = np.max(sorted_points)\n",
    "    for i in range(2):\n",
    "        _min = np.min(sorted_points[:, i])\n",
    "        sorted_points[:, i] -= _min\n",
    "        sorted_points[:, i] /= _max - _min\n",
    "        sorted_points[:, i] -= np.max(sorted_points[:, i]) / 2\n",
    "        sorted_points[:, i] *= 2\n",
    "    # separate out the sorted points and text positions\n",
    "    text_anchors = sorted_points[-len(text_anchors) :]\n",
    "    sorted_points = sorted_points[: -len(text_anchors)]\n",
    "    z = round_floats(sorted_points.tolist())\n",
    "    return {\n",
    "        \"layout\": write_json(out_path, z, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": round_floats(text_anchors.tolist()),\n",
    "                \"labels\": [i[\"key\"] for i in keys_and_counts],\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_categorical_boxes(group_counts, margin=2):\n",
    "    \"\"\"\n",
    "    @arg [int] group_counts: counts of the number of images in each\n",
    "      distinct level within the metadata's caetgories\n",
    "    @kwarg int margin: space between boxes in the 2D layout\n",
    "    @returns [Box] an array of Box() objects; one per level in `group_counts`\n",
    "    \"\"\"\n",
    "    group_counts = sorted(group_counts, reverse=True)\n",
    "    boxes = []\n",
    "    for i in group_counts:\n",
    "        w = h = math.ceil(i ** (1 / 2))\n",
    "        boxes.append(Box(i, w, h, None, None))\n",
    "    # find the position along x axis where we want to create a break\n",
    "    wrap = math.floor(sum([i.cells for i in boxes]) ** (1 / 2)) - (2 * margin)\n",
    "    # find the valid positions on the y axis\n",
    "    y = margin\n",
    "    y_spots = []\n",
    "    for i in boxes:\n",
    "        if (y + i.h + margin) <= wrap:\n",
    "            y_spots.append(y)\n",
    "            y += i.h + margin\n",
    "        else:\n",
    "            y_spots.append(y)\n",
    "            break\n",
    "    # get a list of lists where sublists contain elements at the same y position\n",
    "    y_spot_index = 0\n",
    "    for i in boxes:\n",
    "        # find the y position\n",
    "        y = y_spots[y_spot_index]\n",
    "        # find members with this y position\n",
    "        row_members = [j.x + j.w for j in boxes if j.y == y]\n",
    "        # assign the y position\n",
    "        i.y = y\n",
    "        y_spot_index = (y_spot_index + 1) % len(y_spots)\n",
    "        # assign the x position\n",
    "        i.x = max(row_members) + margin if row_members else margin\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_categorical_points(arr, unit_size=None):\n",
    "    \"\"\"Given an array of Box() objects, return a 2D distribution with shape (n_cells, 2)\"\"\"\n",
    "    points_arr = []\n",
    "    for i in arr:\n",
    "        area = i.w * i.h\n",
    "        per_unit = (area / i.cells) ** (1 / 2)\n",
    "        x_units = math.ceil(i.w / per_unit)\n",
    "        y_units = math.ceil(i.h / per_unit)\n",
    "        if not unit_size:\n",
    "            unit_size = min(i.w / x_units, i.h / y_units)\n",
    "        for j in range(i.cells):\n",
    "            x = j % x_units\n",
    "            y = j // x_units\n",
    "            points_arr.append(\n",
    "                [\n",
    "                    i.x + x * unit_size,\n",
    "                    i.y + y * unit_size,\n",
    "                ]\n",
    "            )\n",
    "    return np.array(points_arr)\n",
    "\n",
    "\n",
    "class Box:\n",
    "    \"\"\"Store the width, height, and x, y coords of a box\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.cells = args[0]\n",
    "        self.w = args[1]\n",
    "        self.h = args[2]\n",
    "        self.x = None if len(args) < 4 else args[3]\n",
    "        self.y = None if len(args) < 5 else args[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Geographic Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_geographic_layout(**kwargs):\n",
    "    \"\"\"Return a 2D array of image positions corresponding to lat, lng coordinates\"\"\"\n",
    "    out_path = get_path(\"layouts\", \"geographic\", **kwargs)\n",
    "    l = []\n",
    "    coords = False\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        lat = float(i.metadata.get(\"lat\", 0)) / 180\n",
    "        lng = (\n",
    "            float(i.metadata.get(\"lng\", 0)) / 180\n",
    "        )  # the plot draws longitude twice as tall as latitude\n",
    "        if lat or lng:\n",
    "            coords = True\n",
    "        l.append([lng, lat])\n",
    "    if coords:\n",
    "        print(timestamp(), \"Creating geographic layout\")\n",
    "        if kwargs[\"geojson\"]:\n",
    "            process_geojson(kwargs[\"geojson\"])\n",
    "        return {\"layout\": write_layout(out_path, l, scale=False, **kwargs)}\n",
    "    elif kwargs[\"geojson\"]:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \"GeoJSON is only processed if you also provide lat/lng coordinates for your images in a metadata file!\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path):\n",
    "    \"\"\"Given a GeoJSON filepath, write a minimal JSON output in lat lng coordinates\"\"\"\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson = json.load(f)\n",
    "    l = []\n",
    "    for i in geojson:\n",
    "        if isinstance(i, dict):\n",
    "            for j in i.get(\"coordinates\", []):\n",
    "                for k in j:\n",
    "                    l.append(k)\n",
    "    with open(\n",
    "        os.path.join(\"output\", \"assets\", \"json\", \"geographic-features.json\"), \"w\"\n",
    "    ) as out:\n",
    "        json.dump(l, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Helpers\n",
    "##\n",
    "\n",
    "\n",
    "def get_path(*args, **kwargs):\n",
    "    \"\"\"Return the path to a JSON file with conditional gz extension\"\"\"\n",
    "    sub_dir, filename = args\n",
    "    out_dir = join(kwargs[\"out_dir\"], sub_dir) if sub_dir else kwargs[\"out_dir\"]\n",
    "    if kwargs.get(\"add_hash\", True):\n",
    "        filename += \"-\" + kwargs[\"plot_id\"]\n",
    "    path = join(out_dir, filename + \".json\")\n",
    "    return path + \".gz\" if kwargs.get(\"gzip\", False) else path\n",
    "\n",
    "\n",
    "def write_layout(path, obj, **kwargs):\n",
    "    \"\"\"Write layout json `obj` to disk and return the path to the saved file\"\"\"\n",
    "    if kwargs.get(\"scale\", True) != False:\n",
    "        obj = (minmax_scale(obj) - 0.5) * 2  # scale -1:1\n",
    "    if kwargs.get(\"round\", True) != False:\n",
    "        obj = round_floats(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        obj = obj.tolist()\n",
    "    return write_json(path, obj, **kwargs)\n",
    "\n",
    "\n",
    "def round_floats(obj, digits=5):\n",
    "    \"\"\"Return 2D array obj with rounded float precision\"\"\"\n",
    "    return [[round(float(j), digits) for j in i] for i in obj]\n",
    "\n",
    "\n",
    "def write_json(path, obj, **kwargs):\n",
    "    \"\"\"Write json object `obj` to disk and return the path to that file\"\"\"\n",
    "    out_dir, filename = os.path.split(path)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"w\") as out:\n",
    "            out.write(json.dumps(obj, indent=4).encode(kwargs[\"encoding\"]))\n",
    "        return path\n",
    "    else:\n",
    "        with open(path, \"w\") as out:\n",
    "            json.dump(obj, out, indent=4)\n",
    "        return path\n",
    "\n",
    "\n",
    "def read_json(path, **kwargs):\n",
    "    \"\"\"Read and return the json object written by the current process at `path`\"\"\"\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"r\") as f:\n",
    "            return json.loads(f.read().decode(kwargs[\"encoding\"]))\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_hotspots(layouts={}, use_high_dimensional_vectors=True, **kwargs):\n",
    "    \"\"\"Return the stable clusters from the condensed tree of connected components from the density graph\"\"\"\n",
    "    print(timestamp(), \"Clustering data with {}\".format(cluster_method))\n",
    "    if use_high_dimensional_vectors:\n",
    "        vecs = kwargs[\"vecs\"]\n",
    "    else:\n",
    "        vecs = read_json(layouts[\"umap\"][\"variants\"][0][\"layout\"], **kwargs)\n",
    "    model = get_cluster_model(**kwargs)\n",
    "    z = model.fit(vecs)\n",
    "    # create a map from cluster label to image indices in cluster\n",
    "    d = defaultdict(lambda: defaultdict(list))\n",
    "    for idx, i in enumerate(z.labels_):\n",
    "        if i != -1:\n",
    "            d[i][\"images\"].append(idx)\n",
    "            d[i][\"img\"] = clean_filename(kwargs[\"image_paths\"][idx])\n",
    "            d[i][\"layout\"] = \"inception_vectors\"\n",
    "    # remove massive clusters\n",
    "    deletable = []\n",
    "    for i in d:\n",
    "        # find percent of images in cluster\n",
    "        image_percent = len(d[i][\"images\"]) / len(vecs)\n",
    "        # determine if image or area percent is too large\n",
    "        if image_percent > 0.5:\n",
    "            deletable.append(i)\n",
    "    for i in deletable:\n",
    "        del d[i]\n",
    "    # sort the clusers by size and then label the clusters\n",
    "    clusters = d.values()\n",
    "    clusters = sorted(clusters, key=lambda i: len(i[\"images\"]), reverse=True)\n",
    "    for idx, i in enumerate(clusters):\n",
    "        i[\"label\"] = \"Cluster {}\".format(idx + 1)\n",
    "    # slice off the first `max_clusters`\n",
    "    clusters = clusters[: kwargs[\"max_clusters\"]]\n",
    "    # save the hotspots to disk and return the path to the saved json\n",
    "    print(timestamp(), \"Found\", len(clusters), \"hotspots\")\n",
    "    return write_json(get_path(\"hotspots\", \"hotspot\", **kwargs), clusters, **kwargs)\n",
    "\n",
    "\n",
    "def get_cluster_model(**kwargs):\n",
    "    \"\"\"Return a model with .fit() method that can be used to cluster input vectors\"\"\"\n",
    "    config = {\n",
    "        \"core_dist_n_jobs\": multiprocessing.cpu_count(),\n",
    "        \"min_cluster_size\": kwargs[\"min_cluster_size\"],\n",
    "        \"cluster_selection_epsilon\": 0.01,\n",
    "        \"min_samples\": 1,\n",
    "        \"approx_min_span_tree\": False,\n",
    "    }\n",
    "    return HDBSCAN(**config)\n",
    "\n",
    "\n",
    "def get_heightmap(path, label, **kwargs):\n",
    "    \"\"\"Create a heightmap using the distribution of points stored at `path`\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    X = read_json(path, **kwargs)\n",
    "    if \"positions\" in X:\n",
    "        X = X[\"positions\"]\n",
    "    X = np.array(X)\n",
    "    if X.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create heightmap because data is not 2D\")\n",
    "        return\n",
    "    # create kernel density estimate of distribution X\n",
    "    nbins = 200\n",
    "    x, y = X.T\n",
    "    xi, yi = np.mgrid[x.min() : x.max() : nbins * 1j, y.min() : y.max() : nbins * 1j]\n",
    "    zi = kde.gaussian_kde(X.T)(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading=\"gouraud\", cmap=plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # save the plot\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"heightmaps\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_path = os.path.join(out_dir, label + \"-heightmap.png\")\n",
    "    plt.savefig(out_path, pad_inches=0)\n",
    "\n",
    "\n",
    "def write_images(**kwargs):\n",
    "    \"\"\"Write all originals and thumbs to the output dir\"\"\"\n",
    "    for i in stream_images(**kwargs):\n",
    "        filename = clean_filename(i.path)\n",
    "        # copy original for lightbox\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"originals\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        if not os.path.exists(out_path):\n",
    "            resized = i.resize_to_height(600)\n",
    "            resized = array_to_img(resized)\n",
    "            save_img(out_path, resized)\n",
    "        # copy thumb for lod texture\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"thumbs\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        img = array_to_img(i.resize_to_max(kwargs[\"lod_cell_height\"]))\n",
    "        save_img(out_path, img)\n",
    "\n",
    "\n",
    "def get_version():\n",
    "    \"\"\"\n",
    "    Return the version of clipplot installed\n",
    "    Hardcoded for now\n",
    "    \"\"\"\n",
    "    # return pkg_resources.get_distribution(\"clipplot\").version\n",
    "    return \"0.0.1\"\n",
    "\n",
    "\n",
    "class Image:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.path = args[0]\n",
    "        self.original = load_img(self.path)\n",
    "        self.metadata = kwargs[\"metadata\"] if kwargs[\"metadata\"] else {}\n",
    "\n",
    "    def resize_to_max(self, n):\n",
    "        \"\"\"\n",
    "        Resize self.original so its longest side has n pixels (maintain proportion)\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        size = (n, int(n * h / w)) if w > h else (int(n * w / h), n)\n",
    "        return img_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_height(self, height):\n",
    "        \"\"\"\n",
    "        Resize self.original into an image with height h and proportional width\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        if (w / h * height) < 1:\n",
    "            resizedwidth = 1\n",
    "        else:\n",
    "            resizedwidth = int(w / h * height)\n",
    "        size = (resizedwidth, height)\n",
    "        return img_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_square(self, n, center=False):\n",
    "        \"\"\"\n",
    "        Resize self.original to an image with nxn pixels (maintain proportion)\n",
    "        if center, center the colored pixels in the square, else left align\n",
    "        \"\"\"\n",
    "        a = self.resize_to_max(n)\n",
    "        h, w, c = a.shape\n",
    "        pad_lr = int((n - w) / 2)  # left right pad\n",
    "        pad_tb = int((n - h) / 2)  # top bottom pad\n",
    "        b = np.zeros((n, n, 3))\n",
    "        if center:\n",
    "            b[pad_tb : pad_tb + h, pad_lr : pad_lr + w, :] = a\n",
    "        else:\n",
    "            b[:h, :w, :] = a\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Entry Point\n",
    "##\n",
    "\n",
    "\n",
    "def parse():\n",
    "    \"\"\"Read command line args and begin data processing\"\"\"\n",
    "    description = \"Create the data required to create a clipplot viewer\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--images\",\n",
    "        \"-i\",\n",
    "        type=str,\n",
    "        default=config[\"images\"],\n",
    "        help=\"path to a glob of images to process\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metadata\",\n",
    "        \"-m\",\n",
    "        type=str,\n",
    "        default=config[\"metadata\"],\n",
    "        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_images\",\n",
    "        type=int,\n",
    "        default=config[\"max_images\"],\n",
    "        help=\"maximum number of images to process from the input glob\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cache\",\n",
    "        type=bool,\n",
    "        default=config[\"use_cache\"],\n",
    "        help=\"given inputs identical to prior inputs, load outputs from cache\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoding\",\n",
    "        type=str,\n",
    "        default=config[\"encoding\"],\n",
    "        help=\"the encoding of input metadata\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_cluster_size\",\n",
    "        type=int,\n",
    "        default=config[\"min_cluster_size\"],\n",
    "        help=\"the minimum number of images in a cluster\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_clusters\",\n",
    "        type=int,\n",
    "        default=config[\"max_clusters\"],\n",
    "        help=\"the maximum number of clusters to return\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_dir\",\n",
    "        type=str,\n",
    "        default=config[\"out_dir\"],\n",
    "        help=\"the directory to which outputs will be saved\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cell_size\",\n",
    "        type=int,\n",
    "        default=config[\"cell_size\"],\n",
    "        help=\"the size of atlas cells in px\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_neighbors\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=config[\"n_neighbors\"],\n",
    "        help=\"the n_neighbors arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_dist\",\n",
    "        nargs=\"+\",\n",
    "        type=float,\n",
    "        default=config[\"min_dist\"],\n",
    "        help=\"the min_dist arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_components\",\n",
    "        type=int,\n",
    "        default=config[\"n_components\"],\n",
    "        help=\"the n_components argument for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        default=config[\"metric\"],\n",
    "        help=\"the metric argument for umap\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pointgrid_fill\",\n",
    "        type=float,\n",
    "        default=config[\"pointgrid_fill\"],\n",
    "        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--copy_web_only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"update ./output/assets without reprocessing data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_size\",\n",
    "        type=float,\n",
    "        default=config[\"min_size\"],\n",
    "        help=\"min size of cropped images\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gzip\", action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\",\n",
    "        action=\"store_true\",\n",
    "        help=\"shuffle the input images before data processing begins\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plot_id\",\n",
    "        type=str,\n",
    "        default=config[\"plot_id\"],\n",
    "        help=\"unique id for a plot; useful for resuming processing on a started plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=config[\"seed\"], help=\"seed for random processes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_clusters\",\n",
    "        type=int,\n",
    "        default=config[\"n_clusters\"],\n",
    "        help=\"number of clusters to use when clustering with kmeans\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--geojson\",\n",
    "        type=str,\n",
    "        default=config[\"geojson\"],\n",
    "        help=\"path to a GeoJSON file with shapes to be rendered on a map\",\n",
    "    )\n",
    "    config.update(vars(parser.parse_args()))\n",
    "    process_images(**config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timestamp():\n",
    "    \"\"\"Return a string for printing the current time\"\"\"\n",
    "    return str(datetime.datetime.now()) + \":\"\n",
    "\n",
    "\n",
    "##\n",
    "# Image processing imports\n",
    "##\n",
    "\n",
    "if \"--copy_web_only\" not in sys.argv:\n",
    "\n",
    "    from sklearn.metrics import pairwise_distances_argmin_min\n",
    "    from collections import defaultdict, namedtuple\n",
    "    from dateutil.parser import parse as parse_date\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    from pointgrid import align_points_to_grid\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from sklearn.decomposition import PCA\n",
    "    from iiif_downloader import Manifest\n",
    "    from rasterfairy import coonswarp\n",
    "    from scipy.stats import kde\n",
    "    from PIL import ImageFile\n",
    "    import multiprocessing\n",
    "    from tqdm import tqdm\n",
    "    import rasterfairy\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    import operator\n",
    "    import pickle\n",
    "    import random\n",
    "    import copy\n",
    "    import math\n",
    "    import gzip\n",
    "    import json\n",
    "    import csv\n",
    "\n",
    "    from urllib.parse import unquote\n",
    "    from urllib.request import retrieve as download_function\n",
    "\n",
    "    # Keras imports\n",
    "    from tensorflow.keras.preprocessing.image import save_img, img_to_array, array_to_img\n",
    "    from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "    from tensorflow.keras.applications import InceptionV3, imagenet_utils\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "    ##\n",
    "    # Optional install imports\n",
    "    ##\n",
    "    from hdbscan import HDBSCAN\n",
    "    cluster_method = \"hdbscan\"\n",
    "\n",
    "    try:\n",
    "        from cuml.manifold.umap import UMAP\n",
    "\n",
    "        print(timestamp(), \"Using cuml UMAP\")\n",
    "        cuml_ready = True\n",
    "        from umap import AlignedUMAP\n",
    "    except:\n",
    "        from umap import UMAP, AlignedUMAP\n",
    "\n",
    "        print(timestamp(), \"CUML not available; using umap-learn UMAP\")\n",
    "        cuml_ready = False\n",
    "\n",
    "    # handle truncated images in PIL (managed by Pillow)\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\"\"\"\n",
    "NB: Keras Image class objects return image.size as w,h\n",
    "    Numpy array representations of images return image.shape as h,w,c\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"images\": None,\n",
    "    \"metadata\": None,\n",
    "    \"out_dir\": \"output\",\n",
    "    \"max_images\": None,\n",
    "    \"use_cache\": True,\n",
    "    \"encoding\": \"utf8\",\n",
    "    \"min_cluster_size\": 20,\n",
    "    \"max_clusters\": 10,\n",
    "    \"atlas_size\": 2048,\n",
    "    \"cell_size\": 32,\n",
    "    \"lod_cell_height\": 128,\n",
    "    \"n_neighbors\": [15],\n",
    "    \"min_dist\": [0.01],\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"correlation\",\n",
    "    \"pointgrid_fill\": 0.05,\n",
    "    \"gzip\": False,\n",
    "    \"min_size\": 100,\n",
    "    \"min_score\": 0.3,\n",
    "    \"min_vertices\": 18,\n",
    "    \"plot_id\": str(uuid.uuid1()),\n",
    "    \"seed\": 24,\n",
    "    \"n_clusters\": 12,\n",
    "    \"geojson\": None,\n",
    "}\n",
    "\n",
    "\n",
    "##\n",
    "# Entry\n",
    "##\n",
    "\n",
    "\n",
    "def process_images(**kwargs):\n",
    "    \"\"\"Main method for processing user images and metadata\"\"\"\n",
    "    kwargs = preprocess_kwargs(**kwargs)\n",
    "    copy_web_assets(**kwargs)\n",
    "    np.random.seed(kwargs[\"seed\"])\n",
    "    compat.v1.set_random_seed(kwargs[\"seed\"])\n",
    "    kwargs[\"out_dir\"] = join(kwargs[\"out_dir\"], \"data\")\n",
    "    kwargs[\"image_paths\"], kwargs[\"metadata\"] = filter_images(**kwargs)\n",
    "    kwargs[\"atlas_dir\"] = get_atlas_data(**kwargs)\n",
    "    kwargs[\"vecs\"] = get_inception_vectors(**kwargs)\n",
    "    get_manifest(**kwargs)\n",
    "    write_images(**kwargs)\n",
    "    print(timestamp(), \"Done!\")\n",
    "\n",
    "\n",
    "def preprocess_kwargs(**kwargs):\n",
    "    \"\"\"Preprocess incoming key word arguments\"\"\"\n",
    "    for i in [\"n_neighbors\", \"min_dist\"]:\n",
    "        if not isinstance(kwargs[i], list):\n",
    "            kwargs[i] = [kwargs[i]]\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def copy_web_assets(**kwargs):\n",
    "    \"\"\"Copy the /web directory from the clipplot source to the users cwd\"\"\"\n",
    "    src = join(dirname(realpath(__file__)), \"web\")\n",
    "    dest = join(os.getcwd(), kwargs[\"out_dir\"])\n",
    "    copy_tree(src, dest)\n",
    "    # write version numbers into output\n",
    "    for i in [\"index.html\", os.path.join(\"assets\", \"js\", \"tsne.js\")]:\n",
    "        path = join(dest, i)\n",
    "        with open(path, \"r\") as f:\n",
    "            f = f.read().replace(\"VERSION_NUMBER\", get_version())\n",
    "            with open(path, \"w\") as out:\n",
    "                out.write(f)\n",
    "    if kwargs[\"copy_web_only\"]:\n",
    "        print(timestamp(), \"Done!\")\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "##\n",
    "# Images\n",
    "##\n",
    "\n",
    "\n",
    "def filter_images(**kwargs):\n",
    "    \"\"\"Main method for filtering images given user metadata (if provided)\"\"\"\n",
    "    # validate that input image names are unique\n",
    "    image_paths = set()\n",
    "    duplicates = set()\n",
    "    for i in stream_images(image_paths=get_image_paths(**kwargs)):\n",
    "        if i.path in image_paths:\n",
    "            duplicates.add(i.path)\n",
    "        image_paths.add(i.path)\n",
    "    if duplicates:\n",
    "        raise Exception(\n",
    "            \"\"\"\n",
    "      Image filenames should be unique, but the following filenames are duplicated\\n\n",
    "      {}\n",
    "      \"\"\".format(\n",
    "                \"\\n\".join(duplicates)\n",
    "            )\n",
    "        )\n",
    "    if not kwargs.get(\"shuffle\", False):\n",
    "        image_paths = sorted(image_paths)\n",
    "    # process and filter the images\n",
    "    filtered_image_paths = []\n",
    "    for i in stream_images(image_paths=image_paths):\n",
    "        # get image height and width\n",
    "        w, h = i.original.size\n",
    "        # remove images with 0 height or width when resized to lod height\n",
    "        if (h == 0) or (w == 0):\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it contains 0 height or width\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        # remove images that have 0 height or width when resized\n",
    "        try:\n",
    "            resized = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        except ValueError:\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it contains 0 height or width when resized\".format(\n",
    "                    i.path\n",
    "                ),\n",
    "            )\n",
    "            continue\n",
    "        except OSError:\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because it could not be resized\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        # remove images that are too wide for the atlas\n",
    "        if (w / h) > (kwargs[\"atlas_size\"] / kwargs[\"cell_size\"]):\n",
    "            print(\n",
    "                timestamp(),\n",
    "                \"Skipping {} because its dimensions are oblong\".format(i.path),\n",
    "            )\n",
    "            continue\n",
    "        filtered_image_paths.append(i.path)\n",
    "    # if there are no remaining images, throw an error\n",
    "    if len(filtered_image_paths) == 0:\n",
    "        raise Exception(\"No images were found! Please check your input image glob.\")\n",
    "    # handle the case user provided no metadata\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return [filtered_image_paths, []]\n",
    "    # handle user metadata: retain only records with image and metadata\n",
    "    l = get_metadata_list(**kwargs)\n",
    "    meta_bn = set([clean_filename(i.get(\"filename\", \"\")) for i in l])\n",
    "    img_bn = set([clean_filename(i, **kwargs) for i in filtered_image_paths])\n",
    "    # identify images with metadata and those without metadata\n",
    "    meta_present = img_bn.intersection(meta_bn)\n",
    "    meta_missing = list(img_bn - meta_bn)\n",
    "    # notify the user of images that are missing metadata\n",
    "    if meta_missing:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \" ! Some images are missing metadata:\\n  -\",\n",
    "            \"\\n  - \".join(meta_missing[:10]),\n",
    "        )\n",
    "        if len(meta_missing) > 10:\n",
    "            print(timestamp(), \" ...\", len(meta_missing) - 10, \"more\")\n",
    "        with open(\"missing-metadata.txt\", \"w\") as out:\n",
    "            out.write(\"\\n\".join(meta_missing))\n",
    "    # get the sorted lists of images and metadata\n",
    "    d = {clean_filename(i[\"filename\"]): i for i in l}\n",
    "    images = []\n",
    "    metadata = []\n",
    "    for i in filtered_image_paths:\n",
    "        if clean_filename(i, **kwargs) in meta_present:\n",
    "            images.append(i)\n",
    "            metadata.append(copy.deepcopy(d[clean_filename(i, **kwargs)]))\n",
    "    kwargs[\"metadata\"] = metadata\n",
    "    write_metadata(**kwargs)\n",
    "    return [images, metadata]\n",
    "\n",
    "\n",
    "def get_image_paths(**kwargs):\n",
    "    \"\"\"Called once to provide a list of image paths--handles IIIF manifest input\"\"\"\n",
    "    # handle case where --images points to iiif manifest\n",
    "    image_paths = None\n",
    "    if not kwargs[\"images\"]:\n",
    "        print(\"\\nError: please provide an images argument, e.g.:\")\n",
    "        print('clipplot --images \"cat_pictures/*.jpg\"\\n')\n",
    "        sys.exit()\n",
    "    # handle list of IIIF image inputs\n",
    "    if os.path.exists(kwargs[\"images\"]):\n",
    "        with open(kwargs[\"images\"]) as f:\n",
    "            f = [i.strip() for i in f.read().split(\"\\n\") if i.strip()]\n",
    "            for i in f:\n",
    "                if i.startswith(\"http\"):\n",
    "                    try:\n",
    "                        Manifest(url=i).save_images(limit=1)\n",
    "                    except:\n",
    "                        print(timestamp(), \"Could not download url \" + i)\n",
    "            image_paths = sorted(\n",
    "                glob2.glob(os.path.join(\"iiif-downloads\", \"images\", \"*\"))\n",
    "            )\n",
    "    # handle case where images flag points to a glob of images\n",
    "    if not image_paths:\n",
    "        image_paths = sorted(glob2.glob(kwargs[\"images\"]))\n",
    "    # handle case user provided no images\n",
    "    if not image_paths:\n",
    "        print(\"\\nError: No input images were found. Please check your --images glob\\n\")\n",
    "        sys.exit()\n",
    "    # optionally shuffle the image_paths\n",
    "    if kwargs.get(\"shuffle\", False):\n",
    "        print(timestamp(), \"Shuffling input images\")\n",
    "        random.Random(kwargs[\"seed\"]).shuffle(image_paths)\n",
    "    # optionally limit the number of images in image_paths\n",
    "    if kwargs.get(\"max_images\", False):\n",
    "        image_paths = image_paths[: kwargs[\"max_images\"]]\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def stream_images(**kwargs):\n",
    "    \"\"\"Read in all images from args[0], a list of image paths\"\"\"\n",
    "    for idx, i in enumerate(kwargs[\"image_paths\"]):\n",
    "        try:\n",
    "            metadata = None\n",
    "            if kwargs.get(\"metadata\", False) and kwargs[\"metadata\"][idx]:\n",
    "                metadata = kwargs[\"metadata\"][idx]\n",
    "            yield Image(i, metadata=metadata)\n",
    "        except Exception as exc:\n",
    "            print(timestamp(), \"Image\", i, \"could not be processed --\", exc)\n",
    "\n",
    "\n",
    "def clean_filename(s, **kwargs):\n",
    "    \"\"\"Given a string that points to a filename, return a clean filename\"\"\"\n",
    "    s = unquote(os.path.basename(s))\n",
    "    invalid_chars = '<>:;,\"/\\\\|?*[]'\n",
    "    for i in invalid_chars:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "##\n",
    "# Metadata\n",
    "##\n",
    "\n",
    "\n",
    "def get_metadata_list(**kwargs):\n",
    "    \"\"\"Return a list of objects with image metadata\"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return []\n",
    "    # handle csv metadata\n",
    "    l = []\n",
    "    if kwargs[\"metadata\"].endswith(\".csv\"):\n",
    "        with open(kwargs[\"metadata\"]) as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = [i.lower() for i in next(reader)]\n",
    "            for i in reader:\n",
    "                l.append(\n",
    "                    {\n",
    "                        headers[j]: i[j] if len(i) > j and i[j] else \"\"\n",
    "                        for j, _ in enumerate(headers)\n",
    "                    }\n",
    "                )\n",
    "    # handle json metadata\n",
    "    else:\n",
    "        for i in glob2.glob(kwargs[\"metadata\"]):\n",
    "            with open(i) as f:\n",
    "                l.append(json.load(f))\n",
    "    # if the user provided a category but not a tag, use the category as the tag\n",
    "    for i in l:\n",
    "        if i.get(\"category\", False) and not i.get(\"tags\", False):\n",
    "            i.update({\"tags\": i[\"category\"]})\n",
    "    return l\n",
    "\n",
    "\n",
    "def write_metadata(metadata, **kwargs):\n",
    "    \"\"\"Write list `metadata` of objects to disk\"\"\"\n",
    "    if not metadata:\n",
    "        return\n",
    "    out_dir = join(kwargs[\"out_dir\"], \"metadata\")\n",
    "    for i in [\"filters\", \"options\", \"file\"]:\n",
    "        out_path = join(out_dir, i)\n",
    "        if not exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "    # create the lists of images with each tag\n",
    "    d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        filename = clean_filename(i[\"filename\"])\n",
    "        i[\"tags\"] = [j.strip() for j in i.get(\"tags\", \"\").split(\"|\")]\n",
    "        for j in i[\"tags\"]:\n",
    "            d[\"__\".join(j.split())].append(filename)\n",
    "        write_json(os.path.join(out_dir, \"file\", filename + \".json\"), i, **kwargs)\n",
    "    write_json(\n",
    "        os.path.join(out_dir, \"filters\", \"filters.json\"),\n",
    "        [\n",
    "            {\n",
    "                \"filter_name\": \"select\",\n",
    "                \"filter_values\": list(d.keys()),\n",
    "            }\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    # create the options for the category dropdown\n",
    "    for i in d:\n",
    "        write_json(os.path.join(out_dir, \"options\", i + \".json\"), d[i], **kwargs)\n",
    "    # create the map from date to images with that date (if dates present)\n",
    "    date_d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        date = i.get(\"year\", \"\")\n",
    "        if date:\n",
    "            date_d[date].append(clean_filename(i[\"filename\"]))\n",
    "    # find the min and max dates to show on the date slider\n",
    "    dates = np.array([int(i.strip()) for i in date_d if is_number(i)])\n",
    "    domain = {\"min\": float(\"inf\"), \"max\": -float(\"inf\")}\n",
    "    mean = np.mean(dates)\n",
    "    std = np.std(dates)\n",
    "    for i in dates:\n",
    "        # update the date domain with all non-outlier dates\n",
    "        if abs(mean - i) < (std * 4):\n",
    "            domain[\"min\"] = int(min(i, domain[\"min\"]))\n",
    "            domain[\"max\"] = int(max(i, domain[\"max\"]))\n",
    "    # write the dates json\n",
    "    if len(date_d) > 1:\n",
    "        write_json(\n",
    "            os.path.join(out_dir, \"dates.json\"),\n",
    "            {\n",
    "                \"domain\": domain,\n",
    "                \"dates\": date_d,\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Return a boolean indicating if a string is a number\"\"\"\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "##\n",
    "# Main\n",
    "##\n",
    "\n",
    "\n",
    "def get_manifest(**kwargs):\n",
    "    \"\"\"Create and return the base object for the manifest output file\"\"\"\n",
    "    # load the atlas data\n",
    "    atlas_data = json.load(open(join(kwargs[\"atlas_dir\"], \"atlas_positions.json\")))\n",
    "    # store each cell's size and atlas position\n",
    "    atlas_ids = set([i[\"idx\"] for i in atlas_data])\n",
    "    sizes = [[] for _ in atlas_ids]\n",
    "    pos = [[] for _ in atlas_ids]\n",
    "    for idx, i in enumerate(atlas_data):\n",
    "        sizes[i[\"idx\"]].append([i[\"w\"], i[\"h\"]])\n",
    "        pos[i[\"idx\"]].append([i[\"x\"], i[\"y\"]])\n",
    "    # obtain the paths to each layout's JSON positions\n",
    "    layouts = get_layouts(**kwargs)\n",
    "    # create a heightmap for the umap layout\n",
    "    if \"umap\" in layouts and layouts[\"umap\"]:\n",
    "        get_heightmap(layouts[\"umap\"][\"variants\"][0][\"layout\"], \"umap\", **kwargs)\n",
    "    # specify point size scalars\n",
    "    point_sizes = {}\n",
    "    point_sizes[\"min\"] = 0\n",
    "    point_sizes[\"grid\"] = 1 / math.ceil(len(kwargs[\"image_paths\"]) ** (1 / 2))\n",
    "    point_sizes[\"max\"] = point_sizes[\"grid\"] * 1.2\n",
    "    point_sizes[\"scatter\"] = point_sizes[\"grid\"] * 0.2\n",
    "    point_sizes[\"initial\"] = point_sizes[\"scatter\"]\n",
    "    point_sizes[\"categorical\"] = point_sizes[\"grid\"] * 0.6\n",
    "    point_sizes[\"geographic\"] = point_sizes[\"grid\"] * 0.025\n",
    "    # fetch the date distribution data for point sizing\n",
    "    if \"date\" in layouts and layouts[\"date\"]:\n",
    "        date_layout = read_json(layouts[\"date\"][\"labels\"], **kwargs)\n",
    "        point_sizes[\"date\"] = 1 / (\n",
    "            (date_layout[\"cols\"] + 1) * len(date_layout[\"labels\"])\n",
    "        )\n",
    "    # create manifest json\n",
    "    manifest = {\n",
    "        \"version\": get_version(),\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "        \"output_directory\": os.path.split(kwargs[\"out_dir\"])[0],\n",
    "        \"layouts\": layouts,\n",
    "        \"initial_layout\": \"umap\",\n",
    "        \"point_sizes\": point_sizes,\n",
    "        \"imagelist\": get_path(\"imagelists\", \"imagelist\", **kwargs),\n",
    "        \"atlas_dir\": kwargs[\"atlas_dir\"],\n",
    "        \"metadata\": True if kwargs[\"metadata\"] else False,\n",
    "        \"default_hotspots\": get_hotspots(layouts=layouts, **kwargs),\n",
    "        \"custom_hotspots\": get_path(\n",
    "            \"hotspots\", \"user_hotspots\", add_hash=False, **kwargs\n",
    "        ),\n",
    "        \"gzipped\": kwargs[\"gzip\"],\n",
    "        \"config\": {\n",
    "            \"sizes\": {\n",
    "                \"atlas\": kwargs[\"atlas_size\"],\n",
    "                \"cell\": kwargs[\"cell_size\"],\n",
    "                \"lod\": kwargs[\"lod_cell_height\"],\n",
    "            },\n",
    "        },\n",
    "        \"creation_date\": datetime.datetime.today().strftime(\"%d-%B-%Y-%H:%M:%S\"),\n",
    "    }\n",
    "    # write the manifest without gzipping\n",
    "    no_gzip_kwargs = {\n",
    "        \"out_dir\": kwargs[\"out_dir\"],\n",
    "        \"gzip\": False,\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "    }\n",
    "    path = get_path(\"manifests\", \"manifest\", **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    path = get_path(None, \"manifest\", add_hash=False, **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    # create images json\n",
    "    imagelist = {\n",
    "        \"cell_sizes\": sizes,\n",
    "        \"images\": [clean_filename(i) for i in kwargs[\"image_paths\"]],\n",
    "        \"atlas\": {\n",
    "            \"count\": len(atlas_ids),\n",
    "            \"positions\": pos,\n",
    "        },\n",
    "    }\n",
    "    write_json(manifest[\"imagelist\"], imagelist, **kwargs)\n",
    "\n",
    "\n",
    "##\n",
    "# Atlases\n",
    "##\n",
    "\n",
    "\n",
    "def get_atlas_data(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate and save to disk all atlases to be used for this visualization\n",
    "    If square, center each cell in an nxn square, else use uniform height\n",
    "    \"\"\"\n",
    "    # if the atlas files already exist, load from cache\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"atlases\", kwargs[\"plot_id\"])\n",
    "    if (\n",
    "        os.path.exists(out_dir)\n",
    "        and kwargs[\"use_cache\"]\n",
    "        and not kwargs.get(\"shuffle\", False)\n",
    "    ):\n",
    "        print(timestamp(), \"Loading saved atlas data\")\n",
    "        return out_dir\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # else create the atlas images and store the positions of cells in atlases\n",
    "    print(timestamp(), \"Creating atlas files\")\n",
    "    n = 0  # number of atlases\n",
    "    x = 0  # x pos in atlas\n",
    "    y = 0  # y pos in atlas\n",
    "    positions = []  # l[cell_idx] = atlas data\n",
    "    atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        cell_data = i.resize_to_height(kwargs[\"cell_size\"])\n",
    "        _, v, _ = cell_data.shape\n",
    "        appendable = False\n",
    "        if (x + v) <= kwargs[\"atlas_size\"]:\n",
    "            appendable = True\n",
    "        elif (y + (2 * kwargs[\"cell_size\"])) <= kwargs[\"atlas_size\"]:\n",
    "            y += kwargs[\"cell_size\"]\n",
    "            x = 0\n",
    "            appendable = True\n",
    "        if not appendable:\n",
    "            save_atlas(atlas, out_dir, n)\n",
    "            n += 1\n",
    "            atlas = np.zeros((kwargs[\"atlas_size\"], kwargs[\"atlas_size\"], 3))\n",
    "            x = 0\n",
    "            y = 0\n",
    "        atlas[y : y + kwargs[\"cell_size\"], x : x + v] = cell_data\n",
    "        # find the size of the cell in the lod canvas\n",
    "        lod_data = i.resize_to_max(kwargs[\"lod_cell_height\"])\n",
    "        h, w, _ = lod_data.shape  # h,w,colors in lod-cell sized image `i`\n",
    "        positions.append(\n",
    "            {\n",
    "                \"idx\": n,  # atlas idx\n",
    "                \"x\": x,  # x offset of cell in atlas\n",
    "                \"y\": y,  # y offset of cell in atlas\n",
    "                \"w\": w,  # w of cell at lod size\n",
    "                \"h\": h,  # h of cell at lod size\n",
    "            }\n",
    "        )\n",
    "        x += v\n",
    "    save_atlas(atlas, out_dir, n)\n",
    "    out_path = os.path.join(out_dir, \"atlas_positions.json\")\n",
    "    with open(out_path, \"w\") as out:\n",
    "        json.dump(positions, out)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def save_atlas(atlas, out_dir, n):\n",
    "    \"\"\"Save an atlas to disk\"\"\"\n",
    "    out_path = join(out_dir, \"atlas-{}.jpg\".format(n))\n",
    "    save_img(out_path, atlas)\n",
    "\n",
    "\n",
    "##\n",
    "# Layouts\n",
    "##\n",
    "\n",
    "\n",
    "def get_layouts(**kwargs):\n",
    "    \"\"\"Get the image positions in each projection\"\"\"\n",
    "    umap = get_umap_layout(**kwargs)\n",
    "    layouts = {\n",
    "        \"umap\": umap,\n",
    "        \"alphabetic\": {\n",
    "            \"layout\": get_alphabetic_layout(**kwargs),\n",
    "        },\n",
    "        \"grid\": {\n",
    "            \"layout\": get_rasterfairy_layout(umap=umap, **kwargs),\n",
    "        },\n",
    "        \"categorical\": get_categorical_layout(**kwargs),\n",
    "        \"date\": get_date_layout(**kwargs),\n",
    "        \"geographic\": get_geographic_layout(**kwargs),\n",
    "        \"custom\": get_custom_layout(**kwargs),\n",
    "    }\n",
    "    return layouts\n",
    "\n",
    "\n",
    "def get_inception_vectors(**kwargs):\n",
    "    \"\"\"Create and return Inception vector representation of Image() instances\"\"\"\n",
    "    print(\n",
    "        timestamp(),\n",
    "        \"Creating Inception vectors for {} images\".format(len(kwargs[\"image_paths\"])),\n",
    "    )\n",
    "    vector_dir = os.path.join(kwargs[\"out_dir\"], \"image-vectors\", \"inception\")\n",
    "    if not os.path.exists(vector_dir):\n",
    "        os.makedirs(vector_dir)\n",
    "    base = InceptionV3(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    model = Model(inputs=base.input, outputs=base.get_layer(\"avg_pool\").output)\n",
    "    print(timestamp(), \"Creating image array\")\n",
    "    vecs = []\n",
    "    with tqdm(total=len(kwargs[\"image_paths\"])) as progress_bar:\n",
    "        for idx, i in enumerate(stream_images(**kwargs)):\n",
    "            vector_path = os.path.join(vector_dir, clean_filename(i.path) + \".npy\")\n",
    "            if os.path.exists(vector_path) and kwargs[\"use_cache\"]:\n",
    "                vec = np.load(vector_path)\n",
    "            else:\n",
    "                im = preprocess_input(img_to_array(i.original.resize((299, 299))))\n",
    "                vec = model.predict(np.expand_dims(im, 0)).squeeze()\n",
    "                np.save(vector_path, vec)\n",
    "            vecs.append(vec)\n",
    "            progress_bar.update(1)\n",
    "    return np.array(vecs)\n",
    "\n",
    "\n",
    "def get_umap_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images passed through a umap projection\"\"\"\n",
    "    vecs = kwargs[\"vecs\"]\n",
    "    w = PCA(n_components=min(100, len(vecs))).fit_transform(vecs)\n",
    "    # single model umap\n",
    "    if len(kwargs[\"n_neighbors\"]) == 1 and len(kwargs[\"min_dist\"]) == 1:\n",
    "        return process_single_layout_umap(w, **kwargs)\n",
    "    else:\n",
    "        return process_multi_layout_umap(w, **kwargs)\n",
    "\n",
    "\n",
    "def process_single_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a single layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating single umap layout\")\n",
    "    model = get_umap_model(**kwargs)\n",
    "    out_path = get_path(\"layouts\", \"umap\", **kwargs)\n",
    "    if cuml_ready:\n",
    "        z = model.fit(v).embedding_\n",
    "    else:\n",
    "        if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "            return out_path\n",
    "        y = []\n",
    "        if kwargs.get(\"metadata\", False):\n",
    "            labels = [i.get(\"label\", None) for i in kwargs[\"metadata\"]]\n",
    "            # if the user provided labels, integerize them\n",
    "            if any([i for i in labels]):\n",
    "                d = defaultdict(lambda: len(d))\n",
    "                for i in labels:\n",
    "                    if i == None:\n",
    "                        y.append(-1)\n",
    "                    else:\n",
    "                        y.append(d[i])\n",
    "                y = np.array(y)\n",
    "        # project the PCA space down to 2d for visualization\n",
    "        z = model.fit(v, y=y if np.any(y) else None).embedding_\n",
    "    return {\n",
    "        \"variants\": [\n",
    "            {\n",
    "                \"n_neighbors\": kwargs[\"n_neighbors\"][0],\n",
    "                \"min_dist\": kwargs[\"min_dist\"][0],\n",
    "                \"layout\": write_layout(out_path, z, **kwargs),\n",
    "                \"jittered\": get_pointgrid_layout(out_path, \"umap\", **kwargs),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def process_multi_layout_umap(v, **kwargs):\n",
    "    \"\"\"Create a multi-layout UMAP projection\"\"\"\n",
    "    print(timestamp(), \"Creating multi-umap layout\")\n",
    "    params = []\n",
    "    for n_neighbors, min_dist in itertools.product(\n",
    "        kwargs[\"n_neighbors\"], kwargs[\"min_dist\"]\n",
    "    ):\n",
    "        filename = \"umap-n_neighbors_{}-min_dist_{}\".format(n_neighbors, min_dist)\n",
    "        out_path = get_path(\"layouts\", filename, **kwargs)\n",
    "        params.append(\n",
    "            {\n",
    "                \"n_neighbors\": n_neighbors,\n",
    "                \"min_dist\": min_dist,\n",
    "                \"filename\": filename,\n",
    "                \"out_path\": out_path,\n",
    "            }\n",
    "        )\n",
    "    # map each image's index to itself and create one copy of that map for each layout\n",
    "    relations_dict = {idx: idx for idx, _ in enumerate(v)}\n",
    "    # determine the subset of params that have already been computed\n",
    "    uncomputed_params = [i for i in params if not os.path.exists(i[\"out_path\"])]\n",
    "    # determine the filepath where this model will be saved\n",
    "    model_filename = \"umap-\" + str(abs(hash(kwargs[\"images\"])))\n",
    "    model_path = get_path(\"models\", model_filename, **kwargs).replace(\".json\", \".gz\")\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"models\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # load or create the model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        for i in uncomputed_params:\n",
    "            model.update(v, relations_dict.copy())\n",
    "        # after updating, we can read the results from the end of the updated model\n",
    "        for idx, i in enumerate(uncomputed_params):\n",
    "            embedding = z.embeddings_[len(uncomputed_params) - idx]\n",
    "            write_layout(i[\"out_path\"], embedding, **kwargs)\n",
    "    else:\n",
    "        model = AlignedUMAP(\n",
    "            n_neighbors=[i[\"n_neighbors\"] for i in uncomputed_params],\n",
    "            min_dist=[i[\"min_dist\"] for i in uncomputed_params],\n",
    "        )\n",
    "        # fit the model on the data\n",
    "        z = model.fit(\n",
    "            [v for _ in params], relations=[relations_dict for _ in params[1:]]\n",
    "        )\n",
    "        for idx, i in enumerate(params):\n",
    "            write_layout(i[\"out_path\"], z.embeddings_[idx], **kwargs)\n",
    "        # save the model\n",
    "        save_model(model, model_path)\n",
    "    # load the list of layout variants\n",
    "    l = []\n",
    "    for i in params:\n",
    "        l.append(\n",
    "            {\n",
    "                \"n_neighbors\": i[\"n_neighbors\"],\n",
    "                \"min_dist\": i[\"min_dist\"],\n",
    "                \"layout\": i[\"out_path\"],\n",
    "                \"jittered\": get_pointgrid_layout(\n",
    "                    i[\"out_path\"], i[\"filename\"], **kwargs\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"variants\": l,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        params = model.get_params()\n",
    "        attributes_names = [\n",
    "            attr for attr in model.__dir__() if attr not in params and attr[0] != \"_\"\n",
    "        ]\n",
    "        attributes = {key: model.__getattribute__(key) for key in attributes_names}\n",
    "        attributes[\"embeddings_\"] = list(model.embeddings_)\n",
    "        for x in [\"fit\", \"fit_transform\", \"update\", \"get_params\", \"set_params\"]:\n",
    "            del attributes[x]\n",
    "        all_params = {\n",
    "            \"umap_params\": params,\n",
    "            \"umap_attributes\": {key: value for key, value in attributes.items()},\n",
    "        }\n",
    "        pickle.dump(all_params, open(path, \"wb\"))\n",
    "    except:\n",
    "        print(timestamp(), \"Could not save model\")\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    params = pickle.load(open(path, \"rb\"))\n",
    "    model = AlignedUMAP()\n",
    "    model.set_params(**params.get(\"umap_params\"))\n",
    "    for attr, value in params.get(\"umap_attributes\").items():\n",
    "        model.__setattr__(attr, value)\n",
    "    model.__setattr__(\n",
    "        \"embeddings_\", List(params.get(\"umap_attributes\").get(\"embeddings_\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_umap_model(**kwargs):\n",
    "    if cuml_ready:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            verbose=5,\n",
    "        )\n",
    "    else:\n",
    "        return UMAP(\n",
    "            n_neighbors=kwargs[\"n_neighbors\"][0],\n",
    "            min_dist=kwargs[\"min_dist\"][0],\n",
    "            n_components=kwargs[\"n_components\"],\n",
    "            metric=kwargs[\"metric\"],\n",
    "            random_state=kwargs[\"seed\"],\n",
    "            transform_seed=kwargs[\"seed\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def get_rasterfairy_layout(**kwargs):\n",
    "    \"\"\"Get the x, y position of images passed through a rasterfairy projection\"\"\"\n",
    "    print(timestamp(), \"Creating rasterfairy layout\")\n",
    "    out_path = get_path(\"layouts\", \"rasterfairy\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    if umap.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create rasterfairy layout because data is not 2D\")\n",
    "        return None\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    try:\n",
    "        umap = coonswarp.rectifyCloud(\n",
    "            umap,  # stretch the distribution\n",
    "            perimeterSubdivisionSteps=4,\n",
    "            autoPerimeterOffset=False,\n",
    "            paddingScale=1.05,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Coonswarp rectification could not be performed\", exc)\n",
    "    pos = rasterfairy.transformPointCloud2D(umap)[0]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_lap_layout(**kwargs):\n",
    "    print(timestamp(), \"Creating linear assignment layout\")\n",
    "    try:\n",
    "        import lap\n",
    "    except:\n",
    "        raise Exception(\"LAP must be installed to use get_lap_layout\")\n",
    "    out_path = get_path(\"layouts\", \"linear-assignment\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    # load the umap layout\n",
    "    umap = np.array(read_json(kwargs[\"umap\"][\"variants\"][0][\"layout\"], **kwargs))\n",
    "    umap = (umap + 1) / 2  # scale 0:1\n",
    "    # determine length of each side in square grid\n",
    "    side = math.ceil(umap.shape[0] ** (1 / 2))\n",
    "    # create square grid 0:1 in each dimension\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(0, 1, side), np.linspace(0, 1, side))\n",
    "    grid = np.dstack((grid_x, grid_y)).reshape(-1, 2)\n",
    "    # compute pairwise distance costs\n",
    "    cost = cdist(grid, umap, \"sqeuclidean\")\n",
    "    # increase cost\n",
    "    cost = cost * (10000000.0 / cost.max())\n",
    "    # run the linear assignment\n",
    "    min_cost, row_assignments, col_assignments = lap.lapjv(\n",
    "        np.copy(cost), extend_cost=True\n",
    "    )\n",
    "    # use the assignment vals to determine gridified positions of `arr`\n",
    "    pos = grid[col_assignments]\n",
    "    return write_layout(out_path, pos, **kwargs)\n",
    "\n",
    "\n",
    "def get_alphabetic_layout(**kwargs):\n",
    "    \"\"\"Get the x,y positions of images in a grid projection\"\"\"\n",
    "    print(timestamp(), \"Creating grid layout\")\n",
    "    out_path = get_path(\"layouts\", \"grid\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    paths = kwargs[\"image_paths\"]\n",
    "    n = math.ceil(len(paths) ** (1 / 2))\n",
    "    l = []  # positions\n",
    "    for i, _ in enumerate(paths):\n",
    "        x = i % n\n",
    "        y = math.floor(i / n)\n",
    "        l.append([x, y])\n",
    "    z = np.array(l)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_pointgrid_layout(path, label, **kwargs):\n",
    "    \"\"\"Gridify the positions in `path` and return the path to this new layout\"\"\"\n",
    "    print(timestamp(), \"Creating {} pointgrid\".format(label))\n",
    "    out_path = get_path(\"layouts\", label + \"-jittered\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    arr = np.array(read_json(path, **kwargs))\n",
    "    if arr.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create pointgrid layout because data is not 2D\")\n",
    "        return None\n",
    "    z = align_points_to_grid(arr, fill=0.01)\n",
    "    return write_layout(out_path, z, **kwargs)\n",
    "\n",
    "\n",
    "def get_custom_layout(**kwargs):\n",
    "    out_path = get_path(\"layouts\", \"custom\", **kwargs)\n",
    "    if os.path.exists(out_path) and kwargs[\"use_cache\"]:\n",
    "        return out_path\n",
    "    if not kwargs.get(\"metadata\"):\n",
    "        return\n",
    "    found_coords = False\n",
    "    coords = []\n",
    "    for i in stream_images(**kwargs):\n",
    "        x = i.metadata.get(\"x\")\n",
    "        y = i.metadata.get(\"y\")\n",
    "        if x and y:\n",
    "            found_coords = True\n",
    "            coords.append([x, y])\n",
    "        else:\n",
    "            if found_coords:\n",
    "                print(\n",
    "                    timestamp(),\n",
    "                    \"Some images are missing coordinates; skipping custom layout\",\n",
    "                )\n",
    "    if not found_coords:\n",
    "        return\n",
    "    coords = np.array(coords).astype(np.float)\n",
    "    coords = (minmax_scale(coords) - 0.5) * 2\n",
    "    print(timestamp(), \"Creating custom layout\")\n",
    "    return {\n",
    "        \"layout\": write_layout(\n",
    "            out_path, coords.tolist(), scale=False, round=False, **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "##\n",
    "# Date Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_date_layout(cols=3, bin_units=\"years\", **kwargs):\n",
    "    \"\"\"\n",
    "    Get the x,y positions of input images based on their dates\n",
    "    @param int cols: the number of columns to plot for each bar\n",
    "    @param str bin_units: the temporal units to use when creating bins\n",
    "    \"\"\"\n",
    "    date_vals = [\n",
    "        kwargs[\"metadata\"][i].get(\"year\", False) for i in range(len(kwargs[\"metadata\"]))\n",
    "    ]\n",
    "    if not kwargs[\"metadata\"] or not any(date_vals):\n",
    "        return False\n",
    "    # if the data layouts have been cached, return them\n",
    "    positions_out_path = get_path(\"layouts\", \"timeline\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"timeline-labels\", **kwargs)\n",
    "    if (\n",
    "        os.path.exists(positions_out_path)\n",
    "        and os.path.exists(labels_out_path)\n",
    "        and kwargs[\"use_cache\"]\n",
    "    ):\n",
    "        return {\n",
    "            \"layout\": positions_out_path,\n",
    "            \"labels\": labels_out_path,\n",
    "        }\n",
    "    # date layout is not cached, so fetch dates and process\n",
    "    print(timestamp(), \"Creating date layout with {} columns\".format(cols))\n",
    "    datestrings = [i.metadata.get(\"year\", \"no_date\") for i in stream_images(**kwargs)]\n",
    "    dates = [datestring_to_date(i) for i in datestrings]\n",
    "    rounded_dates = [round_date(i, bin_units) for i in dates]\n",
    "    # create d[formatted_date] = [indices into datestrings of dates that round to formatted_date]\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(rounded_dates):\n",
    "        d[i].append(idx)\n",
    "    # determine the number of distinct grid positions in the x and y axes\n",
    "    n_coords_x = (cols + 1) * len(d)\n",
    "    n_coords_y = 1 + max([len(d[i]) for i in d]) // cols\n",
    "    if n_coords_y > n_coords_x:\n",
    "        return get_date_layout(cols=int(cols * 2), **kwargs)\n",
    "    # create a mesh of grid positions in clip space -1:1 given the time distribution\n",
    "    grid_x = (np.arange(0, n_coords_x) / (n_coords_x - 1)) * 2\n",
    "    grid_y = (np.arange(0, n_coords_y) / (n_coords_x - 1)) * 2\n",
    "    # divide each grid axis by half its max length to center at the origin 0,0\n",
    "    grid_x = grid_x - np.max(grid_x) / 2.0\n",
    "    grid_y = grid_y - np.max(grid_y) / 2.0\n",
    "    # make dates increase from left to right by sorting keys of d\n",
    "    d_keys = np.array(list(d.keys()))\n",
    "    seconds = np.array([date_to_seconds(dates[d[i][0]]) for i in d_keys])\n",
    "    d_keys = d_keys[np.argsort(seconds)]\n",
    "    # determine which images will fill which units of the grid established above\n",
    "    coords = np.zeros(\n",
    "        (len(datestrings), 2)\n",
    "    )  # 2D array with x, y clip-space coords of each date\n",
    "    for jdx, j in enumerate(d_keys):\n",
    "        for kdx, k in enumerate(d[j]):\n",
    "            x = jdx * (cols + 1) + (kdx % cols)\n",
    "            y = kdx // cols\n",
    "            coords[k] = [grid_x[x], grid_y[y]]\n",
    "    # find the positions of labels\n",
    "    label_positions = np.array(\n",
    "        [[grid_x[i * (cols + 1)], grid_y[0]] for i in range(len(d))]\n",
    "    )\n",
    "    # move the labels down in the y dimension by a grid unit\n",
    "    dx = grid_x[1] - grid_x[0]  # size of a single cell\n",
    "    label_positions[:, 1] = label_positions[:, 1] - dx\n",
    "    # quantize the label positions and label positions\n",
    "    image_positions = round_floats(coords)\n",
    "    label_positions = round_floats(label_positions.tolist())\n",
    "    # write and return the paths to the date based layout\n",
    "    return {\n",
    "        \"layout\": write_json(positions_out_path, image_positions, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": label_positions,\n",
    "                \"labels\": d_keys.tolist(),\n",
    "                \"cols\": cols,\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def datestring_to_date(datestring):\n",
    "    \"\"\"\n",
    "    Given a string representing a date return a datetime object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parse_date(\n",
    "            str(datestring), fuzzy=True, default=datetime.datetime(9999, 1, 1)\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(timestamp(), \"Could not parse datestring {}\".format(datestring))\n",
    "        return datestring\n",
    "\n",
    "\n",
    "def date_to_seconds(date):\n",
    "    \"\"\"\n",
    "    Given a datetime object return an integer representation for that datetime\n",
    "    \"\"\"\n",
    "    if isinstance(date, datetime.datetime):\n",
    "        return (date - datetime.datetime.today()).total_seconds()\n",
    "    else:\n",
    "        return -float(\"inf\")\n",
    "\n",
    "\n",
    "def round_date(date, unit):\n",
    "    \"\"\"\n",
    "    Return `date` truncated to the temporal unit specified in `units`\n",
    "    \"\"\"\n",
    "    if not isinstance(date, datetime.datetime):\n",
    "        return \"no_date\"\n",
    "    formatted = date.strftime(\"%d %B %Y -- %X\")\n",
    "    if unit in set([\"seconds\", \"minutes\", \"hours\"]):\n",
    "        date = formatted.split(\"--\")[1].strip()\n",
    "        if unit == \"seconds\":\n",
    "            date = date\n",
    "        elif unit == \"minutes\":\n",
    "            date = \":\".join(d.split(\":\")[:-1]) + \":00\"\n",
    "        elif unit == \"hours\":\n",
    "            date = date.split(\":\")[0] + \":00:00\"\n",
    "    elif unit in set([\"days\", \"months\", \"years\", \"decades\", \"centuries\"]):\n",
    "        date = formatted.split(\"--\")[0].strip()\n",
    "        if unit == \"days\":\n",
    "            date = date\n",
    "        elif unit == \"months\":\n",
    "            date = \" \".join(date.split()[1:])\n",
    "        elif unit == \"years\":\n",
    "            date = date.split()[-1]\n",
    "        elif unit == \"decades\":\n",
    "            date = str(int(date.split()[-1]) // 10) + \"0\"\n",
    "        elif unit == \"centuries\":\n",
    "            date = str(int(date.split()[-1]) // 100) + \"00\"\n",
    "    return date\n",
    "\n",
    "\n",
    "##\n",
    "# Metadata Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_categorical_layout(null_category=\"Other\", margin=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a numpy array with shape (n_points, 2) with the point\n",
    "    positions of observations in box regions determined by\n",
    "    each point's category metadata attribute (if applicable)\n",
    "    \"\"\"\n",
    "    if not kwargs.get(\"metadata\", False):\n",
    "        return False\n",
    "    # determine the out path and return from cache if possible\n",
    "    out_path = get_path(\"layouts\", \"categorical\", **kwargs)\n",
    "    labels_out_path = get_path(\"layouts\", \"categorical-labels\", **kwargs)\n",
    "    # accumulate d[category] = [indices of points with category]\n",
    "    categories = [i.get(\"category\", None) for i in kwargs[\"metadata\"]]\n",
    "    if not any(categories) or len(set(categories) - set([None])) == 1:\n",
    "        return False\n",
    "    d = defaultdict(list)\n",
    "    for idx, i in enumerate(categories):\n",
    "        d[i].append(idx)\n",
    "    # store the number of observations in each group\n",
    "    keys_and_counts = [{\"key\": i, \"count\": len(d[i])} for i in d]\n",
    "    keys_and_counts.sort(key=operator.itemgetter(\"count\"), reverse=True)\n",
    "    # get the box layout then subdivide into discrete points\n",
    "    boxes = get_categorical_boxes([i[\"count\"] for i in keys_and_counts], margin=margin)\n",
    "    points = get_categorical_points(boxes)\n",
    "    # sort the points into the order of the observations in the metadata\n",
    "    counts = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    offsets = {i[\"key\"]: 0 for i in keys_and_counts}\n",
    "    for idx, i in enumerate(keys_and_counts):\n",
    "        offsets[i[\"key\"]] += sum([j[\"count\"] for j in keys_and_counts[:idx]])\n",
    "    sorted_points = []\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        category = i.metadata.get(\"category\", null_category)\n",
    "        sorted_points.append(points[offsets[category] + counts[category]])\n",
    "        counts[category] += 1\n",
    "    sorted_points = np.array(sorted_points)\n",
    "    # add to the sorted points the anchors for the text labels for each group\n",
    "    text_anchors = np.array([[i.x, i.y - margin / 2] for i in boxes])\n",
    "    # add the anchors to the points - these will be removed after the points are projected\n",
    "    sorted_points = np.vstack([sorted_points, text_anchors])\n",
    "    # scale -1:1 using the largest axis as the scaling metric\n",
    "    _max = np.max(sorted_points)\n",
    "    for i in range(2):\n",
    "        _min = np.min(sorted_points[:, i])\n",
    "        sorted_points[:, i] -= _min\n",
    "        sorted_points[:, i] /= _max - _min\n",
    "        sorted_points[:, i] -= np.max(sorted_points[:, i]) / 2\n",
    "        sorted_points[:, i] *= 2\n",
    "    # separate out the sorted points and text positions\n",
    "    text_anchors = sorted_points[-len(text_anchors) :]\n",
    "    sorted_points = sorted_points[: -len(text_anchors)]\n",
    "    z = round_floats(sorted_points.tolist())\n",
    "    return {\n",
    "        \"layout\": write_json(out_path, z, **kwargs),\n",
    "        \"labels\": write_json(\n",
    "            labels_out_path,\n",
    "            {\n",
    "                \"positions\": round_floats(text_anchors.tolist()),\n",
    "                \"labels\": [i[\"key\"] for i in keys_and_counts],\n",
    "            },\n",
    "            **kwargs\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_categorical_boxes(group_counts, margin=2):\n",
    "    \"\"\"\n",
    "    @arg [int] group_counts: counts of the number of images in each\n",
    "      distinct level within the metadata's caetgories\n",
    "    @kwarg int margin: space between boxes in the 2D layout\n",
    "    @returns [Box] an array of Box() objects; one per level in `group_counts`\n",
    "    \"\"\"\n",
    "    group_counts = sorted(group_counts, reverse=True)\n",
    "    boxes = []\n",
    "    for i in group_counts:\n",
    "        w = h = math.ceil(i ** (1 / 2))\n",
    "        boxes.append(Box(i, w, h, None, None))\n",
    "    # find the position along x axis where we want to create a break\n",
    "    wrap = math.floor(sum([i.cells for i in boxes]) ** (1 / 2)) - (2 * margin)\n",
    "    # find the valid positions on the y axis\n",
    "    y = margin\n",
    "    y_spots = []\n",
    "    for i in boxes:\n",
    "        if (y + i.h + margin) <= wrap:\n",
    "            y_spots.append(y)\n",
    "            y += i.h + margin\n",
    "        else:\n",
    "            y_spots.append(y)\n",
    "            break\n",
    "    # get a list of lists where sublists contain elements at the same y position\n",
    "    y_spot_index = 0\n",
    "    for i in boxes:\n",
    "        # find the y position\n",
    "        y = y_spots[y_spot_index]\n",
    "        # find members with this y position\n",
    "        row_members = [j.x + j.w for j in boxes if j.y == y]\n",
    "        # assign the y position\n",
    "        i.y = y\n",
    "        y_spot_index = (y_spot_index + 1) % len(y_spots)\n",
    "        # assign the x position\n",
    "        i.x = max(row_members) + margin if row_members else margin\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_categorical_points(arr, unit_size=None):\n",
    "    \"\"\"Given an array of Box() objects, return a 2D distribution with shape (n_cells, 2)\"\"\"\n",
    "    points_arr = []\n",
    "    for i in arr:\n",
    "        area = i.w * i.h\n",
    "        per_unit = (area / i.cells) ** (1 / 2)\n",
    "        x_units = math.ceil(i.w / per_unit)\n",
    "        y_units = math.ceil(i.h / per_unit)\n",
    "        if not unit_size:\n",
    "            unit_size = min(i.w / x_units, i.h / y_units)\n",
    "        for j in range(i.cells):\n",
    "            x = j % x_units\n",
    "            y = j // x_units\n",
    "            points_arr.append(\n",
    "                [\n",
    "                    i.x + x * unit_size,\n",
    "                    i.y + y * unit_size,\n",
    "                ]\n",
    "            )\n",
    "    return np.array(points_arr)\n",
    "\n",
    "\n",
    "class Box:\n",
    "    \"\"\"Store the width, height, and x, y coords of a box\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.cells = args[0]\n",
    "        self.w = args[1]\n",
    "        self.h = args[2]\n",
    "        self.x = None if len(args) < 4 else args[3]\n",
    "        self.y = None if len(args) < 5 else args[4]\n",
    "\n",
    "\n",
    "##\n",
    "# Geographic Layout\n",
    "##\n",
    "\n",
    "\n",
    "def get_geographic_layout(**kwargs):\n",
    "    \"\"\"Return a 2D array of image positions corresponding to lat, lng coordinates\"\"\"\n",
    "    out_path = get_path(\"layouts\", \"geographic\", **kwargs)\n",
    "    l = []\n",
    "    coords = False\n",
    "    for idx, i in enumerate(stream_images(**kwargs)):\n",
    "        lat = float(i.metadata.get(\"lat\", 0)) / 180\n",
    "        lng = (\n",
    "            float(i.metadata.get(\"lng\", 0)) / 180\n",
    "        )  # the plot draws longitude twice as tall as latitude\n",
    "        if lat or lng:\n",
    "            coords = True\n",
    "        l.append([lng, lat])\n",
    "    if coords:\n",
    "        print(timestamp(), \"Creating geographic layout\")\n",
    "        if kwargs[\"geojson\"]:\n",
    "            process_geojson(kwargs[\"geojson\"])\n",
    "        return {\"layout\": write_layout(out_path, l, scale=False, **kwargs)}\n",
    "    elif kwargs[\"geojson\"]:\n",
    "        print(\n",
    "            timestamp(),\n",
    "            \"GeoJSON is only processed if you also provide lat/lng coordinates for your images in a metadata file!\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path):\n",
    "    \"\"\"Given a GeoJSON filepath, write a minimal JSON output in lat lng coordinates\"\"\"\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson = json.load(f)\n",
    "    l = []\n",
    "    for i in geojson:\n",
    "        if isinstance(i, dict):\n",
    "            for j in i.get(\"coordinates\", []):\n",
    "                for k in j:\n",
    "                    l.append(k)\n",
    "    with open(\n",
    "        os.path.join(\"output\", \"assets\", \"json\", \"geographic-features.json\"), \"w\"\n",
    "    ) as out:\n",
    "        json.dump(l, out)\n",
    "\n",
    "\n",
    "##\n",
    "# Helpers\n",
    "##\n",
    "\n",
    "\n",
    "def get_path(*args, **kwargs):\n",
    "    \"\"\"Return the path to a JSON file with conditional gz extension\"\"\"\n",
    "    sub_dir, filename = args\n",
    "    out_dir = join(kwargs[\"out_dir\"], sub_dir) if sub_dir else kwargs[\"out_dir\"]\n",
    "    if kwargs.get(\"add_hash\", True):\n",
    "        filename += \"-\" + kwargs[\"plot_id\"]\n",
    "    path = join(out_dir, filename + \".json\")\n",
    "    return path + \".gz\" if kwargs.get(\"gzip\", False) else path\n",
    "\n",
    "\n",
    "def write_layout(path, obj, **kwargs):\n",
    "    \"\"\"Write layout json `obj` to disk and return the path to the saved file\"\"\"\n",
    "    if kwargs.get(\"scale\", True) != False:\n",
    "        obj = (minmax_scale(obj) - 0.5) * 2  # scale -1:1\n",
    "    if kwargs.get(\"round\", True) != False:\n",
    "        obj = round_floats(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        obj = obj.tolist()\n",
    "    return write_json(path, obj, **kwargs)\n",
    "\n",
    "\n",
    "def round_floats(obj, digits=5):\n",
    "    \"\"\"Return 2D array obj with rounded float precision\"\"\"\n",
    "    return [[round(float(j), digits) for j in i] for i in obj]\n",
    "\n",
    "\n",
    "def write_json(path, obj, **kwargs):\n",
    "    \"\"\"Write json object `obj` to disk and return the path to that file\"\"\"\n",
    "    out_dir, filename = os.path.split(path)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"w\") as out:\n",
    "            out.write(json.dumps(obj, indent=4).encode(kwargs[\"encoding\"]))\n",
    "        return path\n",
    "    else:\n",
    "        with open(path, \"w\") as out:\n",
    "            json.dump(obj, out, indent=4)\n",
    "        return path\n",
    "\n",
    "\n",
    "def read_json(path, **kwargs):\n",
    "    \"\"\"Read and return the json object written by the current process at `path`\"\"\"\n",
    "    if kwargs.get(\"gzip\", False):\n",
    "        with gzip.GzipFile(path, \"r\") as f:\n",
    "            return json.loads(f.read().decode(kwargs[\"encoding\"]))\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_hotspots(layouts={}, use_high_dimensional_vectors=True, **kwargs):\n",
    "    \"\"\"Return the stable clusters from the condensed tree of connected components from the density graph\"\"\"\n",
    "    print(timestamp(), \"Clustering data with {}\".format(cluster_method))\n",
    "    if use_high_dimensional_vectors:\n",
    "        vecs = kwargs[\"vecs\"]\n",
    "    else:\n",
    "        vecs = read_json(layouts[\"umap\"][\"variants\"][0][\"layout\"], **kwargs)\n",
    "    model = get_cluster_model(**kwargs)\n",
    "    z = model.fit(vecs)\n",
    "    # create a map from cluster label to image indices in cluster\n",
    "    d = defaultdict(lambda: defaultdict(list))\n",
    "    for idx, i in enumerate(z.labels_):\n",
    "        if i != -1:\n",
    "            d[i][\"images\"].append(idx)\n",
    "            d[i][\"img\"] = clean_filename(kwargs[\"image_paths\"][idx])\n",
    "            d[i][\"layout\"] = \"inception_vectors\"\n",
    "    # remove massive clusters\n",
    "    deletable = []\n",
    "    for i in d:\n",
    "        # find percent of images in cluster\n",
    "        image_percent = len(d[i][\"images\"]) / len(vecs)\n",
    "        # determine if image or area percent is too large\n",
    "        if image_percent > 0.5:\n",
    "            deletable.append(i)\n",
    "    for i in deletable:\n",
    "        del d[i]\n",
    "    # sort the clusers by size and then label the clusters\n",
    "    clusters = d.values()\n",
    "    clusters = sorted(clusters, key=lambda i: len(i[\"images\"]), reverse=True)\n",
    "    for idx, i in enumerate(clusters):\n",
    "        i[\"label\"] = \"Cluster {}\".format(idx + 1)\n",
    "    # slice off the first `max_clusters`\n",
    "    clusters = clusters[: kwargs[\"max_clusters\"]]\n",
    "    # save the hotspots to disk and return the path to the saved json\n",
    "    print(timestamp(), \"Found\", len(clusters), \"hotspots\")\n",
    "    return write_json(get_path(\"hotspots\", \"hotspot\", **kwargs), clusters, **kwargs)\n",
    "\n",
    "\n",
    "def get_cluster_model(**kwargs):\n",
    "    \"\"\"Return a model with .fit() method that can be used to cluster input vectors\"\"\"\n",
    "    config = {\n",
    "        \"core_dist_n_jobs\": multiprocessing.cpu_count(),\n",
    "        \"min_cluster_size\": kwargs[\"min_cluster_size\"],\n",
    "        \"cluster_selection_epsilon\": 0.01,\n",
    "        \"min_samples\": 1,\n",
    "        \"approx_min_span_tree\": False,\n",
    "    }\n",
    "    return HDBSCAN(**config)\n",
    "\n",
    "\n",
    "def get_heightmap(path, label, **kwargs):\n",
    "    \"\"\"Create a heightmap using the distribution of points stored at `path`\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    X = read_json(path, **kwargs)\n",
    "    if \"positions\" in X:\n",
    "        X = X[\"positions\"]\n",
    "    X = np.array(X)\n",
    "    if X.shape[-1] != 2:\n",
    "        print(timestamp(), \"Could not create heightmap because data is not 2D\")\n",
    "        return\n",
    "    # create kernel density estimate of distribution X\n",
    "    nbins = 200\n",
    "    x, y = X.T\n",
    "    xi, yi = np.mgrid[x.min() : x.max() : nbins * 1j, y.min() : y.max() : nbins * 1j]\n",
    "    zi = kde.gaussian_kde(X.T)(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading=\"gouraud\", cmap=plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # save the plot\n",
    "    out_dir = os.path.join(kwargs[\"out_dir\"], \"heightmaps\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_path = os.path.join(out_dir, label + \"-heightmap.png\")\n",
    "    plt.savefig(out_path, pad_inches=0)\n",
    "\n",
    "\n",
    "def write_images(**kwargs):\n",
    "    \"\"\"Write all originals and thumbs to the output dir\"\"\"\n",
    "    for i in stream_images(**kwargs):\n",
    "        filename = clean_filename(i.path)\n",
    "        # copy original for lightbox\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"originals\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        if not os.path.exists(out_path):\n",
    "            resized = i.resize_to_height(600)\n",
    "            resized = array_to_img(resized)\n",
    "            save_img(out_path, resized)\n",
    "        # copy thumb for lod texture\n",
    "        out_dir = join(kwargs[\"out_dir\"], \"thumbs\")\n",
    "        if not exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = join(out_dir, filename)\n",
    "        img = array_to_img(i.resize_to_max(kwargs[\"lod_cell_height\"]))\n",
    "        save_img(out_path, img)\n",
    "\n",
    "\n",
    "def get_version():\n",
    "    \"\"\"Return the version of clipplot installed\"\"\"\n",
    "    return pkg_resources.get_distribution(\"clipplot\").version\n",
    "\n",
    "\n",
    "class Image:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.path = args[0]\n",
    "        self.original = load_img(self.path)\n",
    "        self.metadata = kwargs[\"metadata\"] if kwargs[\"metadata\"] else {}\n",
    "\n",
    "    def resize_to_max(self, n):\n",
    "        \"\"\"\n",
    "        Resize self.original so its longest side has n pixels (maintain proportion)\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        size = (n, int(n * h / w)) if w > h else (int(n * w / h), n)\n",
    "        return img_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_height(self, height):\n",
    "        \"\"\"\n",
    "        Resize self.original into an image with height h and proportional width\n",
    "        \"\"\"\n",
    "        w, h = self.original.size\n",
    "        if (w / h * height) < 1:\n",
    "            resizedwidth = 1\n",
    "        else:\n",
    "            resizedwidth = int(w / h * height)\n",
    "        size = (resizedwidth, height)\n",
    "        return img_to_array(self.original.resize(size))\n",
    "\n",
    "    def resize_to_square(self, n, center=False):\n",
    "        \"\"\"\n",
    "        Resize self.original to an image with nxn pixels (maintain proportion)\n",
    "        if center, center the colored pixels in the square, else left align\n",
    "        \"\"\"\n",
    "        a = self.resize_to_max(n)\n",
    "        h, w, c = a.shape\n",
    "        pad_lr = int((n - w) / 2)  # left right pad\n",
    "        pad_tb = int((n - h) / 2)  # top bottom pad\n",
    "        b = np.zeros((n, n, 3))\n",
    "        if center:\n",
    "            b[pad_tb : pad_tb + h, pad_lr : pad_lr + w, :] = a\n",
    "        else:\n",
    "            b[:h, :w, :] = a\n",
    "        return b\n",
    "\n",
    "\n",
    "##\n",
    "# Entry Point\n",
    "##\n",
    "\n",
    "\n",
    "def parse():\n",
    "    \"\"\"Read command line args and begin data processing\"\"\"\n",
    "    description = \"Create the data required to create a clipplot viewer\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--images\",\n",
    "        \"-i\",\n",
    "        type=str,\n",
    "        default=config[\"images\"],\n",
    "        help=\"path to a glob of images to process\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metadata\",\n",
    "        \"-m\",\n",
    "        type=str,\n",
    "        default=config[\"metadata\"],\n",
    "        help=\"path to a csv or glob of JSON files with image metadata (see readme for format)\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_images\",\n",
    "        type=int,\n",
    "        default=config[\"max_images\"],\n",
    "        help=\"maximum number of images to process from the input glob\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cache\",\n",
    "        type=bool,\n",
    "        default=config[\"use_cache\"],\n",
    "        help=\"given inputs identical to prior inputs, load outputs from cache\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoding\",\n",
    "        type=str,\n",
    "        default=config[\"encoding\"],\n",
    "        help=\"the encoding of input metadata\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_cluster_size\",\n",
    "        type=int,\n",
    "        default=config[\"min_cluster_size\"],\n",
    "        help=\"the minimum number of images in a cluster\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_clusters\",\n",
    "        type=int,\n",
    "        default=config[\"max_clusters\"],\n",
    "        help=\"the maximum number of clusters to return\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_dir\",\n",
    "        type=str,\n",
    "        default=config[\"out_dir\"],\n",
    "        help=\"the directory to which outputs will be saved\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cell_size\",\n",
    "        type=int,\n",
    "        default=config[\"cell_size\"],\n",
    "        help=\"the size of atlas cells in px\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_neighbors\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=config[\"n_neighbors\"],\n",
    "        help=\"the n_neighbors arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_dist\",\n",
    "        nargs=\"+\",\n",
    "        type=float,\n",
    "        default=config[\"min_dist\"],\n",
    "        help=\"the min_dist arguments for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_components\",\n",
    "        type=int,\n",
    "        default=config[\"n_components\"],\n",
    "        help=\"the n_components argument for UMAP\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        default=config[\"metric\"],\n",
    "        help=\"the metric argument for umap\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pointgrid_fill\",\n",
    "        type=float,\n",
    "        default=config[\"pointgrid_fill\"],\n",
    "        help=\"float 0:1 that determines sparsity of jittered distributions (lower means more sparse)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--copy_web_only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"update ./output/assets without reprocessing data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_size\",\n",
    "        type=float,\n",
    "        default=config[\"min_size\"],\n",
    "        help=\"min size of cropped images\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gzip\", action=\"store_true\", help=\"save outputs with gzip compression\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\",\n",
    "        action=\"store_true\",\n",
    "        help=\"shuffle the input images before data processing begins\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plot_id\",\n",
    "        type=str,\n",
    "        default=config[\"plot_id\"],\n",
    "        help=\"unique id for a plot; useful for resuming processing on a started plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=config[\"seed\"], help=\"seed for random processes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_clusters\",\n",
    "        type=int,\n",
    "        default=config[\"n_clusters\"],\n",
    "        help=\"number of clusters to use when clustering with kmeans\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--geojson\",\n",
    "        type=str,\n",
    "        default=config[\"geojson\"],\n",
    "        help=\"path to a GeoJSON file with shapes to be rendered on a map\",\n",
    "    )\n",
    "    config.update(vars(parser.parse_args()))\n",
    "    process_images(**config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "589df220451a68b9e5c43a1cf71c04097e923ea72e1851693eb18f34dd3f29e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
