{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import glob2\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from clip_plot.utils import clean_filename, FILE_NAME\n",
    "from clip_plot.layouts import get_layouts, get_heightmap, get_hotspots\n",
    "from clip_plot.utils import is_number, get_path, get_version, write_json, read_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle metadata (dates, categories, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_metadata_list(meta_dir: str) -> List[dict]:\n",
    "    \"\"\"Return a list of objects with image metadata.\n",
    "\n",
    "    Will create 'tags' key if 'category' is in metadata\n",
    "    but not 'tags'.\n",
    "    \n",
    "    Args:\n",
    "        metadata (str, default = None): Metadata location\n",
    "\n",
    "    Returns:\n",
    "        l (List[dict]): List of metadata \n",
    "\n",
    "    Notes:\n",
    "        No check for 'filename' is performed\n",
    "\n",
    "    Todo:\n",
    "        Think about separating .csv and json functionality.\n",
    "        Can we use pandas numpy to process csv?\n",
    "    \"\"\"\n",
    "\n",
    "    # handle csv metadata\n",
    "    metaList = []\n",
    "    if meta_dir.endswith(\".csv\"):\n",
    "        with open(meta_dir) as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = [i.lower() for i in next(reader)]\n",
    "            for i in reader:\n",
    "                metaList.append(\n",
    "                    {\n",
    "                        headers[j]: i[j] if len(i) > j and i[j] else \"\"\n",
    "                        for j, _ in enumerate(headers)\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    # handle json metadata\n",
    "    else:\n",
    "        for i in glob2.glob(meta_dir):\n",
    "            with open(i) as f:\n",
    "                metaList.append(json.load(f))\n",
    "\n",
    "    # if the user provided a category but not a tag, use the category as the tag\n",
    "    for metaDict in metaList:\n",
    "        if \"category\" in metaDict and (\"tags\" in metaDict) is False:\n",
    "            metaDict.update({\"tags\": metaDict[\"category\"]})\n",
    "    return metaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_metadata(metadata: List[dict], out_dir: str, gzip: Optional[bool] = False, encoding:  Optional[str] = 'utf8'):\n",
    "    \"\"\"Write list `metadata` of objects to disk\n",
    "    \n",
    "    Args:\n",
    "        metadata (list[dict])\n",
    "        out_dir (str)\n",
    "\n",
    "        subfunctions:\n",
    "            write_json():\n",
    "                gzip (Optional[bool]):\n",
    "                encoding (Optional[str]): Required if gzip is provided\n",
    "                    default = 'utf8'\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        Improve variable naming\n",
    "    \n",
    "    \"\"\"\n",
    "    if not metadata:\n",
    "        return\n",
    "    \n",
    "    # Create kwargs replacement for write_json function\n",
    "    writeJasonDict = {'encoding': encoding, 'gzip': gzip}\n",
    "\n",
    "    out_dir = os.path.join(out_dir, \"metadata\")\n",
    "    for i in [\"filters\", \"options\", \"file\"]:\n",
    "        out_path = os.path.join(out_dir, i)\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "    \n",
    "    # Create the lists of images with each tag\n",
    "    d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        filename = clean_filename(i[FILE_NAME])\n",
    "        i[\"tags\"] = [j.strip() for j in i.get(\"tags\", \"\").split(\"|\")]\n",
    "        for j in i[\"tags\"]:\n",
    "            d[\"__\".join(j.split())].append(filename)\n",
    "        write_json(os.path.join(out_dir, \"file\", filename + \".json\"), i, **writeJasonDict)\n",
    "\n",
    "    write_json(\n",
    "        os.path.join(out_dir, \"filters\", \"filters.json\"),\n",
    "        [\n",
    "            {\n",
    "                \"filter_name\": \"select\",\n",
    "                \"filter_values\": list(d.keys()),\n",
    "            }\n",
    "        ],\n",
    "        **writeJasonDict\n",
    "    )\n",
    "\n",
    "    # create the options for the category dropdown\n",
    "    for i in d:\n",
    "        write_json(os.path.join(out_dir, \"options\", i + \".json\"), d[i], **writeJasonDict)\n",
    "    \n",
    "    # create the map from date to images with that date (if dates present)\n",
    "    date_d = defaultdict(list)\n",
    "    for i in metadata:\n",
    "        date = i.get(\"year\", \"\")\n",
    "        if date:\n",
    "            date_d[date].append(clean_filename(i[FILE_NAME]))\n",
    "\n",
    "    # find the min and max dates to show on the date slider\n",
    "    dates = np.array([int(i.strip()) for i in date_d if is_number(i)])\n",
    "    domain = {\"min\": float(\"inf\"), \"max\": -float(\"inf\")}\n",
    "    mean = np.mean(dates)\n",
    "    std = np.std(dates)\n",
    "    for i in dates:\n",
    "        # update the date domain with all non-outlier dates\n",
    "        if abs(mean - i) < (std * 4):\n",
    "            domain[\"min\"] = int(min(i, domain[\"min\"]))\n",
    "            domain[\"max\"] = int(max(i, domain[\"max\"]))\n",
    "\n",
    "    # write the dates json\n",
    "    if len(date_d) > 1:\n",
    "        write_json(\n",
    "            os.path.join(out_dir, \"dates.json\"),\n",
    "            {\n",
    "                \"domain\": domain,\n",
    "                \"dates\": date_d,\n",
    "            },\n",
    "            **writeJasonDict\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "##\n",
    "# Main\n",
    "##\n",
    "\n",
    "\n",
    "def get_manifest(**kwargs):\n",
    "    \"\"\"Create and return the base object for the manifest output file\n",
    "    \n",
    "    Args:\n",
    "        atlas_dir (str)\n",
    "        image_paths (str)\n",
    "        plot_id (str, default = str(uuid.uuid1()))\n",
    "        out_dir (str)\n",
    "        metadata (list[dict]): Only checking if provided\n",
    "        gzip (bool, default = False)\n",
    "        atlas_size (int, default = 2048)\n",
    "        cell_size (int, default = 32)\n",
    "        lod_cell_height (int, default = 128)\n",
    "\n",
    "        Need to check subfunctions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        Original description is inadequate\n",
    "        Function is to big (god function)\n",
    "    \n",
    "    \"\"\"\n",
    "    # load the atlas data\n",
    "    atlas_data = json.load(open(os.path.join(kwargs[\"atlas_dir\"], \"atlas_positions.json\")))\n",
    "    # store each cell's size and atlas position\n",
    "    atlas_ids = set([i[\"idx\"] for i in atlas_data])\n",
    "    sizes = [[] for _ in atlas_ids]\n",
    "    pos = [[] for _ in atlas_ids]\n",
    "    for idx, i in enumerate(atlas_data):\n",
    "        sizes[i[\"idx\"]].append([i[\"w\"], i[\"h\"]])\n",
    "        pos[i[\"idx\"]].append([i[\"x\"], i[\"y\"]])\n",
    "\n",
    "    # obtain the paths to each layout's JSON positions\n",
    "    layouts = get_layouts(**kwargs)\n",
    "    # create a heightmap for the umap layout\n",
    "    if \"umap\" in layouts and layouts[\"umap\"]:\n",
    "        get_heightmap(layouts[\"umap\"][\"variants\"][0][\"layout\"], \"umap\", **kwargs)\n",
    "    \n",
    "    # specify point size scalars\n",
    "    point_sizes = {}\n",
    "    point_sizes[\"min\"] = 0\n",
    "    point_sizes[\"grid\"] = 1 / ceil(len(kwargs[\"image_paths\"]) ** (1 / 2))\n",
    "    point_sizes[\"max\"] = point_sizes[\"grid\"] * 1.2\n",
    "    point_sizes[\"scatter\"] = point_sizes[\"grid\"] * 0.2\n",
    "    point_sizes[\"initial\"] = point_sizes[\"scatter\"]\n",
    "    point_sizes[\"categorical\"] = point_sizes[\"grid\"] * 0.6\n",
    "    point_sizes[\"geographic\"] = point_sizes[\"grid\"] * 0.025\n",
    "\n",
    "    # fetch the date distribution data for point sizing\n",
    "    if \"date\" in layouts and layouts[\"date\"]:\n",
    "        date_layout = read_json(layouts[\"date\"][\"labels\"], **kwargs)\n",
    "        point_sizes[\"date\"] = 1 / (\n",
    "            (date_layout[\"cols\"] + 1) * len(date_layout[\"labels\"])\n",
    "        )\n",
    "\n",
    "    # create manifest json\n",
    "    manifest = {\n",
    "        \"version\": get_version(),\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "        \"output_directory\": os.path.split(kwargs[\"out_dir\"])[0],\n",
    "        \"layouts\": layouts,\n",
    "        \"initial_layout\": \"umap\",\n",
    "        \"point_sizes\": point_sizes,\n",
    "        \"imagelist\": get_path(\"imagelists\", \"imagelist\", **kwargs),\n",
    "        \"atlas_dir\": kwargs[\"atlas_dir\"],\n",
    "        \"metadata\": True if kwargs[\"metadata\"] else False,\n",
    "        \"default_hotspots\": get_hotspots(layouts=layouts, **kwargs),\n",
    "        \"custom_hotspots\": get_path(\n",
    "            \"hotspots\", \"user_hotspots\", add_hash=False, **kwargs\n",
    "        ),\n",
    "        \"gzipped\": kwargs[\"gzip\"],\n",
    "        \"config\": {\n",
    "            \"sizes\": {\n",
    "                \"atlas\": kwargs[\"atlas_size\"],\n",
    "                \"cell\": kwargs[\"cell_size\"],\n",
    "                \"lod\": kwargs[\"lod_cell_height\"],\n",
    "            },\n",
    "        },\n",
    "        \"creation_date\": datetime.today().strftime(\"%d-%B-%Y-%H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    # write the manifest without gzipping\n",
    "    no_gzip_kwargs = {\n",
    "        \"out_dir\": kwargs[\"out_dir\"],\n",
    "        \"gzip\": False,\n",
    "        \"plot_id\": kwargs[\"plot_id\"],\n",
    "    }\n",
    "    path = get_path(\"manifests\", \"manifest\", **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "    path = get_path(None, \"manifest\", add_hash=False, **no_gzip_kwargs)\n",
    "    write_json(path, manifest, **no_gzip_kwargs)\n",
    "\n",
    "    # create images json\n",
    "    imagelist = {\n",
    "        \"cell_sizes\": sizes,\n",
    "        \"images\": [clean_filename(i) for i in kwargs[\"image_paths\"]],\n",
    "        \"atlas\": {\n",
    "            \"count\": len(atlas_ids),\n",
    "            \"positions\": pos,\n",
    "        },\n",
    "    }\n",
    "    write_json(manifest[\"imagelist\"], imagelist, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
