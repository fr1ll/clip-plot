# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/13_pipelines.ipynb.

# %% auto 0
__all__ = ['project_images_pipeline', 'embed_images_pipeline']

# %% ../../nbs/13_pipelines.ipynb 3
# print separately that we're loading dependencies, as this can take a while
# and we want to give immediate feedback the program is starting
from .utils import timestamp
print(timestamp(), "Beginning to load dependencies")

# %% ../../nbs/13_pipelines.ipynb 4
from fastcore.all import in_ipython
from tqdm.auto import tqdm

from .from_tables import cat_tables, table_to_meta
from .web_config import copy_web_assets, get_clip_plot_root
from .embeddings import get_embeddings, write_embeddings
from .metadata import get_manifest, write_metadata
from .images import create_atlases_and_thumbs, ImageFactory
from .configuration import UmapSpec, ClusterSpec, ViewerOptions, ImageLoaderOptions, Cfg

# %% ../../nbs/13_pipelines.ipynb 5
from shutil import rmtree
from pathlib import Path
import pandas as pd
import numpy as np

# %% ../../nbs/13_pipelines.ipynb 7
def project_images_pipeline(output_dir: Path,
                            plot_id: str,
                            model: str,
                            viewer_opts: ViewerOptions,
                            umap_spec: UmapSpec,
                            cluster_spec: ClusterSpec,
                            image_opts: ImageLoaderOptions,
                            images: list[Path] | None = None,
                            tables: list[Path] | None = None,
                            metadata: list[Path] | None = None,
                            image_path_col: str = "image_path",
                            vectors_path_col: str = "hidden_vectors_path",
        ):
        """Convert a folder of images into a clip-plot visualization"""

        if tables and images:
                raise ValueError("Provide either tables or images parameter, not both.")
        if not tables and not images:
                raise ValueError("No images found from either tables or images input.")
        if tables:
                print(timestamp(), "Loading tables")
                table: pd.DataFrame | None = cat_tables(tables)
                images: list[Path] = [Path(p) for p in table[image_path_col].to_numpy()]
                print(timestamp(), "Loading embeddings from disk")
                hidden_vectors: np.ndarray | None = np.array([np.load(e) for e in tqdm(table[vectors_path_col])])
        else:
                hidden_vectors = None
                table = None

        data_dir = output_dir / "data"
        imageEngine = ImageFactory(images, data_dir, metadata,
                                        **image_opts.model_dump(),)

        # grab metadata from table if provided
        if table is not None:
                imageEngine.meta_headers, imageEngine.metadata = table_to_meta(table)

        print(f"Config to project images: {str(image_opts.model_dump())}")

        np.random.seed(image_opts.seed)
        print(timestamp(), "Starting image processing pipeline.")

        copy_web_assets(output_dir=output_dir,
                        tagline=viewer_opts.tagline, logo=viewer_opts.logo)
        write_metadata(imageEngine)
        _, atlas_data = create_atlases_and_thumbs(imageEngine, plot_id)

        if hidden_vectors is None:
                hidden_vectors = get_embeddings(imageEngine, model_name=model)

        get_manifest(imageEngine, atlas_data, hidden_vectors,
                        plot_id=plot_id, output_dir=output_dir,
                        umap_spec=umap_spec, cluster_spec=cluster_spec
        )
        # write_images(imageEngine)
        print(timestamp(), "Done!")

# %% ../../nbs/13_pipelines.ipynb 9
def embed_images_pipeline(images: list[Path],
                     model: str,
                     metadata: list[Path] | None,
                     output_dir: Path,
                     table_format: str,
                     table_id: str,
                ):
                """Embed a folder of images, save embeddings as .npy file to disk"""
                output_dir = Path(output_dir)
                data_dir = output_dir / "data"

                imageEngine = ImageFactory(image_paths=images, data_dir=data_dir, metadata_paths=metadata)

                embeddings = get_embeddings(imageEngine, model_name=model)

                _model_shortname = "--".join(model.split("/")[-2:])

                embs_dir = data_dir/f"embeddings_{_model_shortname}"
                embs_dir.mkdir(parents=True, exist_ok=True)
                emb_paths, _ = write_embeddings(embeddings, imageEngine.filenames, embs_dir)

                ## TODO: Pass embeddings directly into dataframe
                ## rather than writing to disk
                df = pd.DataFrame({"image_path": imageEngine.image_paths,
                                   "image_filename": imageEngine.filenames,
                                   "hidden_vectors_path": [str(e) for e in emb_paths]})

                if len(imageEngine.metadata) > 0:
                        df_meta = pd.DataFrame(imageEngine.metadata)
                        df_meta = df_meta.rename(columns={"filename": "image_filename"})
                        # drop "image_path" column if df_meta has it
                        if "image_path" in df_meta.columns:
                                df_meta = df_meta.drop(columns=["image_path"])

                        df = df.merge(df_meta.drop_duplicates(["image_filename"]), on="image_filename")

                ## standardize sort order of table
                # put standard columns first if they exist in df
                standard_cols = pd.Index(["image_path", "image_filename", "hidden_vectors_path",
                                                   "category", "tags", "x", "y"])
                cols_sorted = standard_cols.intersection(df.columns)
                # append non-standard columns, sorted alphabetically
                cols_sorted = cols_sorted.append(df.columns.difference(standard_cols).sort_values())
                df = df[cols_sorted]
                for col in ["image_path", "hidden_vectors_path"]:
                        df[col] = df[col].astype(str)

                if table_format == "csv":
                        df.to_csv(data_dir / f"EmbedImages__{table_id}.csv", index=False)
                else:
                        df.to_parquet(data_dir / f"EmbedImages__{table_id}.parquet", index=False)
